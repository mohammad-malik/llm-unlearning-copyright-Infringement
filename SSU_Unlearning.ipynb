{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stable Sequential Unlearning (SSU) Framework\n\nComplete implementation following the paper methodology:\n\n## Pipeline Overview:\n1. **Initial Fine-tuning (Step 0)**: Fine-tune vanilla model on all copyrighted books (D_f) to make it memorize them\n2. **Sequential Unlearning (Steps 1-N)**: Unlearn books one at a time using SSU methodology\n   - Each step unlearns one book (D_f^t)\n   - Uses composite loss (L_fgt + L_rnd) and weight saliency\n   - Applies task vector negation\n\n## Datasets:\n- **D_f**: All copyrighted books (10 books from Project Gutenberg)\n- **D_f^t**: Book to unlearn at time step t\n- **D_prev**: Previously unlearned books (aggregated from previous steps)\n- **D_nor**: Retention data (200 chunks from 100 other books) - for evaluation\n\nWorks on both local and Kaggle environments with automatic retry logic.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. Install Dependencies\n","metadata":{}},{"cell_type":"code","source":"# Install required packages\n%pip install -q torch transformers peft datasets accelerate requests protobuf==3.20.3\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:30.337094Z","iopub.execute_input":"2025-11-24T18:15:30.337922Z","iopub.status.idle":"2025-11-24T18:15:34.114066Z","shell.execute_reply.started":"2025-11-24T18:15:30.337897Z","shell.execute_reply":"2025-11-24T18:15:34.112978Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## 2. Configuration\n","metadata":{}},{"cell_type":"code","source":"# Disable torch.compile globally to avoid CUDA capability issues\nimport os\nimport sys\nimport math\nimport random\nimport requests\n\nimport torch\nimport torch._dynamo\n\nfrom typing import Dict, Sequence\nfrom types import SimpleNamespace\nfrom torch.utils.data import ConcatDataset, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom peft import get_peft_model, LoraConfig\n\n\n# Set environment variable to disable torch compilation\nos.environ['TORCH_COMPILE_DISABLE'] = '1'\n\n# Also disable dynamo\ntorch._dynamo.config.disable = True\n\nprint(\"✓ torch.compile disabled globally (for Tesla P100 compatibility)\")","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:34.116014Z","iopub.execute_input":"2025-11-24T18:15:34.116346Z","iopub.status.idle":"2025-11-24T18:15:34.123311Z","shell.execute_reply.started":"2025-11-24T18:15:34.116318Z","shell.execute_reply":"2025-11-24T18:15:34.122353Z"},"trusted":true},"outputs":[{"name":"stdout","text":"✓ torch.compile disabled globally (for Tesla P100 compatibility)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Configuration Class\nclass Config:\n    # Model Configuration - Use smaller model to avoid download issues\n    MODEL_NAME = \"google/gemma-3-1b-it\"\n    \n    # Alternative options:\n    # MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # 3.8B, non-gated\n    # MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small, fast, non-gated\n    \n    TOKENIZER_NAME = MODEL_NAME\n    \n    # HuggingFace Authentication\n    USE_HF_TOKEN = True  # Set True for gated models\n    \n    # PEFT/LoRA Configuration\n    LORA_R = 8\n    LORA_ALPHA = 16\n    LORA_DROPOUT = 0.05\n    TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n\n    # Sequential Unlearning Configuration\n    NUM_UNLEARNING_STEPS = 3  # Number of sequential unlearning steps\n    \n    # Fine-Tuning Hyperparameters\n    BATCH_SIZE = 1  # Reduced further to avoid OOM\n    GRADIENT_ACCUMULATION_STEPS = 16\n    LEARNING_RATE = 5e-5\n    NUM_EPOCHS_FT = 1  # 1 epoch for initial fine-tuning (as per paper)\n    NUM_EPOCHS_INITIAL_FT = 1  # Initial fine-tuning on all books (D_f)\n    \n    # SSU Methodology Parameters\n    EPSILON_1 = 1.0  # Weight for Forgetting Loss (L_fgt)\n    EPSILON_2 = 0.1  # Weight for Random Labeling Loss (L_rnd)\n    GAMMA = 1e-4  # Saliency threshold\n    \n    # Data Configuration\n    CHUNK_SIZE = 256\n    NUM_CHUNKS_PER_STEP = 50\n    USE_REAL_BOOKS = True  # Use real books from Project Gutenberg\n    DATA_DIR = \"gutenberg_books\"\n    \n    # Project Gutenberg Book IDs - All books for initial fine-tuning (D_f)\n    # Paper uses 10 books total, with specific ones at certain time steps\n    ALL_BOOK_IDS = [\n        1661,   # Sherlock Holmes (used at step 1 in paper)\n        84,     # Frankenstein\n        1342,   # Pride and Prejudice (used at step 5 in paper)\n        11,     # Alice in Wonderland (used at step 8 in paper)\n        2701,   # Moby Dick\n        74,     # The Adventures of Tom Sawyer\n        98,     # A Tale of Two Cities\n        5200,   # Metamorphosis\n        6130,   # The Iliad\n        174,    # The Picture of Dorian Gray\n    ]\n    \n    # Books to unlearn at each time step (sequential)\n    GUTENBERG_BOOK_IDS = {\n        1: [1661],  # Sherlock Holmes - Step 1\n        2: [1342],  # Pride and Prejudice - Step 2\n        3: [11],    # Alice in Wonderland - Step 3\n        # Add more steps as needed\n    }\n    \n    # Retention data (D_nor) - 200 chunks from 100 other books\n    USE_RETENTION_DATA = True  # Include D_nor for retention testing\n    NUM_RETENTION_BOOKS = 10  # Reduced for demo (paper uses 100)\n    NUM_RETENTION_CHUNKS = 20  # Reduced for demo (paper uses 200)\n    \n    OUTPUT_DIR = \"ssu_unlearned_models\"\n\nprint(\"Configuration loaded!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:34.124260Z","iopub.execute_input":"2025-11-24T18:15:34.125163Z","iopub.status.idle":"2025-11-24T18:15:34.138784Z","shell.execute_reply.started":"2025-11-24T18:15:34.125143Z","shell.execute_reply":"2025-11-24T18:15:34.137962Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Configuration loaded!\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## 3. Environment Detection & Setup\n","metadata":{}},{"cell_type":"code","source":"\n\n# Detect if running on Kaggle\nIS_KAGGLE = os.path.exists('/kaggle')\nIS_COLAB = 'google.colab' in sys.modules\n\nprint(f\"Running on: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n\n# HuggingFace Authentication (if needed)\nif Config.USE_HF_TOKEN:\n    from huggingface_hub import login\n    \n    hf_token = None\n    \n    # Try Kaggle Secrets\n    if IS_KAGGLE:\n        try:\n            from kaggle_secrets import UserSecretsClient\n            user_secrets = UserSecretsClient()\n            hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n            print(\"Found HuggingFace token in Kaggle Secrets.\")\n        except:\n            pass\n    \n    # Try environment variable\n    if not hf_token:\n        hf_token = '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'\n    \n    if hf_token:\n        try:\n            login(token=hf_token, add_to_git_credential=False)\n            print(\"Successfully logged in to HuggingFace.\")\n        except Exception as e:\n            print(f\"Warning: Could not login: {e}\")\n    else:\n        print(\"WARNING: No HuggingFace token found. Gated models will fail.\")\nelse:\n    print(\"Using non-gated model - no authentication needed.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:34.140870Z","iopub.execute_input":"2025-11-24T18:15:34.141133Z","iopub.status.idle":"2025-11-24T18:15:34.682581Z","shell.execute_reply.started":"2025-11-24T18:15:34.141116Z","shell.execute_reply":"2025-11-24T18:15:34.681867Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running on: Kaggle\nSuccessfully logged in to HuggingFace.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"## 5. SSU Model & Trainer\n","metadata":{}},{"cell_type":"code","source":"\n\n# Dummy book text for simulation\nDUMMY_BOOK_TEXT = \"\"\"\nIn the beginning God created the heavens and the earth. Now the earth was formless and empty, darkness was over the surface of the deep, and the Spirit of God was hovering over the waters. And God said, \"Let there be light,\" and there was light. God saw that the light was good, and he separated the light from the darkness. God called the light \"day,\" and the darkness he called \"night.\" And there was evening, and there was morning—the first day. \n\nAnd God said, \"Let there be a vault between the waters to separate water from water.\" So God made the vault and separated the water under the vault from the water above it. And it was so. God called the vault \"sky.\" And there was evening, and there was morning—the second day.\n\nAnd God said, \"Let the water under the sky be gathered to one place, and let dry ground appear.\" And it was so. God called the dry ground \"land,\" and the gathered waters he called \"seas.\" And God saw that it was good. Then God said, \"Let the land produce vegetation: seed-bearing plants and trees on the land that bear fruit with seed in it, according to their various kinds.\" And it was so. The land produced vegetation: plants bearing seed according to their kinds and trees bearing fruit with seed in it according to their kinds. And God saw that it was good. And there was evening, and there was morning—the third day.\n\nAnd God said, \"Let there be lights in the vault of the sky to separate the day from the night, and let them serve as signs to mark sacred times, and days and years, and let them be lights in the vault of the sky to give light on the earth.\" And it was so. God made two great lights—the greater light to govern the day and the lesser light to govern the night. He also made the stars. God set them in the vault of the sky to give light on the earth, to govern the day and the night, and to separate light from darkness. And God saw that it was good. And there was evening, and there was morning—the fourth day.\n\nAnd God said, \"Let the water teem with living creatures, and let birds fly above the earth across the vault of the sky.\" So God created the great creatures of the sea and every living thing with which the water teems and that moves about in it, according to their kinds, and every winged bird according to its kind. And God saw that it was good. God blessed them and said, \"Be fruitful and increase in number and fill the water in the seas, and let the birds increase on the earth.\" And there was evening, and there was morning—the fifth day.\n\nAnd God said, \"Let the land produce living creatures according to their kinds: the livestock, the creatures that move along the ground, and the wild animals, each according to its kind.\" And it was so. God made the wild animals according to their kinds, the livestock according to their kinds, and all the creatures that move along the ground according to their kinds. And God saw that it was good. Then God said, \"Let us make mankind in our image, in our likeness, so that they may rule over the fish in the sea and the birds in the sky, over the livestock and all the wild animals, and over all the creatures that move along the ground.\" So God created mankind in his own image, in the image of God he created them; male and female he created them. God blessed them and said to them, \"Be fruitful and increase in number; fill the earth and subdue it. Rule over the fish in the sea and the birds in the sky and over every living creature that moves on the ground.\" Then God said, \"I give you every seed-bearing plant on the face of the whole earth and every tree that has fruit with seed in it. They will be yours for food. And to all the beasts of the earth and all the birds in the sky and all the creatures that move along the ground—everything that has the breath of life in it—I give every green plant for food.\" And it was so. God saw all that he had made, and it was very good. And there was evening, and there was morning—the sixth day.\n\nThus the heavens and the earth were completed in all their vast array. By the seventh day God had finished the work he had been doing; so on the seventh day he rested from all his work. Then God blessed the seventh day and made it holy, because on it he rested from all the work of creating that he had done.\n\"\"\" * 50\n\n\ndef generate_simulated_data(text, chunk_size, num_chunks, tokenizer_name):\n    \"\"\"Simulates a list of text chunks for one book (D_f^t).\n    \n    This function splits long book text into smaller chunks that fit within the model's\n    maximum sequence length. It does this by:\n    1. Splitting text into small word chunks (to avoid tokenization warnings)\n    2. Tokenizing each small chunk separately\n    3. Combining tokenized chunks to create final chunks of the desired size\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Get model max length - this is the MAXIMUM tokens the model can handle\n    # TinyLlama has max_length of 2048 tokens\n    max_length = getattr(tokenizer, 'model_max_length', 32000)\n\n    # Fix for huge model_max_length causing OverflowError\n    if max_length > 100000:\n        max_length = 8192  # Set a reasonable limit\n    \n    # Use a safe chunk size that's smaller than model max length\n    # We use chunk_size from config (256) but ensure it's under the model limit\n    safe_chunk_size = min(chunk_size, max_length - 10)  # Leave some margin for safety\n    \n    # IMPORTANT: We need to split text into VERY small pieces before tokenizing\n    # Why? Because 1 word can become 1-3 tokens, and we want to stay under max_length\n    # Strategy: Tokenize in small batches (500-800 words max) to avoid warnings\n    words = text.split()\n    \n    # Conservative estimate: ~1.5 tokens per word on average\n    # So for max_length=2048, we want max ~1300 words per tokenization batch\n    # But to be extra safe, we'll use even smaller: 500 words per batch\n    words_per_batch = min(500, max_length // 3)  # Very conservative: 500 words max per batch\n    \n    # Step 1: Tokenize text in small batches to avoid warnings\n    all_token_ids = []\n    for i in range(0, len(words), words_per_batch):\n        batch_text = ' '.join(words[i:i + words_per_batch])\n        # Tokenize with truncation - this ensures we never exceed max_length\n        tokenized = tokenizer(\n            batch_text, \n            return_tensors='pt', \n            truncation=True,  # This truncates if too long\n            max_length=max_length,  # Use model's actual max length\n            add_special_tokens=True\n        )['input_ids'][0]\n        all_token_ids.extend(tokenized.tolist())\n    \n    # Step 2: If we don't have enough tokens, repeat the sequence\n    if len(all_token_ids) < safe_chunk_size * num_chunks:\n        repeat_factor = (safe_chunk_size * num_chunks // len(all_token_ids)) + 1\n        all_token_ids = (all_token_ids * repeat_factor)[:safe_chunk_size * num_chunks * 2]\n    \n    # Step 3: Split into chunks of the desired size\n    chunks = []\n    for i in range(0, len(all_token_ids) - safe_chunk_size + 1, safe_chunk_size):\n        chunk = all_token_ids[i:i + safe_chunk_size]\n        if len(chunk) == safe_chunk_size:\n            chunks.append(chunk)\n        if len(chunks) >= num_chunks:\n            break\n    \n    # Step 4: If we still don't have enough, pad the last chunk\n    while len(chunks) < num_chunks:\n        if chunks:\n            # Repeat last chunk or pad\n            last_chunk = chunks[-1][:safe_chunk_size]\n            if len(last_chunk) < safe_chunk_size:\n                last_chunk = last_chunk + all_token_ids[:safe_chunk_size - len(last_chunk)]\n            chunks.append(last_chunk[:safe_chunk_size])\n        else:\n            # If no chunks at all, create a dummy chunk\n            chunks.append(all_token_ids[:safe_chunk_size])\n    \n    chunks = chunks[:num_chunks]\n    \n    # Step 5: Decode back to text (this is what the dataset will use)\n    text_chunks = [tokenizer.decode(c, skip_special_tokens=True) for c in chunks]\n    return text_chunks\n\n\nclass SequentialUnlearningDataset(Dataset):\n    \"\"\"Custom Dataset that supports SSU forget data and retention data.\"\"\"\n    def __init__(self, tokenizer, data_texts, mode=\"forget\"):\n        if mode not in {\"forget\", \"retain\"}:\n            raise ValueError(f\"Unsupported dataset mode: {mode}\")\n        self.mode = mode\n        self.tokenizer = tokenizer\n        self.data_texts = data_texts\n        \n        tokenized = tokenizer(\n            data_texts, \n            truncation=True, \n            padding=\"max_length\", \n            max_length=Config.CHUNK_SIZE, \n            return_tensors='pt'\n        )\n        self.input_ids = tokenized['input_ids']\n        self.attention_mask = tokenized['attention_mask']\n        \n        self.random_indices = None\n        if self.mode == \"forget\":\n            self.random_indices = list(range(len(data_texts)))\n            random.shuffle(self.random_indices)\n\n    def __len__(self):\n        return len(self.data_texts)\n\n    def __getitem__(self, idx):\n        input_ids = self.input_ids[idx].clone()\n        attention_mask = self.attention_mask[idx].clone()\n        sample = {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n        }\n\n        if self.mode == \"forget\":\n            labels_fgt = input_ids.clone()\n            rnd_idx = self.random_indices[idx % len(self.random_indices)]\n            labels_rnd = self.input_ids[rnd_idx].clone()\n            sample.update({\n                'labels_fgt': labels_fgt,\n                'labels_rnd': labels_rnd,\n            })\n        else:\n            sample['labels'] = input_ids.clone()\n        \n        return sample\n\n\nclass SSUDataCollator:\n    \"\"\"Custom data collator supporting mixed forget/retain batches.\"\"\"\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    \n    def __call__(self, features):\n        \n        if not features:\n            raise ValueError(\"Empty features list passed to data collator\")\n        \n        def to_tensor(x):\n            if isinstance(x, torch.Tensor):\n                return x\n            return torch.tensor(x, dtype=torch.long)\n        \n        input_ids = [to_tensor(f['input_ids']) for f in features]\n        attention_mask = [to_tensor(f['attention_mask']) for f in features]\n        \n        batch = {}\n        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n        batch['input_ids'] = pad_sequence(\n            input_ids,\n            batch_first=True,\n            padding_value=pad_token_id,\n        )\n        batch['attention_mask'] = pad_sequence(\n            attention_mask,\n            batch_first=True,\n            padding_value=0,\n        )\n        max_len = batch['input_ids'].size(1)\n        batch_size = len(features)\n        device = batch['input_ids'].device\n        dtype = batch['input_ids'].dtype\n        \n        has_retain = any('labels' in f for f in features)\n        has_forget = any('labels_fgt' in f for f in features)\n        \n        # IMPORTANT FIX: Always create 'labels' field (Trainer expects it)\n        # For forget-only batches, labels will be all -100\n        labels = torch.full((batch_size, max_len), -100, dtype=dtype, device=device)\n        for idx, f in enumerate(features):\n            if 'labels' in f:\n                data = to_tensor(f['labels']).to(device)\n                labels[idx, :data.shape[0]] = data\n        batch['labels'] = labels\n        \n        if has_forget:\n            labels_fgt = torch.full((batch_size, max_len), -100, dtype=dtype, device=device)\n            labels_rnd = torch.full((batch_size, max_len), -100, dtype=dtype, device=device)\n            for idx, f in enumerate(features):\n                if 'labels_fgt' in f:\n                    data_fgt = to_tensor(f['labels_fgt']).to(device)\n                    labels_fgt[idx, :data_fgt.shape[0]] = data_fgt\n                if 'labels_rnd' in f:\n                    data_rnd = to_tensor(f['labels_rnd']).to(device)\n                    labels_rnd[idx, :data_rnd.shape[0]] = data_rnd\n            batch['labels_fgt'] = labels_fgt\n            batch['labels_rnd'] = labels_rnd\n        \n        return batch\n\ndef download_gutenberg_book(book_id, output_dir):\n    \"\"\"Download a book from Project Gutenberg by ID.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    book_file = os.path.join(output_dir, f\"{book_id}.txt\")\n    \n    if os.path.exists(book_file):\n        print(f\"Book {book_id} already exists, skipping download.\")\n        return book_file\n    \n    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n    try:\n        print(f\"Downloading book {book_id} from Project Gutenberg...\")\n        response = requests.get(url, timeout=30)\n        response.raise_for_status()\n        with open(book_file, 'w', encoding='utf-8') as f:\n            f.write(response.text)\n        print(f\"Downloaded book {book_id} successfully.\")\n        return book_file\n    except:\n        url_alt = f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n        try:\n            response = requests.get(url_alt, timeout=30)\n            response.raise_for_status()\n            with open(book_file, 'w', encoding='utf-8') as f:\n                f.write(response.text)\n            print(f\"Downloaded book {book_id} successfully.\")\n            return book_file\n        except Exception as e:\n            print(f\"Warning: Could not download book {book_id}. Using dummy text.\")\n            return None\n\n\ndef load_book_text(book_file):\n    \"\"\"Load and clean text from a book file.\"\"\"\n    if not book_file or not os.path.exists(book_file):\n        return None\n    with open(book_file, 'r', encoding='utf-8', errors='ignore') as f:\n        text = f.read()\n    # Remove Project Gutenberg headers/footers\n    start_markers = [\"*** START OF\", \"***START OF\", \"START OF THE PROJECT\"]\n    end_markers = [\"*** END OF\", \"***END OF\", \"END OF THE PROJECT\"]\n    for marker in start_markers:\n        idx = text.find(marker)\n        if idx != -1:\n            text = text[text.find('\\n', idx) + 1:]\n            break\n    for marker in end_markers:\n        idx = text.find(marker)\n        if idx != -1:\n            text = text[:idx]\n            break\n    text = ' '.join(text.split())\n    return text\n\n\ndef get_all_books_for_initial_finetuning():\n    \"\"\"Downloads all books for initial fine-tuning (D_f) - makes model memorize them.\"\"\"\n    print(\"\\n=== Downloading all books for initial fine-tuning (D_f) ===\")\n    all_books_dir = os.path.join(Config.DATA_DIR, \"all_books\")\n    os.makedirs(all_books_dir, exist_ok=True)\n    \n    book_texts = []\n    for book_id in Config.ALL_BOOK_IDS:\n        book_file = download_gutenberg_book(book_id, all_books_dir)\n        if book_file:\n            text = load_book_text(book_file)\n            if text and len(text) > 1000:\n                book_texts.append(text)\n                print(f\"Loaded book {book_id} ({len(text)} chars)\")\n    \n    if not book_texts:\n        print(\"Warning: No books downloaded. Using dummy text.\")\n        book_texts = [DUMMY_BOOK_TEXT]\n    \n    return book_texts\n\n\ndef get_unlearning_datasets():\n    \"\"\"Generates sequential datasets D_f^1, D_f^2, ... for each time step.\"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    datasets = []\n    \n    for t in range(Config.NUM_UNLEARNING_STEPS):\n        print(f\"\\n--- Preparing dataset D_f^{t+1} for time step {t+1} ---\")\n        \n        if Config.USE_REAL_BOOKS:\n            # Download books for this specific time step\n            time_step_dir = os.path.join(Config.DATA_DIR, f\"time_step_{t+1}\")\n            os.makedirs(time_step_dir, exist_ok=True)\n            \n            book_ids = Config.GUTENBERG_BOOK_IDS.get(t + 1, [])\n            book_texts = []\n            for book_id in book_ids:\n                book_file = download_gutenberg_book(book_id, time_step_dir)\n                if book_file:\n                    text = load_book_text(book_file)\n                    if text and len(text) > 1000:\n                        book_texts.append(text)\n                        print(f\"Loaded book {book_id} for step {t+1}\")\n            if not book_texts:\n                print(f\"Warning: No valid books for step {t+1}. Using dummy text.\")\n                book_texts = [DUMMY_BOOK_TEXT]\n        else:\n            book_texts = [DUMMY_BOOK_TEXT]\n        \n        all_chunks = []\n        for book_text in book_texts:\n            chunks = generate_simulated_data(\n                book_text,\n                Config.CHUNK_SIZE,\n                Config.NUM_CHUNKS_PER_STEP // len(book_texts) + 1,\n                Config.TOKENIZER_NAME\n            )\n            all_chunks.extend(chunks)\n        \n        data_t = all_chunks[:Config.NUM_CHUNKS_PER_STEP]\n        print(f\"Created {len(data_t)} chunks for time step {t+1}\")\n        \n        dataset_t = SequentialUnlearningDataset(tokenizer, data_t)\n        datasets.append(dataset_t)\n        \n    return datasets\n\n\ndef get_retention_dataset():\n    \"\"\"Generates retention dataset D_nor (non-targeted data to keep).\"\"\"\n    if not Config.USE_RETENTION_DATA:\n        return None\n    \n    print(\"\\n=== Preparing retention dataset D_nor ===\")\n    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n    \n    # Use some books that are NOT in the unlearning set\n    retention_book_ids = [1232, 145, 76, 2591, 30254, 844, 345, 520, 6130, 174][:Config.NUM_RETENTION_BOOKS]\n    retention_dir = os.path.join(Config.DATA_DIR, \"retention_books\")\n    os.makedirs(retention_dir, exist_ok=True)\n    \n    all_chunks = []\n    for book_id in retention_book_ids:\n        if book_id not in Config.ALL_BOOK_IDS:  # Don't use books from D_f\n            book_file = download_gutenberg_book(book_id, retention_dir)\n            if book_file:\n                text = load_book_text(book_file)\n                if text and len(text) > 1000:\n                    chunks = generate_simulated_data(\n                        text,\n                        Config.CHUNK_SIZE,\n                        Config.NUM_RETENTION_CHUNKS // Config.NUM_RETENTION_BOOKS + 1,\n                        Config.TOKENIZER_NAME\n                    )\n                    all_chunks.extend(chunks)\n    \n    if not all_chunks:\n        print(\"Warning: No retention books downloaded. Using dummy text.\")\n        all_chunks = generate_simulated_data(\n            DUMMY_BOOK_TEXT,\n            Config.CHUNK_SIZE,\n            Config.NUM_RETENTION_CHUNKS,\n            Config.TOKENIZER_NAME\n        )\n    \n    retention_chunks = all_chunks[:Config.NUM_RETENTION_CHUNKS]\n    print(f\"Created {len(retention_chunks)} retention chunks\")\n    \n    return SequentialUnlearningDataset(tokenizer, retention_chunks, mode=\"retain\")\n\nprint(\"Data utilities loaded!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:34.683509Z","iopub.execute_input":"2025-11-24T18:15:34.684133Z","iopub.status.idle":"2025-11-24T18:15:34.722805Z","shell.execute_reply.started":"2025-11-24T18:15:34.684110Z","shell.execute_reply":"2025-11-24T18:15:34.721809Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Data utilities loaded!\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"class SSUTrainer(Trainer):\n    \"\"\"Custom Trainer implementing SSU + retention loss with Weight Saliency.\"\"\"\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        \"\"\"Compute mixed loss for forget (SSU) and retain (standard LM) batches.\"\"\"\n        inputs_copy = inputs.copy()\n        labels = inputs_copy.pop('labels', None)\n        labels_fgt = inputs_copy.pop('labels_fgt', None)\n        labels_rnd = inputs_copy.pop('labels_rnd', None)\n        \n        loss_total = 0.0\n        components = 0\n        retain_outputs = None\n        forget_outputs = None\n        \n        # Retention loss (standard causal LM)\n        if labels is not None and labels.ne(-100).any():\n            retain_outputs = model(**inputs_copy, labels=labels)\n            loss_total += retain_outputs.loss\n            components += 1\n        \n        # SSU forgetting losses\n        has_forget = (\n            labels_fgt is not None and labels_rnd is not None and\n            (labels_fgt.ne(-100).any() or labels_rnd.ne(-100).any())\n        )\n        if has_forget:\n            outputs_fgt = model(**inputs_copy, labels=labels_fgt)\n            outputs_rnd = model(**inputs_copy, labels=labels_rnd)\n            forget_outputs = outputs_fgt\n            loss_total += Config.EPSILON_1 * outputs_fgt.loss + Config.EPSILON_2 * outputs_rnd.loss\n            components += 1\n        \n        if components == 0:\n            raise ValueError(\"No valid labels provided for retain or forget samples.\")\n        \n        loss = loss_total / components\n        outputs_to_return = forget_outputs or retain_outputs\n        return (loss, outputs_to_return) if return_outputs else loss\n\n    def optimizer_step(self):\n        \"\"\"Override optimizer_step to apply weight saliency masking.\"\"\"\n        # Apply weight saliency mask before optimizer step\n        if self.accelerator.sync_gradients:\n            for name, param in self.model.named_parameters():\n                if param.grad is not None and param.requires_grad:\n                    if \"lora\" in name.lower():\n                        grad = param.grad.data\n                        \n                        # Saliency Mask: m_s = I(|grad| >= gamma)\n                        m_s = (grad.abs() >= Config.GAMMA).float()\n                        \n                        # Apply mask to gradients (only update parameters with high saliency)\n                        param.grad.data = grad * m_s\n        \n        # Call parent optimizer_step to perform the actual update\n        super().optimizer_step()\n\n\ndef create_lora_model(model):\n    \"\"\"Adds LoRA adapters to the base model.\"\"\"\n    peft_config = LoraConfig(\n        r=Config.LORA_R,\n        lora_alpha=Config.LORA_ALPHA,\n        lora_dropout=Config.LORA_DROPOUT,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=Config.TARGET_MODULES,\n    )\n    return get_peft_model(model, peft_config)\n\n\ndef apply_task_vector_negation(base_model, fine_tuned_model, name_prefix):\n    \"\"\"Task Vector Negation: theta_u^t = 2 * theta_u^{t-1} - theta_ft^t\"\"\"\n    print(f\"\\n--- Applying Task Vector Negation for {name_prefix} ---\")\n    \n    device = next(base_model.parameters()).device\n    new_unlearned_model = base_model.__class__(config=base_model.config).to(device)\n    \n    base_state = base_model.state_dict()\n    ft_state = fine_tuned_model.state_dict()\n    \n    new_state = {}\n    for name, param in new_unlearned_model.named_parameters():\n        if name in base_state and name in ft_state:\n            new_state[name] = 2 * base_state[name] - ft_state[name]\n        else:\n            new_state[name] = base_state.get(name, param.data)\n    \n    new_unlearned_model.load_state_dict(new_state, strict=False)\n    print(f\"Task Vector Negation complete for {name_prefix}.\")\n    return new_unlearned_model\n\nprint(\"SSU model utilities loaded!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:34.723863Z","iopub.execute_input":"2025-11-24T18:15:34.724189Z","iopub.status.idle":"2025-11-24T18:15:34.737660Z","shell.execute_reply.started":"2025-11-24T18:15:34.724146Z","shell.execute_reply":"2025-11-24T18:15:34.736754Z"},"trusted":true},"outputs":[{"name":"stdout","text":"SSU model utilities loaded!\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"## 9. Evaluation Utilities\ndef _prepare_prompt_pairs(tokenizer, book_text: str, prompt_tokens: int = 64, continuation_tokens: int = 64, max_pairs: int = 16):\n    tokenized = tokenizer(\n        book_text,\n        return_tensors='pt',\n        truncation=False,\n        add_special_tokens=False,\n    )['input_ids'][0]\n    total_needed = prompt_tokens + continuation_tokens\n    pairs = []\n    for start in range(0, max(0, len(tokenized) - total_needed), total_needed):\n        prompt_ids = tokenized[start:start + prompt_tokens]\n        cont_ids = tokenized[start + prompt_tokens:start + total_needed]\n        if len(prompt_ids) < prompt_tokens or len(cont_ids) < continuation_tokens:\n            continue\n        prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n        cont_text = tokenizer.decode(cont_ids, skip_special_tokens=True)\n        pairs.append((prompt_text, cont_text))\n        if len(pairs) >= max_pairs:\n            break\n    return pairs\n\n\ndef _ngram_overlap(reference: str, hypothesis: str, n: int = 4) -> float:\n    ref_tokens = reference.split()\n    hyp_tokens = hypothesis.split()\n    if len(ref_tokens) < n or len(hyp_tokens) < n:\n        return 0.0\n    ref_ngrams = {tuple(ref_tokens[i:i + n]) for i in range(len(ref_tokens) - n + 1)}\n    hyp_ngrams = {tuple(hyp_tokens[i:i + n]) for i in range(len(hyp_tokens) - n + 1)}\n    if not ref_ngrams or not hyp_ngrams:\n        return 0.0\n    return len(ref_ngrams & hyp_ngrams) / len(ref_ngrams)\n\n\ndef evaluate_regurgitation(model, tokenizer, book_text: str, max_pairs: int = 10, prompt_tokens: int = 64, continuation_tokens: int = 64) -> Dict[str, float]:\n    \"\"\"Estimate regurgitation via 4-gram overlap between generated text and the book.\"\"\"\n    model_device = next(model.parameters()).device\n    pairs = _prepare_prompt_pairs(tokenizer, book_text, prompt_tokens, continuation_tokens, max_pairs)\n    if not pairs:\n        return {\"avg_overlap\": 0.0, \"num_pairs\": 0}\n    overlaps = []\n    model.eval()\n    with torch.no_grad():\n        for prompt, reference in pairs:\n            inputs = tokenizer(prompt, return_tensors='pt').to(model_device)\n            generated = model.generate(\n                **inputs,\n                max_new_tokens=continuation_tokens,\n                do_sample=False,\n            )\n            gen_continuation = generated[0][inputs['input_ids'].shape[1]:]\n            hypothesis = tokenizer.decode(gen_continuation, skip_special_tokens=True)\n            overlaps.append(_ngram_overlap(reference, hypothesis, n=4))\n    avg_overlap = float(sum(overlaps) / len(overlaps)) if overlaps else 0.0\n    return {\"avg_overlap\": avg_overlap, \"num_pairs\": len(overlaps)}\n\n\ndef _normalize_corpus(text_source: Sequence[str] | str, tokenizer, max_samples: int = 32) -> str:\n    if isinstance(text_source, str):\n        return text_source\n    if isinstance(text_source, SequentialUnlearningDataset):\n        samples = [tokenizer.decode(text_source.input_ids[i], skip_special_tokens=True) for i in range(min(len(text_source), max_samples))]\n        return \"\\n\\n\".join(samples)\n    if isinstance(text_source, list):\n        return \"\\n\\n\".join(text_source[:max_samples])\n    raise ValueError(\"Unsupported text source type for perplexity evaluation\")\n\n\ndef evaluate_perplexity(model, tokenizer, text_source, stride: int = 256) -> float:\n    \"\"\"Compute perplexity on the provided text corpus.\"\"\"\n    corpus = _normalize_corpus(text_source, tokenizer)\n    tokenized = tokenizer(\n        corpus,\n        return_tensors='pt',\n        truncation=False,\n        add_special_tokens=False,\n    )['input_ids'][0]\n    if tokenized.size(0) <= 1:\n        return float('inf')\n    model_device = next(model.parameters()).device\n    nll_sum = 0.0\n    token_count = 0\n    model.eval()\n    with torch.no_grad():\n        for start in range(0, tokenized.size(0) - 1, stride):\n            end = min(start + stride, tokenized.size(0))\n            input_ids = tokenized[start:end].unsqueeze(0).to(model_device)\n            labels = input_ids.clone()\n            outputs = model(input_ids=input_ids, labels=labels)\n            n_tokens = labels.size(1) - 1\n            if n_tokens <= 0:\n                continue\n            nll_sum += outputs.loss.item() * n_tokens\n            token_count += n_tokens\n    if token_count == 0:\n        return float('inf')\n    return float(math.exp(nll_sum / token_count))\n\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:34.738726Z","iopub.execute_input":"2025-11-24T18:15:34.738991Z","iopub.status.idle":"2025-11-24T18:15:34.756528Z","shell.execute_reply.started":"2025-11-24T18:15:34.738973Z","shell.execute_reply":"2025-11-24T18:15:34.755533Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## 6. Model Loading with Retry Logic\n","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport time\n\n\ndef load_model_with_retry(model_name, max_retries=3, retry_delay=5):\n    \"\"\"Load model with automatic retry on network errors.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            print(f\"Loading model (attempt {attempt + 1}/{max_retries})...\")\n            \n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n            if tokenizer.pad_token is None:\n                tokenizer.pad_token = tokenizer.eos_token\n            \n            # Determine device\n            if torch.cuda.is_available():\n                device = \"cuda\"\n                dtype = torch.bfloat16\n            else:\n                device = \"cpu\"\n                dtype = torch.float32\n            \n            base_model = AutoModelForCausalLM.from_pretrained(\n                model_name, \n                torch_dtype=dtype,\n                device_map=device,  # Use single device instead of \"auto\"\n                trust_remote_code=True,\n                attn_implementation=\"eager\",\n            )\n            \n            # Ensure model is on the correct device\n            base_model = base_model.to(device)\n            \n            print(f\"Successfully loaded {model_name}!\")\n            return base_model, tokenizer\n            \n        except Exception as e:\n            error_msg = str(e)\n            print(f\"Attempt {attempt + 1} failed: {error_msg[:200]}...\")\n            \n            if attempt < max_retries - 1:\n                if \"IncompleteRead\" in error_msg or \"Connection\" in error_msg or \"timeout\" in error_msg.lower():\n                    print(f\"Network error detected. Retrying in {retry_delay} seconds...\")\n                    time.sleep(retry_delay)\n                    retry_delay *= 2  # Exponential backoff\n                else:\n                    print(\"Non-network error. Retrying...\")\n                    time.sleep(2)\n            else:\n                print(\"\\nAll retry attempts failed!\")\n                print(\"\\nTROUBLESHOOTING:\")\n                print(\"1. Check your internet connection\")\n                print(\"2. Try a smaller model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n                print(\"3. For gated models, ensure HF_TOKEN is set\")\n                raise\n    \n    return None, None\n\n\n# Load the model\nprint(f\"Loading base model: {Config.MODEL_NAME}\")\nbase_model, tokenizer = load_model_with_retry(Config.MODEL_NAME)\nbase_model.requires_grad_(False)\nprint(\"Model loaded and frozen!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:34.757557Z","iopub.execute_input":"2025-11-24T18:15:34.757848Z","iopub.status.idle":"2025-11-24T18:15:38.925111Z","shell.execute_reply.started":"2025-11-24T18:15:34.757822Z","shell.execute_reply":"2025-11-24T18:15:38.924338Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading base model: google/gemma-3-1b-it\nLoading model (attempt 1/3)...\nSuccessfully loaded google/gemma-3-1b-it!\nModel loaded and frozen!\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"## 7. Main SSU Pipeline\n","metadata":{}},{"cell_type":"code","source":"import os\nfrom transformers import TrainingArguments\n\ndef initial_finetuning(model, tokenizer, all_books_texts):\n    \"\"\"\n    Step 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n    This is what the paper does BEFORE unlearning.\n    \"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n    print(\"=\"*60)\n    print(\"Fine-tuning vanilla model on all copyrighted books to memorize them...\")\n    \n    # Generate chunks from all books\n    all_chunks = []\n    for book_text in all_books_texts:\n        chunks = generate_simulated_data(\n            book_text,\n            Config.CHUNK_SIZE,\n            Config.NUM_CHUNKS_PER_STEP * Config.NUM_UNLEARNING_STEPS // len(all_books_texts) + 1,\n            Config.TOKENIZER_NAME\n        )\n        all_chunks.extend(chunks)\n    \n    print(f\"Created {len(all_chunks)} chunks from all books\")\n    \n    # For initial fine-tuning, use standard dataset (not SSU dual labels)\n    from torch.utils.data import Dataset as TorchDataset\n    class StandardDataset(TorchDataset):\n        def __init__(self, tokenizer, data_texts):\n            tokenized = tokenizer(\n                data_texts,\n                truncation=True,\n                padding=\"max_length\",\n                max_length=Config.CHUNK_SIZE,\n                return_tensors='pt'\n            )\n            self.input_ids = tokenized['input_ids']\n            self.attention_mask = tokenized['attention_mask']\n        \n        def __len__(self):\n            return len(self.input_ids)\n        \n        def __getitem__(self, idx):\n            return {\n                'input_ids': self.input_ids[idx].clone(),\n                'attention_mask': self.attention_mask[idx].clone(),\n                'labels': self.input_ids[idx].clone()  # Standard labels for next token prediction\n            }\n    \n    initial_dataset = StandardDataset(tokenizer, all_chunks)\n    \n    # Ensure model is on correct device before creating LoRA\n    device = next(model.parameters()).device\n    if device.type == \"meta\" or str(device) == \"meta\":\n        # If model is on meta device, move to actual device\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        model = model.to(device)\n    else:\n        # Convert device object to string if needed\n        device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n    \n    # Create LoRA model for initial fine-tuning (PEFT handles device automatically)\n    if hasattr(model, \"enable_input_require_grads\"):\n        model.enable_input_require_grads()\n    else:\n        def make_inputs_require_grad(module, input, output):\n            output.requires_grad_(True)\n        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    \n    lora_model = create_lora_model(model)\n    lora_model.print_trainable_parameters()\n    \n    # Disable cache for gradient checkpointing\n    if hasattr(lora_model.config, \"use_cache\"):\n        lora_model.config.use_cache = False\n    \n    # PEFT models inherit device from base model, no need to call .to()\n    \n    training_args = TrainingArguments(\n        output_dir=f\"{Config.OUTPUT_DIR}/initial_ft_checkpoints\",\n        per_device_train_batch_size=Config.BATCH_SIZE,\n        gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n        warmup_steps=10,\n        learning_rate=Config.LEARNING_RATE,\n        num_train_epochs=Config.NUM_EPOCHS_INITIAL_FT,\n        logging_steps=10,\n        save_strategy=\"no\",\n        report_to=\"none\",\n        fp16=False,\n        bf16=torch.cuda.is_available() and device == \"cuda\",\n        dataloader_pin_memory=False,  # Fix device issues\n        label_names=[\"labels\"],\n        gradient_checkpointing=True,  # Enable gradient checkpointing\n    )\n    \n    # Use standard Trainer for initial fine-tuning (not SSU)\n    from transformers import Trainer, DataCollatorForLanguageModeling\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,  # Causal LM, not masked LM\n    )\n    \n    trainer = Trainer(\n        model=lora_model,\n        args=training_args,\n        train_dataset=initial_dataset,\n        processing_class=tokenizer,\n        data_collator=data_collator,\n    )\n    \n    print(\"Starting initial fine-tuning...\")\n    trainer.train()\n    \n    # Merge LoRA weights into base model\n    memorized_model = lora_model.merge_and_unload()\n    memorized_model.requires_grad_(False)\n    \n    print(\"Initial fine-tuning complete. Model has memorized all books.\")\n    return memorized_model\n\n\ndef run_initial_finetuning():\n    \"\"\"STEP 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n    \n    Run this cell once to create the memorized model. After this completes,\n    you can run the sequential unlearning steps without re-running this.\n    \"\"\"\n    # Ensure all required dependencies are available\n    missing = []\n    try:\n        _ = Config.OUTPUT_DIR\n    except NameError:\n        missing.append(\"Config (cell 4)\")\n    \n    try:\n        _ = base_model\n    except NameError:\n        missing.append(\"base_model (cell 11)\")\n    \n    try:\n        _ = tokenizer\n    except NameError:\n        missing.append(\"tokenizer (cell 11)\")\n    \n    if missing:\n        raise NameError(\n            f\"The following are not defined: {', '.join(missing)}. \"\n            f\"Please run the required cells first to set up the dependencies.\"\n        )\n    \n    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n    \n    # Check if memorized model already exists\n    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n    if os.path.exists(memorized_model_path):\n        print(f\"Memorized model already exists at {memorized_model_path}\")\n        print(\"Loading existing memorized model...\")\n        from transformers import AutoModelForCausalLM\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n        memorized_model = AutoModelForCausalLM.from_pretrained(\n            memorized_model_path,\n            torch_dtype=dtype,\n            device_map=device\n        )\n        memorized_model.requires_grad_(False)\n        print(\"Loaded existing memorized model.\")\n        return memorized_model\n    \n    # STEP 0: Initial fine-tuning on all books (D_f)\n    print(\"\\n\" + \"=\"*60)\n    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n    print(\"=\"*60)\n    all_books_texts = get_all_books_for_initial_finetuning()\n    memorized_model = initial_finetuning(base_model, tokenizer, all_books_texts)\n    \n    # Save the memorized model\n    memorized_model.save_pretrained(memorized_model_path)\n    tokenizer.save_pretrained(memorized_model_path)\n    print(f\"\\nMemorized model saved to {memorized_model_path}\")\n    \n    return memorized_model\n\n\n# Run initial fine-tuning (only need to run this once)\nmemorized_model = run_initial_finetuning()\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:38.925992Z","iopub.execute_input":"2025-11-24T18:15:38.926278Z","iopub.status.idle":"2025-11-24T18:15:39.836972Z","shell.execute_reply.started":"2025-11-24T18:15:38.926252Z","shell.execute_reply":"2025-11-24T18:15:39.836294Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Memorized model already exists at ssu_unlearned_models/memorized_model\nLoading existing memorized model...\nLoaded existing memorized model.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"## 8. Sequential Unlearning Steps\n\ndef run_sequential_unlearning(start_step=1, end_step=None, general_validation_text=None, run_evaluation=True):\n    # Suppress dynamo errors\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n    \"\"\"Run sequential unlearning steps with optional evaluation reporting.\"\"\"\n    # Ensure all required dependencies are available\n    missing = []\n    try:\n        _ = Config.OUTPUT_DIR\n    except NameError:\n        missing.append(\"Config (cell 4)\")\n    \n    try:\n        _ = tokenizer\n    except NameError:\n        missing.append(\"tokenizer (cell 11)\")\n    \n    if missing:\n        raise NameError(\n            f\"The following are not defined: {', '.join(missing)}. \"\n            f\"Please run the required cells first.\"\n        )\n    \n    if end_step is None:\n        end_step = Config.NUM_UNLEARNING_STEPS\n    \n    general_validation_text = general_validation_text or DUMMY_BOOK_TEXT\n    \n    # Load or start from memorized model (check if file exists, not variable)\n    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n    if not os.path.exists(memorized_model_path):\n        raise FileNotFoundError(\n            f\"Memorized model not found at {memorized_model_path}. \"\n            f\"Please run cell 13 (initial fine-tuning) first.\"\n        )\n    \n    # Determine starting model\n    if start_step == 1:\n        # Load memorized model\n        from transformers import AutoModelForCausalLM\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n        current_model = AutoModelForCausalLM.from_pretrained(\n            memorized_model_path,\n            torch_dtype=dtype,\n            device_map=device\n        )\n        current_model.requires_grad_(False)\n        print(f\"Starting from memorized model for step 1\")\n    else:\n        # Load model from previous step\n        prev_step_path = f\"{Config.OUTPUT_DIR}/step_{start_step-1}_unlearned_model\"\n        if not os.path.exists(prev_step_path):\n            raise FileNotFoundError(\n                f\"Previous step model not found at {prev_step_path}. \"\n                f\"Please run steps 1 to {start_step-1} first.\"\n            )\n        from transformers import AutoModelForCausalLM\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n        current_model = AutoModelForCausalLM.from_pretrained(\n            prev_step_path,\n            torch_dtype=dtype,\n            device_map=device\n        )\n        current_model.requires_grad_(False)\n        print(f\"Starting from step {start_step-1} model for step {start_step}\")\n    \n    # Load sequential datasets for unlearning\n    unlearning_datasets = get_unlearning_datasets()\n    print(f\"\\nGenerated {len(unlearning_datasets)} sequential unlearning datasets.\")\n    \n    # Get retention dataset (D_nor) if needed\n    retention_dataset = get_retention_dataset()\n    \n    all_target_book_ids = [book_id for ids in Config.GUTENBERG_BOOK_IDS.values() for book_id in ids]\n    evaluation_log = {\"baseline\": {}, \"steps\": []}\n    eval_book_cache = {}\n    eval_book_dir = os.path.join(Config.DATA_DIR, \"evaluation_books\")\n    os.makedirs(eval_book_dir, exist_ok=True)\n    \n    def get_book_text_for_eval(book_id):\n        if book_id in eval_book_cache:\n            return eval_book_cache[book_id]\n        book_file = download_gutenberg_book(book_id, eval_book_dir)\n        text = load_book_text(book_file) if book_file else None\n        if not text or len(text) < 1000:\n            text = DUMMY_BOOK_TEXT\n        eval_book_cache[book_id] = text\n        return text\n    \n    if run_evaluation:\n        print(\"\\n=== Baseline Evaluation ===\")\n        baseline_regurg = {}\n        for book_id in all_target_book_ids:\n            metrics = evaluate_regurgitation(current_model, tokenizer, get_book_text_for_eval(book_id))\n            baseline_regurg[book_id] = metrics\n            print(f\"Book {book_id} baseline overlap: {metrics['avg_overlap']:.4f}\")\n        baseline_perplexity = {}\n        if retention_dataset is not None:\n            baseline_perplexity['retention'] = evaluate_perplexity(current_model, tokenizer, retention_dataset)\n            print(f\"Retention baseline perplexity: {baseline_perplexity['retention']:.2f}\")\n        baseline_perplexity['general'] = evaluate_perplexity(current_model, tokenizer, general_validation_text)\n        print(f\"General baseline perplexity: {baseline_perplexity['general']:.2f}\")\n        evaluation_log['baseline'] = {\n            'regurgitation': baseline_regurg,\n            'perplexity': baseline_perplexity,\n        }\n    \n    # STEP 1-N: Sequential Unlearning Loop\n    for t in range(start_step - 1, min(end_step, Config.NUM_UNLEARNING_STEPS)):\n        print(f\"\\n{'='*60}\")\n        print(f\"UNLEARNING STEP {t+1} (Unlearning D_f^{t+1})\")\n        print(f\"{'='*60}\")\n        \n        dataset_t = unlearning_datasets[t]\n        step_prefix = f\"step_{t+1}\"\n        if retention_dataset is not None and Config.USE_RETENTION_DATA:\n            train_dataset = ConcatDataset([dataset_t, retention_dataset])\n        else:\n            train_dataset = dataset_t\n        \n        # D_prev: Previously unlearned books (for t > 1)\n        # In the paper, this ensures model doesn't re-learn old content\n        if t > 0 and Config.USE_RETENTION_DATA:\n            print(f\"Note: D_prev includes books from steps 1-{t}\")\n        \n        # Fine-Tuning Stage\n        print(f\"Preparing LoRA model for fine-tuning on D_f^{t+1}...\")\n        \n        # Ensure model is on correct device\n        device = next(current_model.parameters()).device\n        if device.type == \"meta\" or str(device) == \"meta\":\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            current_model = current_model.to(device)\n        else:\n            # Convert device object to string if needed\n            device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n        \n        if hasattr(current_model, \"enable_input_require_grads\"):\n            current_model.enable_input_require_grads()\n        else:\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n            current_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n        \n        lora_model = create_lora_model(current_model)\n        # PEFT models inherit device from base model, no need to call .to()\n        lora_model.print_trainable_parameters()\n        \n        # Disable cache for gradient checkpointing\n        if hasattr(lora_model.config, \"use_cache\"):\n            lora_model.config.use_cache = False\n        \n        training_args = TrainingArguments(\n            output_dir=f\"{Config.OUTPUT_DIR}/{step_prefix}_ft_checkpoints\",\n            per_device_train_batch_size=Config.BATCH_SIZE,\n            gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n            warmup_steps=10,\n            learning_rate=Config.LEARNING_RATE,\n            num_train_epochs=Config.NUM_EPOCHS_FT,\n            logging_steps=10,\n            save_strategy=\"no\",\n            report_to=\"none\",\n            fp16=False,\n            bf16=torch.cuda.is_available() and device == \"cuda\",\n            dataloader_pin_memory=False,  # Fix device issues\n            label_names=[\"labels\"],\n            gradient_checkpointing=True,  # Enable gradient checkpointing\n            remove_unused_columns=False,  # Keep custom labels\n            \n        )\n\n        # Use custom data collator that preserves labels_fgt and labels_rnd\n        data_collator = SSUDataCollator(tokenizer=tokenizer)\n        \n        trainer = SSUTrainer(\n            model=lora_model,\n            args=training_args,\n            train_dataset=train_dataset,\n            processing_class=tokenizer,\n            data_collator=data_collator,\n        )\n\n        print(f\"Starting fine-tuning with SSU Loss for {step_prefix}...\")\n        trainer.train()\n        \n        # Task Vector Negation Stage: theta_new = theta_old - Delta_LoRA\n        print(f\"\\n--- Applying Task Vector Negation for {step_prefix} ---\")\n        \n        # 1. Negate LoRA weights: W_new = W_old - (W_ft - W_old) = W_old - W_lora\n        # We achieve this by multiplying LoRA weights by -1, then merging.\n        with torch.no_grad():\n            for name, param in lora_model.named_parameters():\n                if \"lora\" in name:\n                    param.data = -1 * param.data\n        \n        print(\"LoRA weights negated.\")\n        \n        # 2. Merge negated weights into base model\n        current_model = lora_model.merge_and_unload()\n        current_model.requires_grad_(False)\n        \n        print(f\"Task Vector Negation complete for {step_prefix}.\")\n        \n        # Save the unlearned model\n        current_model.save_pretrained(f\"{Config.OUTPUT_DIR}/{step_prefix}_unlearned_model\")\n        tokenizer.save_pretrained(f\"{Config.OUTPUT_DIR}/{step_prefix}_unlearned_model\")\n        print(f\"Unlearned model {step_prefix} saved.\")\n        \n        if run_evaluation:\n            step_metrics = {\n                'step': t + 1,\n                'current_books': {},\n                'future_books': {},\n                'perplexity': {},\n            }\n            current_book_ids = Config.GUTENBERG_BOOK_IDS.get(t + 1, [])\n            for book_id in current_book_ids:\n                metrics = evaluate_regurgitation(current_model, tokenizer, get_book_text_for_eval(book_id))\n                step_metrics['current_books'][book_id] = metrics\n                print(f\"Step {t+1} current book {book_id} overlap: {metrics['avg_overlap']:.4f}\")\n            future_book_ids = []\n            for future_step in range(t + 2, Config.NUM_UNLEARNING_STEPS + 1):\n                future_book_ids.extend(Config.GUTENBERG_BOOK_IDS.get(future_step, []))\n            for book_id in future_book_ids:\n                metrics = evaluate_regurgitation(current_model, tokenizer, get_book_text_for_eval(book_id))\n                step_metrics['future_books'][book_id] = metrics\n                print(f\"Step {t+1} future book {book_id} overlap: {metrics['avg_overlap']:.4f}\")\n            if retention_dataset is not None:\n                retention_ppl = evaluate_perplexity(current_model, tokenizer, retention_dataset)\n                step_metrics['perplexity']['retention'] = retention_ppl\n                print(f\"Step {t+1} retention perplexity: {retention_ppl:.2f}\")\n            general_ppl = evaluate_perplexity(current_model, tokenizer, general_validation_text)\n            step_metrics['perplexity']['general'] = general_ppl\n            print(f\"Step {t+1} general perplexity: {general_ppl:.2f}\")\n            evaluation_log['steps'].append(step_metrics)\n        \n        # current_model is already updated via merge_and_unload\n\n    # Final Evaluation\n    print(\"\\n\" + \"=\"*60)\n    print(\"SEQUENTIAL UNLEARNING COMPLETE\")\n    print(\"=\"*60)\n    final_step = min(end_step, Config.NUM_UNLEARNING_STEPS)\n    print(f\"Final Unlearned Model: {Config.OUTPUT_DIR}/step_{final_step}_unlearned_model\")\n    \n    # Test generation\n    prompt = \"The quick brown fox\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    print(\"\\nTesting generation with final unlearned model...\")\n    \n    # Reload model to remove training hooks/state that interfere with inference\n    print(\"Reloading model for clean inference...\")\n    del current_model\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n    \n    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n    # Re-determine device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \n    current_model = AutoModelForCausalLM.from_pretrained(\n        f\"{Config.OUTPUT_DIR}/step_{final_step}_unlearned_model\",\n        torch_dtype=dtype,\n        device_map=device,\n        attn_implementation=\"eager\"\n    )\n    current_model.requires_grad_(False)\n    current_model.eval()\n    \n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    # Completely disable torch.compile to avoid compilation errors\n    torch._dynamo.reset()\n    # This completely disables dynamo/torch.compile\n    original_disable_state = torch._dynamo.config.disable\n    torch._dynamo.config.disable = True\n    \n    try:\n        with torch.no_grad():\n            output_tokens = current_model.generate(\n                **inputs, \n                max_new_tokens=30, \n                do_sample=True, \n                top_p=0.9, \n                temperature=0.7\n            )\n    finally:\n        # Restore original state\n        torch._dynamo.config.disable = original_disable_state\n    \n    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n    print(f\"Prompt: {prompt}\")\n    print(f\"Generated: {generated_text}\")\n    \n    if run_evaluation:\n        run_sequential_unlearning.last_evaluation = evaluation_log\n    \n    return current_model\n\n\n# Run sequential unlearning steps\n# You can specify start_step and end_step to resume from a specific step\n# Example: run_sequential_unlearning(start_step=2, end_step=3) to run only steps 2-3\nfinal_unlearned_model = run_sequential_unlearning()\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:15:39.839424Z","iopub.execute_input":"2025-11-24T18:15:39.839674Z","iopub.status.idle":"2025-11-24T18:28:02.438577Z","shell.execute_reply.started":"2025-11-24T18:15:39.839656Z","shell.execute_reply":"2025-11-24T18:28:02.437890Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Starting from memorized model for step 1\n\n--- Preparing dataset D_f^1 for time step 1 ---\nBook 1661 already exists, skipping download.\nLoaded book 1661 for step 1\nCreated 50 chunks for time step 1\n\n--- Preparing dataset D_f^2 for time step 2 ---\nBook 1342 already exists, skipping download.\nLoaded book 1342 for step 2\nCreated 50 chunks for time step 2\n\n--- Preparing dataset D_f^3 for time step 3 ---\nBook 11 already exists, skipping download.\nLoaded book 11 for step 3\nCreated 50 chunks for time step 3\n\nGenerated 3 sequential unlearning datasets.\n\n=== Preparing retention dataset D_nor ===\nBook 1232 already exists, skipping download.\nBook 145 already exists, skipping download.\nBook 76 already exists, skipping download.\nBook 2591 already exists, skipping download.\nBook 30254 already exists, skipping download.\nBook 844 already exists, skipping download.\nBook 345 already exists, skipping download.\nBook 520 already exists, skipping download.\nCreated 20 retention chunks\n\n=== Baseline Evaluation ===\nBook 1661 already exists, skipping download.\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Book 1661 baseline overlap: 0.0077\nBook 1342 already exists, skipping download.\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Book 1342 baseline overlap: 0.0000\nBook 11 already exists, skipping download.\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Book 11 baseline overlap: 0.0000\nRetention baseline perplexity: 101.75\nGeneral baseline perplexity: 18.11\n\n============================================================\nUNLEARNING STEP 1 (Unlearning D_f^1)\n============================================================\nPreparing LoRA model for fine-tuning on D_f^1...\ntrainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\nStarting fine-tuning with SSU Loss for step_1...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n--- Applying Task Vector Negation for step_1 ---\nLoRA weights negated.\nTask Vector Negation complete for step_1.\nUnlearned model step_1 saved.\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Step 1 current book 1661 overlap: 0.0051\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Step 1 future book 1342 overlap: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Step 1 future book 11 overlap: 0.0000\nStep 1 retention perplexity: 102.06\nStep 1 general perplexity: 18.08\n\n============================================================\nUNLEARNING STEP 2 (Unlearning D_f^2)\n============================================================\nNote: D_prev includes books from steps 1-1\nPreparing LoRA model for fine-tuning on D_f^2...\ntrainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\nStarting fine-tuning with SSU Loss for step_2...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n--- Applying Task Vector Negation for step_2 ---\nLoRA weights negated.\nTask Vector Negation complete for step_2.\nUnlearned model step_2 saved.\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Step 2 current book 1342 overlap: 0.0000\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Step 2 future book 11 overlap: 0.0000\nStep 2 retention perplexity: 101.78\nStep 2 general perplexity: 18.04\n\n============================================================\nUNLEARNING STEP 3 (Unlearning D_f^3)\n============================================================\nNote: D_prev includes books from steps 1-2\nPreparing LoRA model for fine-tuning on D_f^3...\ntrainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\nStarting fine-tuning with SSU Loss for step_3...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:190: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5/5 00:41, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\n--- Applying Task Vector Negation for step_3 ---\nLoRA weights negated.\nTask Vector Negation complete for step_3.\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Unlearned model step_3 saved.\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Step 3 current book 11 overlap: 0.0000\nStep 3 retention perplexity: 101.63\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Step 3 general perplexity: 18.00\n\n============================================================\nSEQUENTIAL UNLEARNING COMPLETE\n============================================================\nFinal Unlearned Model: ssu_unlearned_models/step_3_unlearned_model\n\nTesting generation with final unlearned model...\nReloading model for clean inference...\nPrompt: The quick brown fox\nGenerated: The quick brown fox jumps over the lazy dog.\n\nThis is a common example of a pangram, a sentence that contains every letter of the alphabet.  It'\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"## 10. Verification & Testing\n\n**Automated checks** ensure the core SSU components work before running long jobs:\n- Dataset construction keeps the correct masking for `labels`, `labels_fgt`, and `labels_rnd`.\n- The custom trainer mixes retention and forgetting losses without shape errors.\n- A miniature pipeline run (concatenated forget + retain batches) executes an optimizer step without crashing.\n\n**Manual checks** after training runs:\n- Generate from a known passage of a removed book and confirm the model no longer regurgitates it.\n- Measure perplexity on `D_nor` before vs. after each step to ensure general capability stays stable.\n","metadata":{}},{"cell_type":"code","source":"# Automated sanity checks for dataset, trainer, and pipeline wiring\n\n\ndef run_automated_tests():\n    tok = tokenizer\n    sample_texts = [\"Sanity sample text \" + str(i) for i in range(4)]\n\n    # Dataset + collator check\n    forget_ds = SequentialUnlearningDataset(tok, sample_texts, mode=\"forget\")\n    retain_ds = SequentialUnlearningDataset(tok, sample_texts, mode=\"retain\")\n    collator = SSUDataCollator(tok)\n    batch = collator([forget_ds[0], retain_ds[0]])\n    assert 'labels' in batch and 'labels_fgt' in batch and 'labels_rnd' in batch\n    assert batch['labels'][0].eq(-100).all(), \"Forget sample should mask retention labels\"\n    assert not batch['labels'][1].eq(-100).all(), \"Retain sample should keep labels\"\n    assert not batch['labels_fgt'][0].eq(-100).all(), \"Forget sample must keep L_fgt\"\n\n    # Trainer loss mixing check\n    class MockModel(torch.nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.losses = [torch.tensor(1.0), torch.tensor(0.4), torch.tensor(0.2)]  # retain, fgt, rnd\n        def forward(self, *args, **kwargs):\n            loss_val = self.losses.pop(0)\n            return SimpleNamespace(loss=loss_val)\n    mock_model = MockModel()\n    args = TrainingArguments(\n        output_dir=os.path.join(Config.OUTPUT_DIR, \"test_runs\"),\n        per_device_train_batch_size=1,\n        num_train_epochs=1,\n        report_to=[],\n        logging_steps=1000,\n    )\n    trainer = SSUTrainer(model=mock_model, args=args, train_dataset=None)\n    dummy_inputs = {\n        'input_ids': torch.ones((1, 4), dtype=torch.long),\n        'attention_mask': torch.ones((1, 4), dtype=torch.long),\n        'labels': torch.ones((1, 4), dtype=torch.long),\n        'labels_fgt': torch.ones((1, 4), dtype=torch.long),\n        'labels_rnd': torch.ones((1, 4), dtype=torch.long),\n    }\n    loss = trainer.compute_loss(mock_model, dummy_inputs)\n    expected = (1.0 + (Config.EPSILON_1 * 0.4 + Config.EPSILON_2 * 0.2)) / 2\n    assert abs(loss.item() - expected) < 1e-6, \"Mixed loss weighting incorrect\"\n\n    # Mini pipeline batch check (no actual training loop)\n    concat_ds = ConcatDataset([forget_ds, retain_ds])\n    trainer = SSUTrainer(\n        model=mock_model,\n        args=args,\n        train_dataset=concat_ds,\n        data_collator=collator,\n    )\n    data_loader = trainer.get_train_dataloader()\n    batch = next(iter(data_loader))\n    assert 'labels' in batch and 'labels_fgt' in batch, \"Combined batch missing labels\"\n\n    print(\"All automated SSU sanity tests passed!\")\n\n# Uncomment to run inline tests\n# run_automated_tests()\n\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:28:02.450235Z","iopub.execute_input":"2025-11-24T18:28:02.450796Z","iopub.status.idle":"2025-11-24T18:28:02.461559Z","shell.execute_reply.started":"2025-11-24T18:28:02.450771Z","shell.execute_reply":"2025-11-24T18:28:02.460622Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## 11. Ablation Study: Removing `D_nor`\n\nTo illustrate the value of retention data, re-run sequential unlearning with `D_nor` disabled.\nYou should observe:\n- Regurgitation on the target books still decreases, proving the SSU losses work.\n- Perplexity on general data collapses, showing catastrophic forgetting without the retention anchor.\n\nAfter the run, compare the perplexity deltas and summarize them using:\n> Including D_nor stabilizes the model and reduces unlearning-induced degradation in general perplexity by **X%** compared to a naive SSU-only objective.\n","metadata":{}},{"cell_type":"code","source":"def run_ablation_without_retention(start_step=1, end_step=None, **kwargs):\n    \"\"\"Helper to rerun SSU without D_nor for ablation studies.\"\"\"\n    original_flag = Config.USE_RETENTION_DATA\n    try:\n        Config.USE_RETENTION_DATA = False\n        print(\"\\n>>> Running ablation: retention data disabled\")\n        return run_sequential_unlearning(\n            start_step=start_step,\n            end_step=end_step,\n            run_evaluation=kwargs.get('run_evaluation', True),\n            general_validation_text=kwargs.get('general_validation_text'),\n        )\n    finally:\n        Config.USE_RETENTION_DATA = original_flag\n\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:28:02.462783Z","iopub.execute_input":"2025-11-24T18:28:02.463065Z","iopub.status.idle":"2025-11-24T18:28:02.483027Z","shell.execute_reply.started":"2025-11-24T18:28:02.463047Z","shell.execute_reply":"2025-11-24T18:28:02.482249Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Quick sanity generation with the latest unlearned model\nif 'final_unlearned_model' in globals():\n    final_unlearned_model.eval()\n    sample_device = next(final_unlearned_model.parameters()).device\n    demo_prompts = [\n        \"The quick brown fox\",\n        \"Once upon a time in a quiet village\",\n    ]\n    for idx, prompt in enumerate(demo_prompts, 1):\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(sample_device)\n        with torch.no_grad():\n            gen_tokens = final_unlearned_model.generate(\n                **inputs,\n                do_sample=True,\n                temperature=0.8,\n                max_new_tokens=80,\n            )\n        gen_text = tokenizer.batch_decode(gen_tokens)[0]\n        print(f\"Prompt {idx}: {prompt}\\nGenerated: {gen_text}\\n\")\nelse:\n    print(\"Run run_sequential_unlearning() first to instantiate final_unlearned_model.\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-24T18:28:02.483898Z","iopub.execute_input":"2025-11-24T18:28:02.484195Z","iopub.status.idle":"2025-11-24T18:28:12.590267Z","shell.execute_reply.started":"2025-11-24T18:28:02.484170Z","shell.execute_reply":"2025-11-24T18:28:12.589408Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Prompt 1: The quick brown fox\nGenerated: <bos>The quick brown fox jumps over the lazy dog.\n\nThis is a classic sentence used to test fonts and keyboards.\n\nIt's a well-known pangram, meaning it contains every letter of the alphabet at least once.\n\nThe sentence is frequently used in typography and testing software.\n\nDo you want to learn more about this sentence?\n<end_of_turn>\n\nPrompt 2: Once upon a time in a quiet village\nGenerated: <bos>Once upon a time in a quiet village nestled beside a sparkling river, lived a young woman named Elara. She was known for her kindness, her bright smile, and her uncanny ability to mend anything broken – from a shattered teacup to a broken heart.\n\nOne day, a traveling merchant arrived in the village, his wagon overflowing with strange and wondrous objects. Amongst them was a beautiful, intricately carved wooden box. Elara felt an\n\n","output_type":"stream"}],"execution_count":32}]}