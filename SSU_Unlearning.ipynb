{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052809e5",
   "metadata": {
    "papermill": {
     "duration": 0.005385,
     "end_time": "2025-11-25T07:51:17.427946",
     "exception": false,
     "start_time": "2025-11-25T07:51:17.422561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stable Sequential Unlearning (SSU) Framework\n",
    "\n",
    "Complete implementation following the paper methodology:\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Initial Fine-tuning (Step 0)**: Fine-tune vanilla model on all copyrighted books (D_f) to make it memorize them\n",
    "2. **Sequential Unlearning (Steps 1-N)**: Unlearn books one at a time using SSU methodology\n",
    "   - Each step unlearns one book (D_f^t)\n",
    "   - Uses composite loss (L_fgt + L_rnd) and weight saliency\n",
    "   - Applies task vector negation\n",
    "\n",
    "## Datasets:\n",
    "- **D_f**: All copyrighted books (10 books from Project Gutenberg)\n",
    "- **D_f^t**: Book to unlearn at time step t\n",
    "- **D_prev**: Previously unlearned books (aggregated from previous steps)\n",
    "- **D_nor**: Retention data (200 chunks from 100 other books) - for evaluation\n",
    "\n",
    "Works on both local and Kaggle environments with automatic retry logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481067ca",
   "metadata": {
    "papermill": {
     "duration": 0.00394,
     "end_time": "2025-11-25T07:51:17.436097",
     "exception": false,
     "start_time": "2025-11-25T07:51:17.432157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c631588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:51:17.445197Z",
     "iopub.status.busy": "2025-11-25T07:51:17.444933Z",
     "iopub.status.idle": "2025-11-25T07:52:30.248465Z",
     "shell.execute_reply": "2025-11-25T07:52:30.247206Z"
    },
    "papermill": {
     "duration": 72.80991,
     "end_time": "2025-11-25T07:52:30.249989",
     "exception": false,
     "start_time": "2025-11-25T07:51:17.440079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q torch transformers peft datasets accelerate requests protobuf==3.20.3 rouge-score sentence-transformers scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d67ca3",
   "metadata": {
    "papermill": {
     "duration": 0.017911,
     "end_time": "2025-11-25T07:52:30.292651",
     "exception": false,
     "start_time": "2025-11-25T07:52:30.274740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2892a846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:52:30.328466Z",
     "iopub.status.busy": "2025-11-25T07:52:30.328187Z",
     "iopub.status.idle": "2025-11-25T07:53:06.914435Z",
     "shell.execute_reply": "2025-11-25T07:53:06.913682Z"
    },
    "papermill": {
     "duration": 36.623607,
     "end_time": "2025-11-25T07:53:06.933255",
     "exception": false,
     "start_time": "2025-11-25T07:52:30.309648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Disable torch.compile globally to avoid CUDA capability issues\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch._dynamo\n",
    "\n",
    "from typing import Dict, Sequence\n",
    "from types import SimpleNamespace\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Set environment variable to disable torch compilation\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "\n",
    "# Also disable dynamo\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "print(\"torch.compile disabled globally (for Tesla P100 compatibility)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29997444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:06.969170Z",
     "iopub.status.busy": "2025-11-25T07:53:06.968607Z",
     "iopub.status.idle": "2025-11-25T07:53:06.976373Z",
     "shell.execute_reply": "2025-11-25T07:53:06.975604Z"
    },
    "papermill": {
     "duration": 0.027117,
     "end_time": "2025-11-25T07:53:06.977499",
     "exception": false,
     "start_time": "2025-11-25T07:53:06.950382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration Class\n",
    "class Config:\n",
    "    # Model Configuration - Use smaller model to avoid download issues\n",
    "    MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "    \n",
    "    # Alternative options:\n",
    "    # MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # 3.8B, non-gated\n",
    "    # MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small, fast, non-gated\n",
    "    \n",
    "    TOKENIZER_NAME = MODEL_NAME\n",
    "    \n",
    "    # HuggingFace Authentication\n",
    "    USE_HF_TOKEN = True  # Set True for gated models\n",
    "    \n",
    "    # PEFT/LoRA Configuration - Match paper values\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "    # Prototyping mode - smaller runs for testing\n",
    "    PROTOTYPE_MODE = True  # Set False for full runs\n",
    "    \n",
    "    if PROTOTYPE_MODE:\n",
    "        NUM_UNLEARNING_STEPS = 3\n",
    "        NUM_CHUNKS_PER_STEP = 30\n",
    "        NUM_RETENTION_BOOKS = 30\n",
    "        NUM_RETENTION_CHUNKS = 60\n",
    "        EVAL_MAX_PAIRS = 5\n",
    "        EVAL_NUM_SAMPLES = 3\n",
    "    else:\n",
    "        NUM_UNLEARNING_STEPS = 10\n",
    "        NUM_CHUNKS_PER_STEP = 50\n",
    "        NUM_RETENTION_BOOKS = 100\n",
    "        NUM_RETENTION_CHUNKS = 200\n",
    "        EVAL_MAX_PAIRS = 10\n",
    "        EVAL_NUM_SAMPLES = 10\n",
    "    \n",
    "    # Fine-Tuning Hyperparameters\n",
    "    BATCH_SIZE = 1  # Reduced further to avoid OOM\n",
    "    GRADIENT_ACCUMULATION_STEPS = 16\n",
    "    LEARNING_RATE = 5e-5  # Base LR, will be overridden per step (1e-5 for steps 1-5, 1e-6 for 6-10)\n",
    "    NUM_EPOCHS_FT = 1  # 1 epoch for initial fine-tuning (as per paper)\n",
    "    NUM_EPOCHS_INITIAL_FT = 1  # Initial fine-tuning on all books (D_f)\n",
    "    WARMUP_STEPS = 10\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    # SSU Methodology Parameters - Match paper values\n",
    "    EPSILON_1 = 1.0  # Weight for Forgetting Loss (L_fgt) - λ1\n",
    "    EPSILON_2 = 0.1  # Weight for Random Labeling Loss (L_rnd) - λ2\n",
    "    # GAMMA removed - now computed dynamically as mu + sigma from gradients\n",
    "    \n",
    "    # Data Configuration\n",
    "    CHUNK_SIZE = 256\n",
    "    USE_REAL_BOOKS = True  # Use real books from Project Gutenberg\n",
    "    DATA_DIR = \"gutenberg_books\"\n",
    "    \n",
    "    # Project Gutenberg Book IDs - Exact 10 books from paper (in order)\n",
    "    ALL_BOOK_IDS = [\n",
    "        1661,   # Sherlock Holmes - Step 1\n",
    "        84,     # Frankenstein - Step 2\n",
    "        1342,   # Pride and Prejudice - Step 3\n",
    "        11,     # Alice in Wonderland - Step 4\n",
    "        2701,   # Moby Dick - Step 5\n",
    "        74,     # The Adventures of Tom Sawyer - Step 6\n",
    "        98,     # A Tale of Two Cities - Step 7\n",
    "        5200,   # Metamorphosis - Step 8\n",
    "        6130,   # The Iliad - Step 9\n",
    "        174,    # The Picture of Dorian Gray - Step 10\n",
    "    ]\n",
    "    \n",
    "    # Books to unlearn at each time step (sequential) - 10 steps\n",
    "    GUTENBERG_BOOK_IDS = {\n",
    "        1: [1661],   # Sherlock Holmes\n",
    "        2: [84],     # Frankenstein\n",
    "        3: [1342],   # Pride and Prejudice\n",
    "        4: [11],     # Alice in Wonderland\n",
    "        5: [2701],   # Moby Dick\n",
    "        6: [74],     # The Adventures of Tom Sawyer\n",
    "        7: [98],     # A Tale of Two Cities\n",
    "        8: [5200],   # Metamorphosis\n",
    "        9: [6130],   # The Iliad\n",
    "        10: [174],   # The Picture of Dorian Gray\n",
    "    }\n",
    "    \n",
    "    # Retention data (D_nor) - chunks from books disjoint from ALL_BOOK_IDS\n",
    "    USE_RETENTION_DATA = True  # D_nor used for evaluation and random labels\n",
    "    \n",
    "    # Storage / disk usage configuration\n",
    "    SAVE_MEMORIZED_MODEL = False  # Avoid saving large memorized checkpoint unless needed\n",
    "    SAVE_STEP_MODELS = False      # Only persist final model by default\n",
    "    DELETE_PREVIOUS_STEP_MODELS = True  # Remove older step checkpoints when new ones are saved\n",
    "    KEEP_DOWNLOADED_BOOKS = False  # Remove raw .txt files after chunking to save space\n",
    "    CLEANUP_FINAL_MODEL_DIR = False  # Optional: remove final model dir after exporting elsewhere\n",
    "    \n",
    "    OUTPUT_DIR = \"ssu_unlearned_models\"\n",
    "\n",
    "print(\"Configuration loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1eeb34",
   "metadata": {
    "papermill": {
     "duration": 0.023724,
     "end_time": "2025-11-25T07:53:07.024836",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.001112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Environment Detection & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfcc96e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.061633Z",
     "iopub.status.busy": "2025-11-25T07:53:07.061367Z",
     "iopub.status.idle": "2025-11-25T07:53:07.593520Z",
     "shell.execute_reply": "2025-11-25T07:53:07.592641Z"
    },
    "papermill": {
     "duration": 0.552754,
     "end_time": "2025-11-25T07:53:07.594860",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.042106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Detect if running on Kaggle\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running on: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "\n",
    "# HuggingFace Authentication (if needed)\n",
    "if Config.USE_HF_TOKEN:\n",
    "    from huggingface_hub import login\n",
    "    \n",
    "    hf_token = None\n",
    "    \n",
    "    # Try Kaggle Secrets\n",
    "    if IS_KAGGLE:\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            user_secrets = UserSecretsClient()\n",
    "            hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "            print(\"Found HuggingFace token in Kaggle Secrets.\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try environment variable\n",
    "    if not hf_token:\n",
    "        hf_token = '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'\n",
    "    \n",
    "    if hf_token:\n",
    "        try:\n",
    "            login(token=hf_token, add_to_git_credential=False)\n",
    "            print(\"Successfully logged in to HuggingFace.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not login: {e}\")\n",
    "    else:\n",
    "        print(\"WARNING: No HuggingFace token found. Gated models will fail.\")\n",
    "else:\n",
    "    print(\"Using non-gated model - no authentication needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d2455",
   "metadata": {
    "papermill": {
     "duration": 0.017311,
     "end_time": "2025-11-25T07:53:07.630223",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.612912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. SSU Model & Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93131985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.666342Z",
     "iopub.status.busy": "2025-11-25T07:53:07.666048Z",
     "iopub.status.idle": "2025-11-25T07:53:07.703723Z",
     "shell.execute_reply": "2025-11-25T07:53:07.702890Z"
    },
    "papermill": {
     "duration": 0.057489,
     "end_time": "2025-11-25T07:53:07.704812",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.647323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dummy book text for simulation\n",
    "DUMMY_BOOK_TEXT = \"\"\"\n",
    "In the beginning God created the heavens and the earth. Now the earth was formless and empty, darkness was over the surface of the deep, and the Spirit of God was hovering over the waters. And God said, \"Let there be light,\" and there was light. God saw that the light was good, and he separated the light from the darkness. God called the light \"day,\" and the darkness he called \"night.\" And there was evening, and there was morning—the first day. \n",
    "\n",
    "And God said, \"Let there be a vault between the waters to separate water from water.\" So God made the vault and separated the water under the vault from the water above it. And it was so. God called the vault \"sky.\" And there was evening, and there was morning—the second day.\n",
    "\n",
    "And God said, \"Let the water under the sky be gathered to one place, and let dry ground appear.\" And it was so. God called the dry ground \"land,\" and the gathered waters he called \"seas.\" And God saw that it was good. Then God said, \"Let the land produce vegetation: seed-bearing plants and trees on the land that bear fruit with seed in it, according to their various kinds.\" And it was so. The land produced vegetation: plants bearing seed according to their kinds and trees bearing fruit with seed in it according to their kinds. And God saw that it was good. And there was evening, and there was morning—the third day.\n",
    "\n",
    "And God said, \"Let there be lights in the vault of the sky to separate the day from the night, and let them serve as signs to mark sacred times, and days and years, and let them be lights in the vault of the sky to give light on the earth.\" And it was so. God made two great lights—the greater light to govern the day and the lesser light to govern the night. He also made the stars. God set them in the vault of the sky to give light on the earth, to govern the day and the night, and to separate light from darkness. And God saw that it was good. And there was evening, and there was morning—the fourth day.\n",
    "\n",
    "And God said, \"Let the water teem with living creatures, and let birds fly above the earth across the vault of the sky.\" So God created the great creatures of the sea and every living thing with which the water teems and that moves about in it, according to their kinds, and every winged bird according to its kind. And God saw that it was good. God blessed them and said, \"Be fruitful and increase in number and fill the water in the seas, and let the birds increase on the earth.\" And there was evening, and there was morning—the fifth day.\n",
    "\n",
    "And God said, \"Let the land produce living creatures according to their kinds: the livestock, the creatures that move along the ground, and the wild animals, each according to its kind.\" And it was so. God made the wild animals according to their kinds, the livestock according to their kinds, and all the creatures that move along the ground according to their kinds. And God saw that it was good. Then God said, \"Let us make mankind in our image, in our likeness, so that they may rule over the fish in the sea and the birds in the sky, over the livestock and all the wild animals, and over all the creatures that move along the ground.\" So God created mankind in his own image, in the image of God he created them; male and female he created them. God blessed them and said to them, \"Be fruitful and increase in number; fill the earth and subdue it. Rule over the fish in the sea and the birds in the sky and over every living creature that moves on the ground.\" Then God said, \"I give you every seed-bearing plant on the face of the whole earth and every tree that has fruit with seed in it. They will be yours for food. And to all the beasts of the earth and all the birds in the sky and all the creatures that move along the ground—everything that has the breath of life in it—I give every green plant for food.\" And it was so. God saw all that he had made, and it was very good. And there was evening, and there was morning—the sixth day.\n",
    "\n",
    "Thus the heavens and the earth were completed in all their vast array. By the seventh day God had finished the work he had been doing; so on the seventh day he rested from all his work. Then God blessed the seventh day and made it holy, because on it he rested from all the work of creating that he had done.\n",
    "\"\"\" * 50\n",
    "\n",
    "\n",
    "def generate_simulated_data(text, chunk_size, num_chunks, tokenizer_name):\n",
    "    \"\"\"Simulates a list of text chunks for one book (D_f^t).\n",
    "    \n",
    "    This function splits long book text into smaller chunks that fit within the model's\n",
    "    maximum sequence length. It does this by:\n",
    "    1. Splitting text into small word chunks (to avoid tokenization warnings)\n",
    "    2. Tokenizing each small chunk separately\n",
    "    3. Combining tokenized chunks to create final chunks of the desired size\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Get model max length - this is the MAXIMUM tokens the model can handle\n",
    "    # TinyLlama has max_length of 2048 tokens\n",
    "    max_length = getattr(tokenizer, 'model_max_length', 32000)\n",
    "\n",
    "    # Fix for huge model_max_length causing OverflowError\n",
    "    if max_length > 100000:\n",
    "        max_length = 8192  # Set a reasonable limit\n",
    "    \n",
    "    # Use a safe chunk size that's smaller than model max length\n",
    "    # We use chunk_size from config (256) but ensure it's under the model limit\n",
    "    safe_chunk_size = min(chunk_size, max_length - 10)  # Leave some margin for safety\n",
    "    \n",
    "    # IMPORTANT: We need to split text into VERY small pieces before tokenizing\n",
    "    # Why? Because 1 word can become 1-3 tokens, and we want to stay under max_length\n",
    "    # Strategy: Tokenize in small batches (500-800 words max) to avoid warnings\n",
    "    words = text.split()\n",
    "    \n",
    "    # Conservative estimate: ~1.5 tokens per word on average\n",
    "    # So for max_length=2048, we want max ~1300 words per tokenization batch\n",
    "    # But to be extra safe, we'll use even smaller: 500 words per batch\n",
    "    words_per_batch = min(500, max_length // 3)  # Very conservative: 500 words max per batch\n",
    "    \n",
    "    # Step 1: Tokenize text in small batches to avoid warnings\n",
    "    all_token_ids = []\n",
    "    for i in range(0, len(words), words_per_batch):\n",
    "        batch_text = ' '.join(words[i:i + words_per_batch])\n",
    "        # Tokenize with truncation - this ensures we never exceed max_length\n",
    "        tokenized = tokenizer(\n",
    "            batch_text, \n",
    "            return_tensors='pt', \n",
    "            truncation=True,  # This truncates if too long\n",
    "            max_length=max_length,  # Use model's actual max length\n",
    "            add_special_tokens=True\n",
    "        )['input_ids'][0]\n",
    "        all_token_ids.extend(tokenized.tolist())\n",
    "    \n",
    "    # Step 2: If we don't have enough tokens, repeat the sequence\n",
    "    if len(all_token_ids) < safe_chunk_size * num_chunks:\n",
    "        repeat_factor = (safe_chunk_size * num_chunks // len(all_token_ids)) + 1\n",
    "        all_token_ids = (all_token_ids * repeat_factor)[:safe_chunk_size * num_chunks * 2]\n",
    "    \n",
    "    # Step 3: Split into chunks of the desired size\n",
    "    chunks = []\n",
    "    for i in range(0, len(all_token_ids) - safe_chunk_size + 1, safe_chunk_size):\n",
    "        chunk = all_token_ids[i:i + safe_chunk_size]\n",
    "        if len(chunk) == safe_chunk_size:\n",
    "            chunks.append(chunk)\n",
    "        if len(chunks) >= num_chunks:\n",
    "            break\n",
    "    \n",
    "    # Step 4: If we still don't have enough, pad the last chunk\n",
    "    while len(chunks) < num_chunks:\n",
    "        if chunks:\n",
    "            # Repeat last chunk or pad\n",
    "            last_chunk = chunks[-1][:safe_chunk_size]\n",
    "            if len(last_chunk) < safe_chunk_size:\n",
    "                last_chunk = last_chunk + all_token_ids[:safe_chunk_size - len(last_chunk)]\n",
    "            chunks.append(last_chunk[:safe_chunk_size])\n",
    "        else:\n",
    "            # If no chunks at all, create a dummy chunk\n",
    "            chunks.append(all_token_ids[:safe_chunk_size])\n",
    "    \n",
    "    chunks = chunks[:num_chunks]\n",
    "    \n",
    "    # Step 5: Decode back to text (this is what the dataset will use)\n",
    "    text_chunks = [tokenizer.decode(c, skip_special_tokens=True) for c in chunks]\n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "def maybe_delete_file(file_path):\n",
    "    \"\"\"Delete a file if KEEP_DOWNLOADED_BOOKS is False.\"\"\"\n",
    "    if Config.KEEP_DOWNLOADED_BOOKS:\n",
    "        return\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def cleanup_dir(path):\n",
    "    \"\"\"Remove a directory tree if it exists.\"\"\"\n",
    "    if path and os.path.exists(path):\n",
    "        shutil.rmtree(path, ignore_errors=True)\n",
    "\n",
    "\n",
    "class SequentialUnlearningDataset(Dataset):\n",
    "    \"\"\"Custom Dataset that supports SSU forget data and retention data.\"\"\"\n",
    "    def __init__(self, tokenizer, data_texts, mode=\"forget\", random_label_ids=None):\n",
    "        if mode not in {\"forget\", \"retain\"}:\n",
    "            raise ValueError(f\"Unsupported dataset mode: {mode}\")\n",
    "        self.mode = mode\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_texts = data_texts\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            data_texts, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=Config.CHUNK_SIZE, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.input_ids = tokenized['input_ids']\n",
    "        self.attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "        self.random_label_ids = random_label_ids\n",
    "        self.random_indices = None\n",
    "        if self.mode == \"forget\":\n",
    "            if self.random_label_ids is None:\n",
    "                raise ValueError(\"random_label_ids is required for forget mode\")\n",
    "            self.random_indices = list(range(self.random_label_ids.size(0)))\n",
    "            random.shuffle(self.random_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx].clone()\n",
    "        attention_mask = self.attention_mask[idx].clone()\n",
    "        sample = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "        if self.mode == \"forget\":\n",
    "            labels_fgt = input_ids.clone()\n",
    "            # Sample labels_rnd from D_nor (random_label_ids), not from D_f\n",
    "            rnd_idx = self.random_indices[idx % len(self.random_indices)]\n",
    "            labels_rnd = self.random_label_ids[rnd_idx].clone()\n",
    "            sample.update({\n",
    "                'labels_fgt': labels_fgt,\n",
    "                'labels_rnd': labels_rnd,\n",
    "            })\n",
    "        else:\n",
    "            sample['labels'] = input_ids.clone()\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "class SSUDataCollator:\n",
    "    \"\"\"Custom data collator supporting mixed forget/retain batches.\"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        \n",
    "        if not features:\n",
    "            raise ValueError(\"Empty features list passed to data collator\")\n",
    "        \n",
    "        def to_tensor(x):\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                return x\n",
    "            return torch.tensor(x, dtype=torch.long)\n",
    "        \n",
    "        input_ids = [to_tensor(f['input_ids']) for f in features]\n",
    "        attention_mask = [to_tensor(f['attention_mask']) for f in features]\n",
    "        \n",
    "        batch = {}\n",
    "        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
    "        batch['input_ids'] = pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=pad_token_id,\n",
    "        )\n",
    "        batch['attention_mask'] = pad_sequence(\n",
    "            attention_mask,\n",
    "            batch_first=True,\n",
    "            padding_value=0,\n",
    "        )\n",
    "        max_len = batch['input_ids'].size(1)\n",
    "        batch_size = len(features)\n",
    "        device = batch['input_ids'].device\n",
    "        dtype = batch['input_ids'].dtype\n",
    "        \n",
    "        has_retain = any('labels' in f for f in features)\n",
    "        has_forget = any('labels_fgt' in f for f in features)\n",
    "        \n",
    "        # For forget-only batches, labels will be all -100\n",
    "        labels = torch.full((batch_size, max_len), -100, dtype=dtype, device=device)\n",
    "        for idx, f in enumerate(features):\n",
    "            if 'labels' in f:\n",
    "                data = to_tensor(f['labels']).to(device)\n",
    "                labels[idx, :data.shape[0]] = data\n",
    "        batch['labels'] = labels\n",
    "        \n",
    "        if has_forget:\n",
    "            labels_fgt = torch.full((batch_size, max_len), -100, dtype=dtype, device=device)\n",
    "            labels_rnd = torch.full((batch_size, max_len), -100, dtype=dtype, device=device)\n",
    "            for idx, f in enumerate(features):\n",
    "                if 'labels_fgt' in f:\n",
    "                    data_fgt = to_tensor(f['labels_fgt']).to(device)\n",
    "                    labels_fgt[idx, :data_fgt.shape[0]] = data_fgt\n",
    "                if 'labels_rnd' in f:\n",
    "                    data_rnd = to_tensor(f['labels_rnd']).to(device)\n",
    "                    labels_rnd[idx, :data_rnd.shape[0]] = data_rnd\n",
    "            batch['labels_fgt'] = labels_fgt\n",
    "            batch['labels_rnd'] = labels_rnd\n",
    "        \n",
    "        return batch\n",
    "\n",
    "def download_gutenberg_book(book_id, output_dir):\n",
    "    \"\"\"Download a book from Project Gutenberg by ID.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to downloaded book file, or None if download failed.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    book_file = os.path.join(output_dir, f\"{book_id}.txt\")\n",
    "    \n",
    "    if os.path.exists(book_file):\n",
    "        print(f\"Book {book_id} already exists, skipping download.\")\n",
    "        return book_file\n",
    "    \n",
    "    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "    try:\n",
    "        print(f\"Downloading book {book_id} from Project Gutenberg...\")\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(book_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        print(f\"Downloaded book {book_id} successfully.\")\n",
    "        return book_file\n",
    "    except:\n",
    "        url_alt = f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "        try:\n",
    "            response = requests.get(url_alt, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            with open(book_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Downloaded book {book_id} successfully.\")\n",
    "            return book_file\n",
    "        except Exception as e:\n",
    "            print(f\"Error: Could not download book {book_id} from Project Gutenberg.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def load_book_text(book_file):\n",
    "    \"\"\"Load and clean text from a book file.\"\"\"\n",
    "    if not book_file or not os.path.exists(book_file):\n",
    "        return None\n",
    "    with open(book_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    # Remove Project Gutenberg headers/footers\n",
    "    start_markers = [\"*** START OF\", \"***START OF\", \"START OF THE PROJECT\"]\n",
    "    end_markers = [\"*** END OF\", \"***END OF\", \"END OF THE PROJECT\"]\n",
    "    for marker in start_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            text = text[text.find('\\n', idx) + 1:]\n",
    "            break\n",
    "    for marker in end_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "            break\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_all_books_for_initial_finetuning():\n",
    "    \"\"\"Downloads all books for initial fine-tuning (D_f) - makes model memorize them.\"\"\"\n",
    "    print(\"\\n=== Downloading all books for initial fine-tuning (D_f) ===\")\n",
    "    all_books_dir = os.path.join(Config.DATA_DIR, \"all_books\")\n",
    "    os.makedirs(all_books_dir, exist_ok=True)\n",
    "    \n",
    "    book_texts = []\n",
    "    for book_id in Config.ALL_BOOK_IDS:\n",
    "        book_file = download_gutenberg_book(book_id, all_books_dir)\n",
    "        if not book_file:\n",
    "            raise RuntimeError(f\"Failed to download book {book_id} for initial fine-tuning\")\n",
    "        text = load_book_text(book_file)\n",
    "        if not text or len(text) < 1000:\n",
    "            raise RuntimeError(f\"Book {book_id} text is invalid or too short for initial fine-tuning\")\n",
    "        book_texts.append(text)\n",
    "        maybe_delete_file(book_file)\n",
    "        print(f\"Loaded book {book_id} ({len(text)} chars)\")\n",
    "    \n",
    "    if not book_texts:\n",
    "        raise RuntimeError(\"No books downloaded for initial fine-tuning\")\n",
    "    \n",
    "    return book_texts\n",
    "\n",
    "\n",
    "def get_unlearning_datasets(random_label_ids=None):\n",
    "    \"\"\"Generates sequential datasets D_f^1, D_f^2, ... for each time step.\n",
    "    \n",
    "    Args:\n",
    "        random_label_ids: Tensor of input_ids from D_nor to use for labels_rnd.\n",
    "                          Required for forget mode datasets.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    for t in range(Config.NUM_UNLEARNING_STEPS):\n",
    "        print(f\"\\n--- Preparing dataset D_f^{t+1} for time step {t+1} ---\")\n",
    "        \n",
    "        if Config.USE_REAL_BOOKS:\n",
    "            # Download books for this specific time step\n",
    "            time_step_dir = os.path.join(Config.DATA_DIR, f\"time_step_{t+1}\")\n",
    "            os.makedirs(time_step_dir, exist_ok=True)\n",
    "            \n",
    "            book_ids = Config.GUTENBERG_BOOK_IDS.get(t + 1, [])\n",
    "            book_texts = []\n",
    "            for book_id in book_ids:\n",
    "                book_file = download_gutenberg_book(book_id, time_step_dir)\n",
    "                if not book_file:\n",
    "                    raise RuntimeError(f\"Failed to download book {book_id} for step {t+1}\")\n",
    "                text = load_book_text(book_file)\n",
    "                if not text or len(text) < 1000:\n",
    "                    raise RuntimeError(f\"Book {book_id} text is invalid or too short for step {t+1}\")\n",
    "                book_texts.append(text)\n",
    "                maybe_delete_file(book_file)\n",
    "                print(f\"Loaded book {book_id} for step {t+1}\")\n",
    "            \n",
    "            if not book_texts:\n",
    "                raise RuntimeError(f\"No valid books for step {t+1}\")\n",
    "        else:\n",
    "            raise RuntimeError(\"USE_REAL_BOOKS must be True for paper reproduction\")\n",
    "        \n",
    "        all_chunks = []\n",
    "        for book_text in book_texts:\n",
    "            chunks = generate_simulated_data(\n",
    "                book_text,\n",
    "                Config.CHUNK_SIZE,\n",
    "                Config.NUM_CHUNKS_PER_STEP // len(book_texts) + 1,\n",
    "                Config.TOKENIZER_NAME\n",
    "            )\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        data_t = all_chunks[:Config.NUM_CHUNKS_PER_STEP]\n",
    "        print(f\"Created {len(data_t)} chunks for time step {t+1}\")\n",
    "        \n",
    "        # Create forget dataset with random_label_ids from D_nor\n",
    "        dataset_t = SequentialUnlearningDataset(\n",
    "            tokenizer, \n",
    "            data_t, \n",
    "            mode=\"forget\",\n",
    "            random_label_ids=random_label_ids\n",
    "        )\n",
    "        datasets.append(dataset_t)\n",
    "        \n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_retention_dataset():\n",
    "    \"\"\"Generates retention dataset D_nor (non-targeted data to keep).\n",
    "    \n",
    "    Returns dataset with 200 chunks from 100 books disjoint from ALL_BOOK_IDS.\n",
    "    \"\"\"\n",
    "    if not Config.USE_RETENTION_DATA:\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n=== Preparing retention dataset D_nor ===\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Generate 100 book IDs that are NOT in the unlearning set (ALL_BOOK_IDS)\n",
    "    # Use a range of common Gutenberg IDs, excluding those in ALL_BOOK_IDS\n",
    "    excluded_ids = set(Config.ALL_BOOK_IDS)\n",
    "    candidate_ids = []\n",
    "    # Common Gutenberg book IDs (expanded list)\n",
    "    common_ids = [\n",
    "        1232, 145, 76, 2591, 30254, 844, 345, 520, 6130, 174,\n",
    "        1342, 11, 2701, 74, 98, 5200, 6130, 174, 1661, 84,\n",
    "        100, 200, 300, 400, 500, 600, 700, 800, 900, 1000,\n",
    "        1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000,\n",
    "        2100, 2200, 2300, 2400, 2500, 2600, 2800, 2900, 3000, 3100,\n",
    "        3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100,\n",
    "        4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100,\n",
    "        5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200,\n",
    "        6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200,\n",
    "        7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200,\n",
    "    ]\n",
    "    \n",
    "    for bid in common_ids:\n",
    "        if bid not in excluded_ids and len(candidate_ids) < Config.NUM_RETENTION_BOOKS:\n",
    "            candidate_ids.append(bid)\n",
    "    \n",
    "    # If we don't have enough, generate more sequential IDs\n",
    "    next_id = 10000\n",
    "    while len(candidate_ids) < Config.NUM_RETENTION_BOOKS:\n",
    "        if next_id not in excluded_ids:\n",
    "            candidate_ids.append(next_id)\n",
    "        next_id += 1\n",
    "        if next_id > 100000:  # Safety limit\n",
    "            break\n",
    "    \n",
    "    retention_book_ids = candidate_ids[:Config.NUM_RETENTION_BOOKS]\n",
    "    retention_dir = os.path.join(Config.DATA_DIR, \"retention_books\")\n",
    "    os.makedirs(retention_dir, exist_ok=True)\n",
    "    \n",
    "    all_chunks = []\n",
    "    successful_books = 0\n",
    "    for book_id in retention_book_ids:\n",
    "        book_file = download_gutenberg_book(book_id, retention_dir)\n",
    "        if book_file:\n",
    "            text = load_book_text(book_file)\n",
    "            if text and len(text) > 1000:\n",
    "                chunks = generate_simulated_data(\n",
    "                    text,\n",
    "                    Config.CHUNK_SIZE,\n",
    "                    Config.NUM_RETENTION_CHUNKS // Config.NUM_RETENTION_BOOKS + 1,\n",
    "                    Config.TOKENIZER_NAME\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "                successful_books += 1\n",
    "                if len(all_chunks) >= Config.NUM_RETENTION_CHUNKS:\n",
    "                    break\n",
    "        maybe_delete_file(book_file)\n",
    "    \n",
    "    if not all_chunks:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to download any retention books. \"\n",
    "            f\"Expected {Config.NUM_RETENTION_BOOKS} books with {Config.NUM_RETENTION_CHUNKS} chunks total. \"\n",
    "            f\"Only {successful_books} books downloaded successfully.\"\n",
    "        )\n",
    "    \n",
    "    retention_chunks = all_chunks[:Config.NUM_RETENTION_CHUNKS]\n",
    "    print(f\"Created {len(retention_chunks)} retention chunks from {successful_books} books\")\n",
    "    \n",
    "    return SequentialUnlearningDataset(tokenizer, retention_chunks, mode=\"retain\")\n",
    "\n",
    "print(\"Data utilities loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018cf78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.741138Z",
     "iopub.status.busy": "2025-11-25T07:53:07.740892Z",
     "iopub.status.idle": "2025-11-25T07:53:07.750636Z",
     "shell.execute_reply": "2025-11-25T07:53:07.749943Z"
    },
    "papermill": {
     "duration": 0.029217,
     "end_time": "2025-11-25T07:53:07.751647",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.722430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SSUTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer implementing SSU loss with Weight Saliency.\n",
    "    \n",
    "    SSU loss: L = λ1 * L_fgt + λ2 * L_rnd (Equation 4 from paper)\n",
    "    No retention LM loss - D_nor is only used for evaluation and random labels.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Compute SSU loss: λ1 * L_fgt + λ2 * L_rnd (Equation 4).\"\"\"\n",
    "        inputs_copy = inputs.copy()\n",
    "        _ = inputs_copy.pop('labels', None)  # Not used in SSU loss\n",
    "        labels_fgt = inputs_copy.pop('labels_fgt', None)\n",
    "        labels_rnd = inputs_copy.pop('labels_rnd', None)\n",
    "        \n",
    "        if labels_fgt is None or labels_rnd is None:\n",
    "            raise ValueError(\"SSU requires labels_fgt and labels_rnd for forget batches\")\n",
    "        \n",
    "        # Compute L_fgt and L_rnd\n",
    "        outputs_fgt = model(**inputs_copy, labels=labels_fgt)\n",
    "        outputs_rnd = model(**inputs_copy, labels=labels_rnd)\n",
    "        \n",
    "        # SSU loss: L = λ1 * L_fgt + λ2 * L_rnd\n",
    "        loss = Config.EPSILON_1 * outputs_fgt.loss + Config.EPSILON_2 * outputs_rnd.loss\n",
    "        outputs_to_return = outputs_fgt\n",
    "        \n",
    "        return (loss, outputs_to_return) if return_outputs else loss\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Override optimizer_step to apply dynamic weight saliency masking.\n",
    "        \n",
    "        Gamma (γ) is computed dynamically as: γ = μ + σ\n",
    "        where μ is the mean and σ is the standard deviation of gradient magnitudes.\n",
    "        \"\"\"\n",
    "        # Apply weight saliency mask before optimizer step\n",
    "        if self.accelerator.sync_gradients:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.grad is not None and param.requires_grad:\n",
    "                    if \"lora\" in name.lower():\n",
    "                        grad = param.grad.data\n",
    "                        grad_abs = grad.abs()\n",
    "                        \n",
    "                        # Compute dynamic gamma: γ = μ + σ\n",
    "                        flat = grad_abs.view(-1)\n",
    "                        mu = flat.mean()\n",
    "                        sigma = flat.std()\n",
    "                        gamma = mu + sigma  # 1 std above mean\n",
    "                        \n",
    "                        # Saliency Mask: m_s = I(|grad| >= gamma)\n",
    "                        m_s = (grad_abs >= gamma).float()\n",
    "                        \n",
    "                        # Apply mask to gradients (only update parameters with high saliency)\n",
    "                        param.grad.data = grad * m_s\n",
    "        \n",
    "        # Call parent optimizer_step to perform the actual update\n",
    "        super().optimizer_step()\n",
    "\n",
    "\n",
    "def create_lora_model(model):\n",
    "    \"\"\"Adds LoRA adapters to the base model.\"\"\"\n",
    "    peft_config = LoraConfig(\n",
    "        r=Config.LORA_R,\n",
    "        lora_alpha=Config.LORA_ALPHA,\n",
    "        lora_dropout=Config.LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=Config.TARGET_MODULES,\n",
    "    )\n",
    "    return get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "def apply_task_vector_negation(base_model, fine_tuned_model, name_prefix):\n",
    "    \"\"\"Task Vector Negation: theta_u^t = 2 * theta_u^{t-1} - theta_ft^t\"\"\"\n",
    "    print(f\"\\n--- Applying Task Vector Negation for {name_prefix} ---\")\n",
    "    \n",
    "    device = next(base_model.parameters()).device\n",
    "    new_unlearned_model = base_model.__class__(config=base_model.config).to(device)\n",
    "    \n",
    "    base_state = base_model.state_dict()\n",
    "    ft_state = fine_tuned_model.state_dict()\n",
    "    \n",
    "    new_state = {}\n",
    "    for name, param in new_unlearned_model.named_parameters():\n",
    "        if name in base_state and name in ft_state:\n",
    "            new_state[name] = 2 * base_state[name] - ft_state[name]\n",
    "        else:\n",
    "            new_state[name] = base_state.get(name, param.data)\n",
    "    \n",
    "    new_unlearned_model.load_state_dict(new_state, strict=False)\n",
    "    print(f\"Task Vector Negation complete for {name_prefix}.\")\n",
    "    return new_unlearned_model\n",
    "\n",
    "print(\"SSU model utilities loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a033f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.787698Z",
     "iopub.status.busy": "2025-11-25T07:53:07.787490Z",
     "iopub.status.idle": "2025-11-25T07:53:07.801704Z",
     "shell.execute_reply": "2025-11-25T07:53:07.801153Z"
    },
    "papermill": {
     "duration": 0.033761,
     "end_time": "2025-11-25T07:53:07.802734",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.768973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 9. Evaluation Utilities\n",
    "from typing import List, Tuple, Any\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Semantic similarity model (cached)\n",
    "SEMANTIC_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "_semantic_model = None\n",
    "\n",
    "def get_semantic_model():\n",
    "    \"\"\"Return a cached sentence-transformers model on CPU.\"\"\"\n",
    "    global _semantic_model\n",
    "    if _semantic_model is None:\n",
    "        _semantic_model = SentenceTransformer(SEMANTIC_MODEL_NAME, device=\"cpu\")\n",
    "    return _semantic_model\n",
    "\n",
    "\n",
    "def evaluate_semantic_similarity(\n",
    "    best_pairs: List[Tuple[str, str]],\n",
    "    similarity_threshold: float = 0.8\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compute semantic similarity between GT continuations and model generations.\n",
    "\n",
    "    Args:\n",
    "        best_pairs: list of (reference_text, generated_text) pairs per prompt.\n",
    "    \"\"\"\n",
    "    if not best_pairs:\n",
    "        return {\"mean_cosine\": 0.0, \"frac_high\": 0.0, \"cosine_raw\": []}\n",
    "\n",
    "    embed_model = get_semantic_model()\n",
    "\n",
    "    refs = [r for (r, _) in best_pairs]\n",
    "    hyps = [h for (_, h) in best_pairs]\n",
    "\n",
    "    ref_emb = embed_model.encode(refs, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    hyp_emb = embed_model.encode(hyps, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "    cos = (ref_emb * hyp_emb).sum(dim=1).cpu().tolist()\n",
    "    mean_cos = float(sum(cos) / len(cos))\n",
    "    frac_high = float(sum(c >= similarity_threshold for c in cos) / len(cos))\n",
    "\n",
    "    return {\n",
    "        \"mean_cosine\": mean_cos,\n",
    "        \"frac_high\": frac_high,\n",
    "        \"cosine_raw\": cos,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_nll_per_chunk(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    text_chunks: List[str],\n",
    "    chunk_size: int = 256,\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Compute per-chunk NLL (average cross-entropy loss per token) for membership analysis.\n",
    "    \"\"\"\n",
    "    if not text_chunks:\n",
    "        return []\n",
    "\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    nlls = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for chunk in text_chunks:\n",
    "            tokenized = tokenizer(\n",
    "                chunk,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=chunk_size,\n",
    "            )\n",
    "            input_ids = tokenized['input_ids'].to(device)\n",
    "            outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "            nlls.append(outputs.loss.item())\n",
    "\n",
    "    return nlls\n",
    "\n",
    "\n",
    "def evaluate_membership_risk(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    forget_chunks: List[str],\n",
    "    retain_chunks: List[str],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Membership-style analysis comparing NLL distributions on forget vs retain chunks.\n",
    "    \"\"\"\n",
    "    nll_forget = compute_nll_per_chunk(model, tokenizer, forget_chunks)\n",
    "    nll_retain = compute_nll_per_chunk(model, tokenizer, retain_chunks)\n",
    "\n",
    "    if not nll_forget or not nll_retain:\n",
    "        return {\n",
    "            \"mean_nll_forget\": float(\"nan\"),\n",
    "            \"mean_nll_retain\": float(\"nan\"),\n",
    "            \"delta_nll\": float(\"nan\"),\n",
    "            \"roc_auc\": float(\"nan\"),\n",
    "            \"nll_forget_raw\": nll_forget,\n",
    "            \"nll_retain_raw\": nll_retain,\n",
    "        }\n",
    "\n",
    "    mean_forget = sum(nll_forget) / len(nll_forget)\n",
    "    mean_retain = sum(nll_retain) / len(nll_retain)\n",
    "    delta_nll = mean_forget - mean_retain\n",
    "\n",
    "    labels = [1] * len(nll_forget) + [0] * len(nll_retain)\n",
    "    scores = nll_forget + nll_retain\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "\n",
    "    return {\n",
    "        \"mean_nll_forget\": mean_forget,\n",
    "        \"mean_nll_retain\": mean_retain,\n",
    "        \"delta_nll\": delta_nll,\n",
    "        \"roc_auc\": auc,\n",
    "        \"nll_forget_raw\": nll_forget,\n",
    "        \"nll_retain_raw\": nll_retain,\n",
    "    }\n",
    "\n",
    "\n",
    "def _prepare_prompt_pairs(tokenizer, book_text: str, prompt_tokens: int = 100, continuation_tokens: int = 100, max_pairs: int = 16):\n",
    "    \"\"\"Prepare prompt-continuation pairs from book text.\n",
    "    \n",
    "    Args:\n",
    "        prompt_tokens: Number of tokens for prompt (default 100 per paper)\n",
    "        continuation_tokens: Number of tokens for continuation (default 100 per paper)\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        book_text,\n",
    "        return_tensors='pt',\n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "    )['input_ids'][0]\n",
    "    total_needed = prompt_tokens + continuation_tokens\n",
    "    pairs = []\n",
    "    for start in range(0, max(0, len(tokenized) - total_needed), total_needed):\n",
    "        prompt_ids = tokenized[start:start + prompt_tokens]\n",
    "        cont_ids = tokenized[start + prompt_tokens:start + total_needed]\n",
    "        if len(prompt_ids) < prompt_tokens or len(cont_ids) < continuation_tokens:\n",
    "            continue\n",
    "        prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "        cont_text = tokenizer.decode(cont_ids, skip_special_tokens=True)\n",
    "        pairs.append((prompt_text, cont_text))\n",
    "        if len(pairs) >= max_pairs:\n",
    "            break\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def evaluate_regurgitation(model, tokenizer, book_text: str, max_pairs: int = 10, \n",
    "                          prompt_tokens: int = 100, continuation_tokens: int = 100,\n",
    "                          num_samples: int = 10, verbose: bool = True,\n",
    "                          return_best_hypotheses: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"Estimate regurgitation via ROUGE-L and ROUGE-1 between generated text and the book.\n",
    "    \n",
    "    For each prompt, generates N samples and takes the maximum ROUGE score per prompt.\n",
    "    Matches paper's evaluation protocol.\n",
    "    \n",
    "    Args:\n",
    "        return_best_hypotheses: If True, also return best_pairs list of (reference, best_hypothesis) tuples.\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    pairs = _prepare_prompt_pairs(tokenizer, book_text, prompt_tokens, continuation_tokens, max_pairs)\n",
    "    if not pairs:\n",
    "        result = {\"rouge1\": 0.0, \"rougeL\": 0.0, \"num_pairs\": 0}\n",
    "        if return_best_hypotheses:\n",
    "            result[\"best_pairs\"] = []\n",
    "        return result\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=False)\n",
    "    max_rouge1_scores = []\n",
    "    max_rougeL_scores = []\n",
    "    best_pairs = []  # list of (reference, best_hypothesis)\n",
    "    \n",
    "    model.eval()\n",
    "    total_generations = len(pairs) * num_samples\n",
    "    generation_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for pair_idx, (prompt, reference) in enumerate(pairs):\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to(model_device)\n",
    "            \n",
    "            # Generate N samples per prompt and take max ROUGE\n",
    "            prompt_rouge1_scores = []\n",
    "            prompt_rougeL_scores = []\n",
    "            hypotheses = []  # track all generated hypotheses for this prompt\n",
    "            \n",
    "            for sample_idx in range(num_samples):\n",
    "                if verbose and generation_count % 10 == 0:\n",
    "                    print(f\"  Generating sample {generation_count + 1}/{total_generations} (pair {pair_idx + 1}/{len(pairs)}, sample {sample_idx + 1}/{num_samples})\", end='\\r')\n",
    "                \n",
    "                generated = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=continuation_tokens,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.6,\n",
    "                    temperature=0.6,\n",
    "                    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "                )\n",
    "                gen_continuation = generated[0][inputs['input_ids'].shape[1]:]\n",
    "                hypothesis = tokenizer.decode(gen_continuation, skip_special_tokens=True)\n",
    "                hypotheses.append(hypothesis)\n",
    "                \n",
    "                scores = scorer.score(reference, hypothesis)\n",
    "                prompt_rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "                prompt_rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "                \n",
    "                generation_count += 1\n",
    "            \n",
    "            # Find best hypothesis by ROUGE-L and track it\n",
    "            best_idx = prompt_rougeL_scores.index(max(prompt_rougeL_scores))\n",
    "            best_pairs.append((reference, hypotheses[best_idx]))\n",
    "            \n",
    "            # Take maximum per prompt\n",
    "            max_rouge1_scores.append(max(prompt_rouge1_scores))\n",
    "            max_rougeL_scores.append(max(prompt_rougeL_scores))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Completed pair {pair_idx + 1}/{len(pairs)} - Max ROUGE-L: {max_rougeL_scores[-1]:.4f}                    \")\n",
    "    \n",
    "    avg_rouge1 = float(sum(max_rouge1_scores) / len(max_rouge1_scores)) if max_rouge1_scores else 0.0\n",
    "    avg_rougeL = float(sum(max_rougeL_scores) / len(max_rougeL_scores)) if max_rougeL_scores else 0.0\n",
    "    \n",
    "    result = {\n",
    "        \"rouge1\": avg_rouge1,\n",
    "        \"rougeL\": avg_rougeL,\n",
    "        \"num_pairs\": len(pairs)\n",
    "    }\n",
    "    if return_best_hypotheses:\n",
    "        result[\"best_pairs\"] = best_pairs\n",
    "    return result\n",
    "\n",
    "\n",
    "def _normalize_corpus(text_source: Sequence[str] | str, tokenizer, max_samples: int = 32) -> str:\n",
    "    if isinstance(text_source, str):\n",
    "        return text_source\n",
    "    if isinstance(text_source, SequentialUnlearningDataset):\n",
    "        samples = [tokenizer.decode(text_source.input_ids[i], skip_special_tokens=True) for i in range(min(len(text_source), max_samples))]\n",
    "        return \"\\n\\n\".join(samples)\n",
    "    if isinstance(text_source, list):\n",
    "        return \"\\n\\n\".join(text_source[:max_samples])\n",
    "    raise ValueError(\"Unsupported text source type for perplexity evaluation\")\n",
    "\n",
    "\n",
    "def evaluate_perplexity(model, tokenizer, text_source, stride: int = 256) -> float:\n",
    "    \"\"\"Compute perplexity on the provided text corpus.\"\"\"\n",
    "    corpus = _normalize_corpus(text_source, tokenizer)\n",
    "    tokenized = tokenizer(\n",
    "        corpus,\n",
    "        return_tensors='pt',\n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "    )['input_ids'][0]\n",
    "    if tokenized.size(0) <= 1:\n",
    "        return float('inf')\n",
    "    model_device = next(model.parameters()).device\n",
    "    nll_sum = 0.0\n",
    "    token_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, tokenized.size(0) - 1, stride):\n",
    "            end = min(start + stride, tokenized.size(0))\n",
    "            input_ids = tokenized[start:end].unsqueeze(0).to(model_device)\n",
    "            labels = input_ids.clone()\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            n_tokens = labels.size(1) - 1\n",
    "            if n_tokens <= 0:\n",
    "                continue\n",
    "            nll_sum += outputs.loss.item() * n_tokens\n",
    "            token_count += n_tokens\n",
    "    if token_count == 0:\n",
    "        return float('inf')\n",
    "    return float(math.exp(nll_sum / token_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42fc802",
   "metadata": {
    "papermill": {
     "duration": 0.017487,
     "end_time": "2025-11-25T07:53:07.837451",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.819964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Model Loading with Retry Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a8517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.873031Z",
     "iopub.status.busy": "2025-11-25T07:53:07.872817Z",
     "iopub.status.idle": "2025-11-25T07:53:26.089800Z",
     "shell.execute_reply": "2025-11-25T07:53:26.089054Z"
    },
    "papermill": {
     "duration": 18.236121,
     "end_time": "2025-11-25T07:53:26.090943",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.854822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_with_retry(model_name, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load model with automatic retry on network errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading model (attempt {attempt + 1}/{max_retries})...\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Determine device\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "                dtype = torch.bfloat16\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "                dtype = torch.float32\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                torch_dtype=dtype,\n",
    "                device_map=device,  # Use single device instead of \"auto\"\n",
    "                trust_remote_code=True,\n",
    "                attn_implementation=\"eager\",\n",
    "            )\n",
    "            \n",
    "            # Ensure model is on the correct device\n",
    "            base_model = base_model.to(device)\n",
    "            \n",
    "            print(f\"Successfully loaded {model_name}!\")\n",
    "            return base_model, tokenizer\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"Attempt {attempt + 1} failed: {error_msg[:200]}...\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                if \"IncompleteRead\" in error_msg or \"Connection\" in error_msg or \"timeout\" in error_msg.lower():\n",
    "                    print(f\"Network error detected. Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(\"Non-network error. Retrying...\")\n",
    "                    time.sleep(2)\n",
    "            else:\n",
    "                print(\"\\nAll retry attempts failed!\")\n",
    "                print(\"\\nTROUBLESHOOTING:\")\n",
    "                print(\"1. Check your internet connection\")\n",
    "                print(\"2. Try a smaller model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "                print(\"3. For gated models, ensure HF_TOKEN is set\")\n",
    "                raise\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "\n",
    "# Load the model\n",
    "print(f\"Loading base model: {Config.MODEL_NAME}\")\n",
    "base_model, tokenizer = load_model_with_retry(Config.MODEL_NAME)\n",
    "base_model.requires_grad_(False)\n",
    "print(\"Model loaded and frozen!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347102a",
   "metadata": {
    "papermill": {
     "duration": 0.018378,
     "end_time": "2025-11-25T07:53:26.128399",
     "exception": false,
     "start_time": "2025-11-25T07:53:26.110021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Main SSU Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b70fa5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:26.165986Z",
     "iopub.status.busy": "2025-11-25T07:53:26.165775Z",
     "iopub.status.idle": "2025-11-25T07:57:53.503885Z",
     "shell.execute_reply": "2025-11-25T07:57:53.502958Z"
    },
    "papermill": {
     "duration": 267.358603,
     "end_time": "2025-11-25T07:57:53.505032",
     "exception": false,
     "start_time": "2025-11-25T07:53:26.146429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "def initial_finetuning(model, tokenizer, all_books_texts):\n",
    "    \"\"\"\n",
    "    Step 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n",
    "    This is what the paper does BEFORE unlearning.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Fine-tuning vanilla model on all copyrighted books to memorize them...\")\n",
    "    \n",
    "    # Generate chunks from all books\n",
    "    all_chunks = []\n",
    "    for book_text in all_books_texts:\n",
    "        chunks = generate_simulated_data(\n",
    "            book_text,\n",
    "            Config.CHUNK_SIZE,\n",
    "            Config.NUM_CHUNKS_PER_STEP * Config.NUM_UNLEARNING_STEPS // len(all_books_texts) + 1,\n",
    "            Config.TOKENIZER_NAME\n",
    "        )\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"Created {len(all_chunks)} chunks from all books\")\n",
    "    \n",
    "    # For initial fine-tuning, use standard dataset (not SSU dual labels)\n",
    "    from torch.utils.data import Dataset as TorchDataset\n",
    "    class StandardDataset(TorchDataset):\n",
    "        def __init__(self, tokenizer, data_texts):\n",
    "            tokenized = tokenizer(\n",
    "                data_texts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=Config.CHUNK_SIZE,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.input_ids = tokenized['input_ids']\n",
    "            self.attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                'input_ids': self.input_ids[idx].clone(),\n",
    "                'attention_mask': self.attention_mask[idx].clone(),\n",
    "                'labels': self.input_ids[idx].clone()  # Standard labels for next token prediction\n",
    "            }\n",
    "    \n",
    "    initial_dataset = StandardDataset(tokenizer, all_chunks)\n",
    "    \n",
    "    # Ensure model is on correct device before creating LoRA\n",
    "    device = next(model.parameters()).device\n",
    "    if device.type == \"meta\" or str(device) == \"meta\":\n",
    "        # If model is on meta device, move to actual device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        # Convert device object to string if needed\n",
    "        device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n",
    "    \n",
    "    # Create LoRA model for initial fine-tuning (PEFT handles device automatically)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "    \n",
    "    lora_model = create_lora_model(model)\n",
    "    lora_model.print_trainable_parameters()\n",
    "    \n",
    "    # Disable cache for gradient checkpointing\n",
    "    if hasattr(lora_model.config, \"use_cache\"):\n",
    "        lora_model.config.use_cache = False\n",
    "    \n",
    "    # PEFT models inherit device from base model, no need to call .to()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{Config.OUTPUT_DIR}/initial_ft_checkpoints\",\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=Config.LEARNING_RATE,\n",
    "        num_train_epochs=Config.NUM_EPOCHS_INITIAL_FT,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        fp16=False,\n",
    "        bf16=torch.cuda.is_available() and device == \"cuda\",\n",
    "        dataloader_pin_memory=False,  # Fix device issues\n",
    "        label_names=[\"labels\"],\n",
    "        gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    # Use standard Trainer for initial fine-tuning (not SSU)\n",
    "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Causal LM, not masked LM\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=initial_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting initial fine-tuning...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Merge LoRA weights into base model\n",
    "    memorized_model = lora_model.merge_and_unload()\n",
    "    memorized_model.requires_grad_(False)\n",
    "    \n",
    "    print(\"Initial fine-tuning complete. Model has memorized all books.\")\n",
    "    return memorized_model\n",
    "\n",
    "\n",
    "def run_initial_finetuning():\n",
    "    \"\"\"STEP 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n",
    "    \n",
    "    Run this cell once to create the memorized model. After this completes,\n",
    "    you can run the sequential unlearning steps without re-running this.\n",
    "    \"\"\"\n",
    "    # Ensure all required dependencies are available\n",
    "    missing = []\n",
    "    try:\n",
    "        _ = Config.OUTPUT_DIR\n",
    "    except NameError:\n",
    "        missing.append(\"Config (cell 4)\")\n",
    "    \n",
    "    try:\n",
    "        _ = base_model\n",
    "    except NameError:\n",
    "        missing.append(\"base_model (cell 11)\")\n",
    "    \n",
    "    try:\n",
    "        _ = tokenizer\n",
    "    except NameError:\n",
    "        missing.append(\"tokenizer (cell 11)\")\n",
    "    \n",
    "    if missing:\n",
    "        raise NameError(\n",
    "            f\"The following are not defined: {', '.join(missing)}. \"\n",
    "            f\"Please run the required cells first to set up the dependencies.\"\n",
    "        )\n",
    "    \n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Check if memorized model already exists\n",
    "    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n",
    "    if os.path.exists(memorized_model_path):\n",
    "        print(f\"Memorized model already exists at {memorized_model_path}\")\n",
    "        print(\"Loading existing memorized model...\")\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        memorized_model = AutoModelForCausalLM.from_pretrained(\n",
    "            memorized_model_path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device\n",
    "        )\n",
    "        memorized_model.requires_grad_(False)\n",
    "        print(\"Loaded existing memorized model.\")\n",
    "        return memorized_model\n",
    "    \n",
    "    # STEP 0: Initial fine-tuning on all books (D_f)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n",
    "    print(\"=\"*60)\n",
    "    all_books_texts = get_all_books_for_initial_finetuning()\n",
    "    memorized_model = initial_finetuning(base_model, tokenizer, all_books_texts)\n",
    "    \n",
    "    # Save the memorized model (optional)\n",
    "    if Config.SAVE_MEMORIZED_MODEL:\n",
    "        memorized_model.save_pretrained(memorized_model_path)\n",
    "        tokenizer.save_pretrained(memorized_model_path)\n",
    "        print(f\"\\nMemorized model saved to {memorized_model_path}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping on-disk save of memorized model (SAVE_MEMORIZED_MODEL=False)\")\n",
    "    \n",
    "    return memorized_model\n",
    "\n",
    "\n",
    "memorized_model = run_initial_finetuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f42950e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:57:53.544778Z",
     "iopub.status.busy": "2025-11-25T07:57:53.544567Z",
     "iopub.status.idle": "2025-11-25T19:27:45.245035Z",
     "shell.execute_reply": "2025-11-25T19:27:45.244316Z"
    },
    "papermill": {
     "duration": 41391.722088,
     "end_time": "2025-11-25T19:27:45.246567",
     "exception": false,
     "start_time": "2025-11-25T07:57:53.524479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 8. Sequential Unlearning Steps\n",
    "def run_sequential_unlearning(start_step=1, end_step=None, general_validation_text=None, run_evaluation=True):\n",
    "    # Suppress dynamo errors\n",
    "    import torch._dynamo\n",
    "    torch._dynamo.config.suppress_errors = True\n",
    "    \"\"\"Run sequential unlearning steps with optional evaluation reporting.\"\"\"\n",
    "    # Ensure all required dependencies are available\n",
    "    missing = []\n",
    "    try:\n",
    "        _ = Config.OUTPUT_DIR\n",
    "    except NameError:\n",
    "        missing.append(\"Config (cell 4)\")\n",
    "    \n",
    "    try:\n",
    "        _ = tokenizer\n",
    "    except NameError:\n",
    "        missing.append(\"tokenizer (cell 11)\")\n",
    "    \n",
    "    if missing:\n",
    "        raise NameError(\n",
    "            f\"The following are not defined: {', '.join(missing)}. \"\n",
    "            f\"Please run the required cells first.\"\n",
    "        )\n",
    "    \n",
    "    if end_step is None:\n",
    "        end_step = Config.NUM_UNLEARNING_STEPS\n",
    "    \n",
    "    general_validation_text = general_validation_text or DUMMY_BOOK_TEXT\n",
    "    \n",
    "    # Load or start from memorized model (check if file exists, not variable)\n",
    "    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n",
    "    memorized_model_on_disk = os.path.exists(memorized_model_path)\n",
    "    memorized_model_in_memory = 'memorized_model' in globals()\n",
    "    if not memorized_model_on_disk and not memorized_model_in_memory:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Memorized model not found at {memorized_model_path} and no in-memory model available. \"\n",
    "            f\"Please run initial fine-tuning first.\"\n",
    "        )\n",
    "    \n",
    "    # Determine starting model\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    if start_step == 1:\n",
    "        if memorized_model_on_disk:\n",
    "            current_model = AutoModelForCausalLM.from_pretrained(\n",
    "                memorized_model_path,\n",
    "                torch_dtype=dtype,\n",
    "                device_map=device\n",
    "            )\n",
    "            print(\"Starting from memorized model (disk) for step 1\")\n",
    "        else:\n",
    "            current_model = memorized_model.to(device)\n",
    "            print(\"Starting from memorized model (in-memory) for step 1\")\n",
    "        current_model.requires_grad_(False)\n",
    "    else:\n",
    "        # Load model from previous step\n",
    "        prev_step_path = f\"{Config.OUTPUT_DIR}/step_{start_step-1}_unlearned_model\"\n",
    "        if not os.path.exists(prev_step_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Previous step model not found at {prev_step_path}. \"\n",
    "                f\"Resume is only possible if step checkpoints were saved.\"\n",
    "            )\n",
    "        current_model = AutoModelForCausalLM.from_pretrained(\n",
    "            prev_step_path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device\n",
    "        )\n",
    "        current_model.requires_grad_(False)\n",
    "        print(f\"Starting from step {start_step-1} model for step {start_step}\")\n",
    "    \n",
    "    # Get retention dataset (D_nor) first - needed for random_label_ids\n",
    "    retention_dataset = get_retention_dataset()\n",
    "    \n",
    "    # Extract random_label_ids from D_nor for use in forget datasets\n",
    "    random_label_ids = retention_dataset.input_ids if retention_dataset is not None else None\n",
    "    if random_label_ids is None:\n",
    "        raise RuntimeError(\"retention_dataset is required for SSU (provides random_label_ids)\")\n",
    "    \n",
    "    # Load sequential datasets for unlearning with random_label_ids\n",
    "    unlearning_datasets = get_unlearning_datasets(random_label_ids=random_label_ids)\n",
    "    print(f\"\\nGenerated {len(unlearning_datasets)} sequential unlearning datasets.\")\n",
    "    \n",
    "    all_target_book_ids = [book_id for ids in Config.GUTENBERG_BOOK_IDS.values() for book_id in ids]\n",
    "    evaluation_log = {\"baseline\": {}, \"steps\": []}\n",
    "    eval_book_cache = {}\n",
    "    eval_book_dir = os.path.join(Config.DATA_DIR, \"evaluation_books\")\n",
    "    os.makedirs(eval_book_dir, exist_ok=True)\n",
    "    \n",
    "    def get_book_text_for_eval(book_id):\n",
    "        if book_id in eval_book_cache:\n",
    "            return eval_book_cache[book_id]\n",
    "        book_file = download_gutenberg_book(book_id, eval_book_dir)\n",
    "        if not book_file:\n",
    "            raise RuntimeError(f\"Failed to download book {book_id} for evaluation\")\n",
    "        text = load_book_text(book_file)\n",
    "        if not text or len(text) < 1000:\n",
    "            raise RuntimeError(f\"Book {book_id} text is invalid or too short for evaluation\")\n",
    "        eval_book_cache[book_id] = text\n",
    "        maybe_delete_file(book_file)\n",
    "        return text\n",
    "    \n",
    "    if run_evaluation:\n",
    "        print(\"\\n=== Baseline Evaluation ===\")\n",
    "        print(f\"Note: Evaluation generates {Config.EVAL_NUM_SAMPLES} samples per prompt, this may take a while...\")\n",
    "        baseline_regurg = {}\n",
    "        for book_id in all_target_book_ids:\n",
    "            print(f\"\\nEvaluating book {book_id}...\")\n",
    "            regurg_results = evaluate_regurgitation(\n",
    "                current_model, \n",
    "                tokenizer, \n",
    "                get_book_text_for_eval(book_id),\n",
    "                max_pairs=Config.EVAL_MAX_PAIRS,\n",
    "                num_samples=Config.EVAL_NUM_SAMPLES,\n",
    "                verbose=True,\n",
    "                return_best_hypotheses=True,\n",
    "            )\n",
    "            rouge_metrics = {\"rouge1\": regurg_results[\"rouge1\"], \"rougeL\": regurg_results[\"rougeL\"]}\n",
    "            semantic_metrics = evaluate_semantic_similarity(regurg_results[\"best_pairs\"])\n",
    "            baseline_regurg[book_id] = {\n",
    "                \"rouge\": rouge_metrics,\n",
    "                \"semantic\": semantic_metrics,\n",
    "            }\n",
    "            print(f\"Book {book_id} baseline ROUGE-L: {rouge_metrics['rougeL']:.4f}, ROUGE-1: {rouge_metrics['rouge1']:.4f}\")\n",
    "            print(f\"Book {book_id} baseline semantic: mean_cosine={semantic_metrics['mean_cosine']:.4f}, frac_high={semantic_metrics['frac_high']:.2f}\")\n",
    "        \n",
    "        baseline_perplexity = {}\n",
    "        if retention_dataset is not None:\n",
    "            baseline_perplexity['retention'] = evaluate_perplexity(current_model, tokenizer, retention_dataset)\n",
    "            print(f\"Retention baseline perplexity: {baseline_perplexity['retention']:.2f}\")\n",
    "        baseline_perplexity['general'] = evaluate_perplexity(current_model, tokenizer, general_validation_text)\n",
    "        print(f\"General baseline perplexity: {baseline_perplexity['general']:.2f}\")\n",
    "        \n",
    "        # Baseline membership evaluation\n",
    "        baseline_ds = unlearning_datasets[0]\n",
    "        baseline_forget_chunks = baseline_ds.data_texts\n",
    "        baseline_retain_chunks = retention_dataset.data_texts[:len(baseline_forget_chunks)]\n",
    "        baseline_membership = evaluate_membership_risk(\n",
    "            current_model,\n",
    "            tokenizer,\n",
    "            baseline_forget_chunks,\n",
    "            baseline_retain_chunks,\n",
    "        )\n",
    "        print(f\"Baseline membership delta_NLL: {baseline_membership['delta_nll']:.4f}, ROC-AUC: {baseline_membership['roc_auc']:.4f}\")\n",
    "        \n",
    "        evaluation_log['baseline'] = {\n",
    "            'regurgitation': baseline_regurg,\n",
    "            'perplexity': baseline_perplexity,\n",
    "            'membership': baseline_membership,\n",
    "        }\n",
    "    \n",
    "    # Track latest saved checkpoint to delete previous ones if configured\n",
    "    last_saved_checkpoint = None\n",
    "    \n",
    "    # STEP 1-N: Sequential Unlearning Loop\n",
    "    for t in range(start_step - 1, min(end_step, Config.NUM_UNLEARNING_STEPS)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"UNLEARNING STEP {t+1} (Unlearning D_f^{t+1})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        dataset_t = unlearning_datasets[t]\n",
    "        step_prefix = f\"step_{t+1}\"\n",
    "        \n",
    "        # SSU training uses only D_f^t (dataset_t), not D_nor\n",
    "        # D_nor is only used for evaluation and random labels\n",
    "        train_dataset = dataset_t\n",
    "        \n",
    "        # Fine-Tuning Stage\n",
    "        print(f\"Preparing LoRA model for fine-tuning on D_f^{t+1}...\")\n",
    "        \n",
    "        # Ensure model is on correct device\n",
    "        device = next(current_model.parameters()).device\n",
    "        if device.type == \"meta\" or str(device) == \"meta\":\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            current_model = current_model.to(device)\n",
    "        else:\n",
    "            # Convert device object to string if needed\n",
    "            device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n",
    "        \n",
    "        if hasattr(current_model, \"enable_input_require_grads\"):\n",
    "            current_model.enable_input_require_grads()\n",
    "        else:\n",
    "            def make_inputs_require_grad(module, input, output):\n",
    "                output.requires_grad_(True)\n",
    "            current_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "        \n",
    "        lora_model = create_lora_model(current_model)\n",
    "        # PEFT models inherit device from base model, no need to call .to()\n",
    "        lora_model.print_trainable_parameters()\n",
    "        \n",
    "        # Disable cache for gradient checkpointing\n",
    "        if hasattr(lora_model.config, \"use_cache\"):\n",
    "            lora_model.config.use_cache = False\n",
    "        \n",
    "        # Dynamic learning rate: 1e-5 for steps 1-5, 1e-6 for steps 6-10\n",
    "        lr = 1e-5 if (t + 1) <= 5 else 1e-6\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{Config.OUTPUT_DIR}/{step_prefix}_ft_checkpoints\",\n",
    "            per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "            gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "            warmup_steps=Config.WARMUP_STEPS,\n",
    "            learning_rate=lr,\n",
    "            num_train_epochs=Config.NUM_EPOCHS_FT,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"no\",\n",
    "            report_to=\"none\",\n",
    "            fp16=False,\n",
    "            bf16=torch.cuda.is_available() and device == \"cuda\",\n",
    "            dataloader_pin_memory=False,  # Fix device issues\n",
    "            label_names=[\"labels\"],\n",
    "            gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "            remove_unused_columns=False,  # Keep custom labels\n",
    "            weight_decay=Config.WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        # Use custom data collator that preserves labels_fgt and labels_rnd\n",
    "        data_collator = SSUDataCollator(tokenizer=tokenizer)\n",
    "        \n",
    "        trainer = SSUTrainer(\n",
    "            model=lora_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        print(f\"Starting fine-tuning with SSU Loss for {step_prefix}...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Task Vector Negation Stage: theta_new = theta_old - Delta_LoRA\n",
    "        print(f\"\\n--- Applying Task Vector Negation for {step_prefix} ---\")\n",
    "        \n",
    "        # 1. Negate LoRA weights: W_new = W_old - (W_ft - W_old) = W_old - W_lora\n",
    "        # We achieve this by multiplying LoRA weights by -1, then merging.\n",
    "        with torch.no_grad():\n",
    "            for name, param in lora_model.named_parameters():\n",
    "                if \"lora\" in name:\n",
    "                    param.data = -1 * param.data\n",
    "        \n",
    "        print(\"LoRA weights negated.\")\n",
    "        \n",
    "        # 2. Merge negated weights into base model\n",
    "        current_model = lora_model.merge_and_unload()\n",
    "        current_model.requires_grad_(False)\n",
    "        \n",
    "        print(f\"Task Vector Negation complete for {step_prefix}.\")\n",
    "        \n",
    "        # Save the unlearned model (optionally keep only final checkpoint)\n",
    "        save_path = f\"{Config.OUTPUT_DIR}/{step_prefix}_unlearned_model\"\n",
    "        is_final_step = (t + 1) == min(end_step, Config.NUM_UNLEARNING_STEPS)\n",
    "        save_this_step = Config.SAVE_STEP_MODELS or is_final_step\n",
    "        if save_this_step:\n",
    "            current_model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "            print(f\"Unlearned model {step_prefix} saved.\")\n",
    "            if Config.DELETE_PREVIOUS_STEP_MODELS and last_saved_checkpoint and last_saved_checkpoint != save_path:\n",
    "                print(f\"Deleting previous checkpoint: {last_saved_checkpoint}\")\n",
    "                cleanup_dir(last_saved_checkpoint)\n",
    "            last_saved_checkpoint = save_path\n",
    "        else:\n",
    "            # Remove any stale checkpoint directory to avoid disk bloat\n",
    "            cleanup_dir(save_path)\n",
    "            print(f\"Skipped saving {step_prefix} checkpoint (SAVE_STEP_MODELS=False)\")\n",
    "        \n",
    "        if run_evaluation:\n",
    "            step_metrics = {\n",
    "                'step': t + 1,\n",
    "                'current_books': {},  # D_f^t (current step)\n",
    "                'prev_books': {},     # D_prev (previously unlearned)\n",
    "                'perplexity': {},\n",
    "            }\n",
    "            \n",
    "            # Evaluate on current books (D_f^t)\n",
    "            current_book_ids = Config.GUTENBERG_BOOK_IDS.get(t + 1, [])\n",
    "            for book_id in current_book_ids:\n",
    "                print(f\"\\nEvaluating current book {book_id}...\")\n",
    "                regurg_results = evaluate_regurgitation(\n",
    "                    current_model, \n",
    "                    tokenizer, \n",
    "                    get_book_text_for_eval(book_id),\n",
    "                    max_pairs=Config.EVAL_MAX_PAIRS,\n",
    "                    num_samples=Config.EVAL_NUM_SAMPLES,\n",
    "                    verbose=True,\n",
    "                    return_best_hypotheses=True,\n",
    "                )\n",
    "                rouge_metrics = {\"rouge1\": regurg_results[\"rouge1\"], \"rougeL\": regurg_results[\"rougeL\"]}\n",
    "                semantic_metrics = evaluate_semantic_similarity(regurg_results[\"best_pairs\"])\n",
    "                step_metrics['current_books'][book_id] = {\n",
    "                    \"rouge\": rouge_metrics,\n",
    "                    \"semantic\": semantic_metrics,\n",
    "                }\n",
    "                print(f\"Step {t+1} current book {book_id} ROUGE-L: {rouge_metrics['rougeL']:.4f}, ROUGE-1: {rouge_metrics['rouge1']:.4f}\")\n",
    "                print(f\"Step {t+1} current book {book_id} semantic: mean_cosine={semantic_metrics['mean_cosine']:.4f}, frac_high={semantic_metrics['frac_high']:.2f}\")\n",
    "            \n",
    "            # Evaluate on previous books (D_prev) - union of steps 1..t\n",
    "            prev_book_ids = []\n",
    "            for prev_step in range(1, t + 1):\n",
    "                prev_book_ids.extend(Config.GUTENBERG_BOOK_IDS.get(prev_step, []))\n",
    "            for book_id in prev_book_ids:\n",
    "                print(f\"\\nEvaluating prev book {book_id}...\")\n",
    "                regurg_results = evaluate_regurgitation(\n",
    "                    current_model, \n",
    "                    tokenizer, \n",
    "                    get_book_text_for_eval(book_id),\n",
    "                    max_pairs=Config.EVAL_MAX_PAIRS,\n",
    "                    num_samples=Config.EVAL_NUM_SAMPLES,\n",
    "                    verbose=True,\n",
    "                    return_best_hypotheses=True,\n",
    "                )\n",
    "                rouge_metrics = {\"rouge1\": regurg_results[\"rouge1\"], \"rougeL\": regurg_results[\"rougeL\"]}\n",
    "                semantic_metrics = evaluate_semantic_similarity(regurg_results[\"best_pairs\"])\n",
    "                step_metrics['prev_books'][book_id] = {\n",
    "                    \"rouge\": rouge_metrics,\n",
    "                    \"semantic\": semantic_metrics,\n",
    "                }\n",
    "                print(f\"Step {t+1} prev book {book_id} ROUGE-L: {rouge_metrics['rougeL']:.4f}, ROUGE-1: {rouge_metrics['rouge1']:.4f}\")\n",
    "                print(f\"Step {t+1} prev book {book_id} semantic: mean_cosine={semantic_metrics['mean_cosine']:.4f}, frac_high={semantic_metrics['frac_high']:.2f}\")\n",
    "            \n",
    "            # Evaluate perplexity on D_nor (retention) and general text\n",
    "            if retention_dataset is not None:\n",
    "                retention_ppl = evaluate_perplexity(current_model, tokenizer, retention_dataset)\n",
    "                step_metrics['perplexity']['retention'] = retention_ppl\n",
    "                print(f\"Step {t+1} retention perplexity: {retention_ppl:.2f}\")\n",
    "            general_ppl = evaluate_perplexity(current_model, tokenizer, general_validation_text)\n",
    "            step_metrics['perplexity']['general'] = general_ppl\n",
    "            print(f\"Step {t+1} general perplexity: {general_ppl:.2f}\")\n",
    "            \n",
    "            # Membership evaluation for current step\n",
    "            forget_chunks = dataset_t.data_texts\n",
    "            retain_chunks = retention_dataset.data_texts[:len(forget_chunks)]\n",
    "            membership_metrics = evaluate_membership_risk(\n",
    "                current_model,\n",
    "                tokenizer,\n",
    "                forget_chunks,\n",
    "                retain_chunks,\n",
    "            )\n",
    "            step_metrics['membership'] = membership_metrics\n",
    "            print(f\"Step {t+1} membership delta_NLL: {membership_metrics['delta_nll']:.4f}, ROC-AUC: {membership_metrics['roc_auc']:.4f}\")\n",
    "            \n",
    "            evaluation_log['steps'].append(step_metrics)\n",
    "        \n",
    "        # current_model is already updated via merge_and_unload\n",
    "\n",
    "    # Final Evaluation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SEQUENTIAL UNLEARNING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    final_step = min(end_step, Config.NUM_UNLEARNING_STEPS)\n",
    "    final_model_dir = f\"{Config.OUTPUT_DIR}/step_{final_step}_unlearned_model\"\n",
    "    print(f\"Final Unlearned Model: {final_model_dir}\")\n",
    "    \n",
    "    # Test generation\n",
    "    prompt = \"The quick brown fox\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    print(\"\\nTesting generation with final unlearned model...\")\n",
    "    \n",
    "    # Reload model to remove training hooks/state that interfere with inference\n",
    "    print(\"Reloading model for clean inference...\")\n",
    "    del current_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    # Re-determine device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    current_model = AutoModelForCausalLM.from_pretrained(\n",
    "        final_model_dir,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device,\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "    current_model.requires_grad_(False)\n",
    "    current_model.eval()\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Completely disable torch.compile to avoid compilation errors\n",
    "    torch._dynamo.reset()\n",
    "    # This completely disables dynamo/torch.compile\n",
    "    original_disable_state = torch._dynamo.config.disable\n",
    "    torch._dynamo.config.disable = True\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            output_tokens = current_model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=30, \n",
    "                do_sample=True, \n",
    "                top_p=0.9, \n",
    "                temperature=0.7\n",
    "            )\n",
    "    finally:\n",
    "        # Restore original state\n",
    "        torch._dynamo.config.disable = original_disable_state\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    \n",
    "    if run_evaluation:\n",
    "        run_sequential_unlearning.last_evaluation = evaluation_log\n",
    "    \n",
    "    if Config.CLEANUP_FINAL_MODEL_DIR:\n",
    "        print(f\"Removing final model directory per configuration: {final_model_dir}\")\n",
    "        cleanup_dir(final_model_dir)\n",
    "    \n",
    "    return current_model\n",
    "\n",
    "# You can specify start_step and end_step to resume from a specific step\n",
    "# Example: run_sequential_unlearning(start_step=2, end_step=3) to run only steps 2-3\n",
    "final_unlearned_model = run_sequential_unlearning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b780e1",
   "metadata": {},
   "source": [
    "## 9. Inference-Only Evaluation\n",
    "\n",
    "Run this cell to evaluate existing models with the new metrics (semantic similarity and membership NLL) **without retraining**.\n",
    "\n",
    "**Two modes:**\n",
    "1. `run_inference_only_evaluation()` - Evaluate just the unlearned model\n",
    "2. `run_before_after_evaluation()` - Compare memorized (before) vs unlearned (after) models\n",
    "\n",
    "**Setup:** Copy your models to:\n",
    "- `ssu_unlearned_models/memorized_model/` - Model after initial fine-tuning (before unlearning)\n",
    "- `ssu_unlearned_models/step_10_unlearned_model/` - Final unlearned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7134526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_only_evaluation(model_path=\"ssu_unlearned_models/step_10_unlearned_model\"):\n",
    "    \"\"\"\n",
    "    Run evaluation on an existing unlearned model WITHOUT retraining.\n",
    "    Computes: ROUGE, semantic similarity, perplexity, and membership metrics.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    # Check model exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model not found at {model_path}. Run training first or specify correct path.\")\n",
    "    \n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "    model.eval()\n",
    "    model.requires_grad_(False)\n",
    "    \n",
    "    tok = AutoTokenizer.from_pretrained(model_path)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "    \n",
    "    print(\"Model loaded. Preparing evaluation data...\")\n",
    "    \n",
    "    # Get retention dataset for membership evaluation\n",
    "    retention_dataset = get_retention_dataset()\n",
    "    \n",
    "    # Prepare evaluation books directory\n",
    "    eval_book_dir = os.path.join(Config.DATA_DIR, \"evaluation_books\")\n",
    "    os.makedirs(eval_book_dir, exist_ok=True)\n",
    "    eval_book_cache = {}\n",
    "    \n",
    "    def get_book_text_for_eval(book_id):\n",
    "        if book_id in eval_book_cache:\n",
    "            return eval_book_cache[book_id]\n",
    "        book_file = download_gutenberg_book(book_id, eval_book_dir)\n",
    "        if not book_file:\n",
    "            raise RuntimeError(f\"Failed to download book {book_id} for evaluation\")\n",
    "        text = load_book_text(book_file)\n",
    "        if not text or len(text) < 1000:\n",
    "            raise RuntimeError(f\"Book {book_id} text is invalid or too short\")\n",
    "        eval_book_cache[book_id] = text\n",
    "        return text\n",
    "    \n",
    "    # Get all book IDs from config (all 10 books)\n",
    "    all_book_ids = Config.ALL_BOOK_IDS\n",
    "    \n",
    "    evaluation_results = {\n",
    "        \"model_path\": model_path,\n",
    "        \"books\": {},\n",
    "        \"membership\": {},\n",
    "        \"perplexity\": {},\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== Evaluating {len(all_book_ids)} books ===\")\n",
    "    print(f\"Using EVAL_MAX_PAIRS={Config.EVAL_MAX_PAIRS}, EVAL_NUM_SAMPLES={Config.EVAL_NUM_SAMPLES}\")\n",
    "    \n",
    "    # Evaluate each book\n",
    "    for book_id in all_book_ids:\n",
    "        print(f\"\\n--- Evaluating book {book_id} ---\")\n",
    "        book_text = get_book_text_for_eval(book_id)\n",
    "        \n",
    "        # ROUGE + semantic similarity\n",
    "        regurg_results = evaluate_regurgitation(\n",
    "            model,\n",
    "            tok,\n",
    "            book_text,\n",
    "            max_pairs=Config.EVAL_MAX_PAIRS,\n",
    "            num_samples=Config.EVAL_NUM_SAMPLES,\n",
    "            verbose=True,\n",
    "            return_best_hypotheses=True,\n",
    "        )\n",
    "        \n",
    "        rouge_metrics = {\"rouge1\": regurg_results[\"rouge1\"], \"rougeL\": regurg_results[\"rougeL\"]}\n",
    "        semantic_metrics = evaluate_semantic_similarity(regurg_results[\"best_pairs\"])\n",
    "        \n",
    "        evaluation_results[\"books\"][book_id] = {\n",
    "            \"rouge\": rouge_metrics,\n",
    "            \"semantic\": semantic_metrics,\n",
    "        }\n",
    "        \n",
    "        print(f\"Book {book_id} ROUGE-L: {rouge_metrics['rougeL']:.4f}, ROUGE-1: {rouge_metrics['rouge1']:.4f}\")\n",
    "        print(f\"Book {book_id} semantic: mean_cosine={semantic_metrics['mean_cosine']:.4f}, frac_high={semantic_metrics['frac_high']:.2f}\")\n",
    "    \n",
    "    # Perplexity on retention dataset\n",
    "    print(\"\\n--- Evaluating perplexity ---\")\n",
    "    if retention_dataset is not None:\n",
    "        retention_ppl = evaluate_perplexity(model, tok, retention_dataset)\n",
    "        evaluation_results[\"perplexity\"][\"retention\"] = retention_ppl\n",
    "        print(f\"Retention perplexity: {retention_ppl:.2f}\")\n",
    "    \n",
    "    general_ppl = evaluate_perplexity(model, tok, DUMMY_BOOK_TEXT)\n",
    "    evaluation_results[\"perplexity\"][\"general\"] = general_ppl\n",
    "    print(f\"General perplexity: {general_ppl:.2f}\")\n",
    "    \n",
    "    # Membership evaluation (using first book's chunks as example)\n",
    "    print(\"\\n--- Evaluating membership metrics ---\")\n",
    "    # Generate forget chunks from first book\n",
    "    first_book_text = get_book_text_for_eval(all_book_ids[0])\n",
    "    forget_chunks = generate_simulated_data(\n",
    "        first_book_text,\n",
    "        Config.CHUNK_SIZE,\n",
    "        min(Config.NUM_CHUNKS_PER_STEP, 30),  # Limit for speed\n",
    "        Config.TOKENIZER_NAME\n",
    "    )\n",
    "    retain_chunks = retention_dataset.data_texts[:len(forget_chunks)]\n",
    "    \n",
    "    membership_metrics = evaluate_membership_risk(model, tok, forget_chunks, retain_chunks)\n",
    "    evaluation_results[\"membership\"] = membership_metrics\n",
    "    print(f\"Membership delta_NLL: {membership_metrics['delta_nll']:.4f}, ROC-AUC: {membership_metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFERENCE-ONLY EVALUATION COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    avg_rouge_l = sum(b[\"rouge\"][\"rougeL\"] for b in evaluation_results[\"books\"].values()) / len(evaluation_results[\"books\"])\n",
    "    avg_cosine = sum(b[\"semantic\"][\"mean_cosine\"] for b in evaluation_results[\"books\"].values()) / len(evaluation_results[\"books\"])\n",
    "    \n",
    "    print(f\"Average ROUGE-L across all books: {avg_rouge_l:.4f}\")\n",
    "    print(f\"Average semantic cosine similarity: {avg_cosine:.4f}\")\n",
    "    print(f\"Membership delta_NLL: {membership_metrics['delta_nll']:.4f}\")\n",
    "    print(f\"Membership ROC-AUC: {membership_metrics['roc_auc']:.4f}\")\n",
    "    \n",
    "    # Save results to JSON\n",
    "    results_path = os.path.join(Config.OUTPUT_DIR, \"inference_evaluation_results.json\")\n",
    "    # Convert to serializable format (remove raw arrays for cleaner JSON)\n",
    "    serializable_results = {\n",
    "        \"model_path\": evaluation_results[\"model_path\"],\n",
    "        \"books\": evaluation_results[\"books\"],\n",
    "        \"perplexity\": evaluation_results[\"perplexity\"],\n",
    "        \"membership\": {\n",
    "            \"mean_nll_forget\": membership_metrics[\"mean_nll_forget\"],\n",
    "            \"mean_nll_retain\": membership_metrics[\"mean_nll_retain\"],\n",
    "            \"delta_nll\": membership_metrics[\"delta_nll\"],\n",
    "            \"roc_auc\": membership_metrics[\"roc_auc\"],\n",
    "        },\n",
    "        \"summary\": {\n",
    "            \"avg_rouge_l\": avg_rouge_l,\n",
    "            \"avg_semantic_cosine\": avg_cosine,\n",
    "        }\n",
    "    }\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(serializable_results, f, indent=2)\n",
    "    print(f\"\\nResults saved to {results_path}\")\n",
    "    \n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "def run_before_after_evaluation(\n",
    "    memorized_model_path=\"ssu_unlearned_models/memorized_model\",\n",
    "    unlearned_model_path=\"ssu_unlearned_models/step_10_unlearned_model\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare memorized (before unlearning) vs unlearned (after) models.\n",
    "    Shows the delta in ROUGE, semantic similarity, and membership metrics.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    \n",
    "    # Check both models exist\n",
    "    if not os.path.exists(memorized_model_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Memorized model not found at {memorized_model_path}.\\n\"\n",
    "            f\"Copy your memorized model folder there first.\"\n",
    "        )\n",
    "    if not os.path.exists(unlearned_model_path):\n",
    "        raise FileNotFoundError(f\"Unlearned model not found at {unlearned_model_path}\")\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    # Get retention dataset (shared between both evaluations)\n",
    "    print(\"Preparing evaluation data...\")\n",
    "    retention_dataset = get_retention_dataset()\n",
    "    \n",
    "    eval_book_dir = os.path.join(Config.DATA_DIR, \"evaluation_books\")\n",
    "    os.makedirs(eval_book_dir, exist_ok=True)\n",
    "    eval_book_cache = {}\n",
    "    \n",
    "    def get_book_text_for_eval(book_id):\n",
    "        if book_id in eval_book_cache:\n",
    "            return eval_book_cache[book_id]\n",
    "        book_file = download_gutenberg_book(book_id, eval_book_dir)\n",
    "        if not book_file:\n",
    "            raise RuntimeError(f\"Failed to download book {book_id}\")\n",
    "        text = load_book_text(book_file)\n",
    "        if not text or len(text) < 1000:\n",
    "            raise RuntimeError(f\"Book {book_id} text invalid\")\n",
    "        eval_book_cache[book_id] = text\n",
    "        return text\n",
    "    \n",
    "    all_book_ids = Config.ALL_BOOK_IDS\n",
    "    results = {\"memorized\": {\"books\": {}}, \"unlearned\": {\"books\": {}}, \"delta\": {\"books\": {}}}\n",
    "    \n",
    "    # Helper to evaluate a single model\n",
    "    def evaluate_model(model_path, label):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"EVALUATING {label.upper()} MODEL: {model_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, torch_dtype=dtype, device_map=device, attn_implementation=\"eager\"\n",
    "        )\n",
    "        model.eval()\n",
    "        model.requires_grad_(False)\n",
    "        \n",
    "        tok = AutoTokenizer.from_pretrained(model_path)\n",
    "        if tok.pad_token is None:\n",
    "            tok.pad_token = tok.eos_token\n",
    "        \n",
    "        model_results = {\"books\": {}, \"perplexity\": {}, \"membership\": {}}\n",
    "        \n",
    "        # Evaluate each book\n",
    "        for book_id in all_book_ids:\n",
    "            print(f\"\\n--- {label}: Book {book_id} ---\")\n",
    "            book_text = get_book_text_for_eval(book_id)\n",
    "            \n",
    "            regurg_results = evaluate_regurgitation(\n",
    "                model, tok, book_text,\n",
    "                max_pairs=Config.EVAL_MAX_PAIRS,\n",
    "                num_samples=Config.EVAL_NUM_SAMPLES,\n",
    "                verbose=True,\n",
    "                return_best_hypotheses=True,\n",
    "            )\n",
    "            \n",
    "            rouge = {\"rouge1\": regurg_results[\"rouge1\"], \"rougeL\": regurg_results[\"rougeL\"]}\n",
    "            semantic = evaluate_semantic_similarity(regurg_results[\"best_pairs\"])\n",
    "            \n",
    "            model_results[\"books\"][book_id] = {\"rouge\": rouge, \"semantic\": semantic}\n",
    "            print(f\"  ROUGE-L: {rouge['rougeL']:.4f}, Semantic cosine: {semantic['mean_cosine']:.4f}\")\n",
    "        \n",
    "        # Perplexity\n",
    "        if retention_dataset is not None:\n",
    "            model_results[\"perplexity\"][\"retention\"] = evaluate_perplexity(model, tok, retention_dataset)\n",
    "        model_results[\"perplexity\"][\"general\"] = evaluate_perplexity(model, tok, DUMMY_BOOK_TEXT)\n",
    "        print(f\"\\n{label} perplexity - retention: {model_results['perplexity'].get('retention', 'N/A'):.2f}, general: {model_results['perplexity']['general']:.2f}\")\n",
    "        \n",
    "        # Membership\n",
    "        first_book_text = get_book_text_for_eval(all_book_ids[0])\n",
    "        forget_chunks = generate_simulated_data(\n",
    "            first_book_text, Config.CHUNK_SIZE, min(Config.NUM_CHUNKS_PER_STEP, 30), Config.TOKENIZER_NAME\n",
    "        )\n",
    "        retain_chunks = retention_dataset.data_texts[:len(forget_chunks)]\n",
    "        \n",
    "        membership = evaluate_membership_risk(model, tok, forget_chunks, retain_chunks)\n",
    "        model_results[\"membership\"] = membership\n",
    "        print(f\"{label} membership - delta_NLL: {membership['delta_nll']:.4f}, ROC-AUC: {membership['roc_auc']:.4f}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return model_results\n",
    "    \n",
    "    # Evaluate both models\n",
    "    results[\"memorized\"] = evaluate_model(memorized_model_path, \"Memorized\")\n",
    "    results[\"unlearned\"] = evaluate_model(unlearned_model_path, \"Unlearned\")\n",
    "    \n",
    "    # Compute deltas\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"BEFORE vs AFTER COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(f\"\\n{'Book ID':<10} {'ROUGE-L (Before)':<18} {'ROUGE-L (After)':<18} {'Δ ROUGE-L':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total_rouge_before, total_rouge_after = 0, 0\n",
    "    total_sem_before, total_sem_after = 0, 0\n",
    "    \n",
    "    for book_id in all_book_ids:\n",
    "        mem = results[\"memorized\"][\"books\"][book_id]\n",
    "        unl = results[\"unlearned\"][\"books\"][book_id]\n",
    "        \n",
    "        rouge_before = mem[\"rouge\"][\"rougeL\"]\n",
    "        rouge_after = unl[\"rouge\"][\"rougeL\"]\n",
    "        delta_rouge = rouge_after - rouge_before\n",
    "        \n",
    "        sem_before = mem[\"semantic\"][\"mean_cosine\"]\n",
    "        sem_after = unl[\"semantic\"][\"mean_cosine\"]\n",
    "        delta_sem = sem_after - sem_before\n",
    "        \n",
    "        results[\"delta\"][\"books\"][book_id] = {\n",
    "            \"delta_rouge_l\": delta_rouge,\n",
    "            \"delta_semantic\": delta_sem,\n",
    "        }\n",
    "        \n",
    "        total_rouge_before += rouge_before\n",
    "        total_rouge_after += rouge_after\n",
    "        total_sem_before += sem_before\n",
    "        total_sem_after += sem_after\n",
    "        \n",
    "        print(f\"{book_id:<10} {rouge_before:<18.4f} {rouge_after:<18.4f} {delta_rouge:<12.4f}\")\n",
    "    \n",
    "    n = len(all_book_ids)\n",
    "    avg_rouge_before = total_rouge_before / n\n",
    "    avg_rouge_after = total_rouge_after / n\n",
    "    avg_sem_before = total_sem_before / n\n",
    "    avg_sem_after = total_sem_after / n\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'AVERAGE':<10} {avg_rouge_before:<18.4f} {avg_rouge_after:<18.4f} {avg_rouge_after - avg_rouge_before:<12.4f}\")\n",
    "    \n",
    "    # Semantic comparison\n",
    "    print(f\"\\n{'Book ID':<10} {'Semantic (Before)':<18} {'Semantic (After)':<18} {'Δ Semantic':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    for book_id in all_book_ids:\n",
    "        mem = results[\"memorized\"][\"books\"][book_id]\n",
    "        unl = results[\"unlearned\"][\"books\"][book_id]\n",
    "        print(f\"{book_id:<10} {mem['semantic']['mean_cosine']:<18.4f} {unl['semantic']['mean_cosine']:<18.4f} {unl['semantic']['mean_cosine'] - mem['semantic']['mean_cosine']:<12.4f}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'AVERAGE':<10} {avg_sem_before:<18.4f} {avg_sem_after:<18.4f} {avg_sem_after - avg_sem_before:<12.4f}\")\n",
    "    \n",
    "    # Membership comparison\n",
    "    mem_mem = results[\"memorized\"][\"membership\"]\n",
    "    mem_unl = results[\"unlearned\"][\"membership\"]\n",
    "    \n",
    "    print(f\"\\n{'Metric':<20} {'Memorized (Before)':<20} {'Unlearned (After)':<20} {'Delta':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'delta_NLL':<20} {mem_mem['delta_nll']:<20.4f} {mem_unl['delta_nll']:<20.4f} {mem_unl['delta_nll'] - mem_mem['delta_nll']:<15.4f}\")\n",
    "    print(f\"{'ROC-AUC':<20} {mem_mem['roc_auc']:<20.4f} {mem_unl['roc_auc']:<20.4f} {mem_unl['roc_auc'] - mem_mem['roc_auc']:<15.4f}\")\n",
    "    \n",
    "    results[\"delta\"][\"membership\"] = {\n",
    "        \"delta_nll_change\": mem_unl[\"delta_nll\"] - mem_mem[\"delta_nll\"],\n",
    "        \"roc_auc_change\": mem_unl[\"roc_auc\"] - mem_mem[\"roc_auc\"],\n",
    "    }\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Average ROUGE-L: {avg_rouge_before:.4f} → {avg_rouge_after:.4f} (Δ = {avg_rouge_after - avg_rouge_before:+.4f})\")\n",
    "    print(f\"Average semantic: {avg_sem_before:.4f} → {avg_sem_after:.4f} (Δ = {avg_sem_after - avg_sem_before:+.4f})\")\n",
    "    print(f\"Membership delta_NLL: {mem_mem['delta_nll']:.4f} → {mem_unl['delta_nll']:.4f} (Δ = {mem_unl['delta_nll'] - mem_mem['delta_nll']:+.4f})\")\n",
    "    \n",
    "    if avg_rouge_after < avg_rouge_before:\n",
    "        print(\"\\n✓ ROUGE decreased after unlearning (less regurgitation)\")\n",
    "    if mem_unl['delta_nll'] > mem_mem['delta_nll']:\n",
    "        print(\"✓ delta_NLL increased after unlearning (forget chunks became harder)\")\n",
    "    \n",
    "    # Save results\n",
    "    results_path = os.path.join(Config.OUTPUT_DIR, \"before_after_evaluation_results.json\")\n",
    "    serializable = {\n",
    "        \"memorized\": {\n",
    "            \"books\": results[\"memorized\"][\"books\"],\n",
    "            \"perplexity\": results[\"memorized\"][\"perplexity\"],\n",
    "            \"membership\": {k: v for k, v in results[\"memorized\"][\"membership\"].items() if k not in [\"nll_forget_raw\", \"nll_retain_raw\"]},\n",
    "        },\n",
    "        \"unlearned\": {\n",
    "            \"books\": results[\"unlearned\"][\"books\"],\n",
    "            \"perplexity\": results[\"unlearned\"][\"perplexity\"],\n",
    "            \"membership\": {k: v for k, v in results[\"unlearned\"][\"membership\"].items() if k not in [\"nll_forget_raw\", \"nll_retain_raw\"]},\n",
    "        },\n",
    "        \"delta\": results[\"delta\"],\n",
    "        \"summary\": {\n",
    "            \"avg_rouge_l_before\": avg_rouge_before,\n",
    "            \"avg_rouge_l_after\": avg_rouge_after,\n",
    "            \"avg_semantic_before\": avg_sem_before,\n",
    "            \"avg_semantic_after\": avg_sem_after,\n",
    "        }\n",
    "    }\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(serializable, f, indent=2)\n",
    "    print(f\"\\nResults saved to {results_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# === RUN EVALUATION ===\n",
    "# Option 1: Evaluate only the unlearned model\n",
    "# eval_results = run_inference_only_evaluation()\n",
    "\n",
    "# Option 2: Compare memorized (before) vs unlearned (after) models\n",
    "# First copy your memorized_model folder to: ssu_unlearned_models/memorized_model/\n",
    "# Then uncomment:\n",
    "# comparison_results = run_before_after_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deaf902",
   "metadata": {
    "papermill": {
     "duration": 0.05637,
     "end_time": "2025-11-25T19:27:45.360932",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.304562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Verification & Testing\n",
    "\n",
    "**Automated checks** ensure the core SSU components work before running long jobs:\n",
    "- Dataset construction keeps the correct masking for `labels`, `labels_fgt`, and `labels_rnd`.\n",
    "- The custom trainer mixes retention and forgetting losses without shape errors.\n",
    "- A miniature pipeline run (concatenated forget + retain batches) executes an optimizer step without crashing.\n",
    "\n",
    "**Manual checks** after training runs:\n",
    "- Generate from a known passage of a removed book and confirm the model no longer regurgitates it.\n",
    "- Measure perplexity on `D_nor` before vs. after each step to ensure general capability stays stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d6ac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:27:45.478565Z",
     "iopub.status.busy": "2025-11-25T19:27:45.478141Z",
     "iopub.status.idle": "2025-11-25T19:27:45.493451Z",
     "shell.execute_reply": "2025-11-25T19:27:45.492750Z"
    },
    "papermill": {
     "duration": 0.078354,
     "end_time": "2025-11-25T19:27:45.494613",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.416259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automated sanity checks for dataset, trainer, and pipeline wiring\n",
    "def run_automated_tests():\n",
    "    tok = tokenizer\n",
    "    sample_texts = [\"Sanity sample text \" + str(i) for i in range(4)]\n",
    "\n",
    "    # Create dummy random_label_ids for forget dataset\n",
    "    dummy_tokenized = tok(\n",
    "        sample_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=Config.CHUNK_SIZE,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    dummy_random_label_ids = dummy_tokenized['input_ids']\n",
    "\n",
    "    # Dataset + collator check\n",
    "    forget_ds = SequentialUnlearningDataset(tok, sample_texts, mode=\"forget\", random_label_ids=dummy_random_label_ids)\n",
    "    retain_ds = SequentialUnlearningDataset(tok, sample_texts, mode=\"retain\")\n",
    "    collator = SSUDataCollator(tok)\n",
    "    batch = collator([forget_ds[0], retain_ds[0]])\n",
    "    assert 'labels' in batch and 'labels_fgt' in batch and 'labels_rnd' in batch\n",
    "    assert batch['labels'][0].eq(-100).all(), \"Forget sample should mask retention labels\"\n",
    "    assert not batch['labels'][1].eq(-100).all(), \"Retain sample should keep labels\"\n",
    "    assert not batch['labels_fgt'][0].eq(-100).all(), \"Forget sample must keep L_fgt\"\n",
    "    assert not batch['labels_rnd'][0].eq(-100).all(), \"Forget sample must keep L_rnd\"\n",
    "\n",
    "    # Trainer loss check - SSU loss should be λ1 * L_fgt + λ2 * L_rnd (no retention loss)\n",
    "    class MockModel(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.losses = [torch.tensor(0.4), torch.tensor(0.2)]  # fgt, rnd\n",
    "        def forward(self, *args, **kwargs):\n",
    "            loss_val = self.losses.pop(0) if self.losses else torch.tensor(0.0)\n",
    "            return SimpleNamespace(loss=loss_val)\n",
    "    mock_model = MockModel()\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(Config.OUTPUT_DIR, \"test_runs\"),\n",
    "        per_device_train_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        report_to=[],\n",
    "        logging_steps=1000,\n",
    "    )\n",
    "    trainer = SSUTrainer(model=mock_model, args=args, train_dataset=None)\n",
    "    dummy_inputs = {\n",
    "        'input_ids': torch.ones((1, 4), dtype=torch.long),\n",
    "        'attention_mask': torch.ones((1, 4), dtype=torch.long),\n",
    "        'labels': torch.ones((1, 4), dtype=torch.long),  # Not used in SSU loss\n",
    "        'labels_fgt': torch.ones((1, 4), dtype=torch.long),\n",
    "        'labels_rnd': torch.ones((1, 4), dtype=torch.long),\n",
    "    }\n",
    "    loss = trainer.compute_loss(mock_model, dummy_inputs)\n",
    "    expected = Config.EPSILON_1 * 0.4 + Config.EPSILON_2 * 0.2\n",
    "    assert abs(loss.item() - expected) < 1e-6, f\"SSU loss incorrect: expected {expected}, got {loss.item()}\"\n",
    "\n",
    "    # Mini pipeline batch check (forget-only batch, no retain)\n",
    "    args_with_columns = TrainingArguments(\n",
    "        output_dir=os.path.join(Config.OUTPUT_DIR, \"test_runs\"),\n",
    "        per_device_train_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        report_to=[],\n",
    "        logging_steps=1000,\n",
    "        remove_unused_columns=False,  # Keep custom columns like labels_fgt, labels_rnd\n",
    "    )\n",
    "    trainer = SSUTrainer(\n",
    "        model=mock_model,\n",
    "        args=args_with_columns,\n",
    "        train_dataset=forget_ds,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    data_loader = trainer.get_train_dataloader()\n",
    "    batch = next(iter(data_loader))\n",
    "    assert 'labels' in batch and 'labels_fgt' in batch and 'labels_rnd' in batch, \"Batch missing required labels\"\n",
    "\n",
    "    print(\"All automated SSU sanity tests passed!\")\n",
    "\n",
    "run_automated_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4025a",
   "metadata": {
    "papermill": {
     "duration": 0.057137,
     "end_time": "2025-11-25T19:27:45.610182",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.553045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Ablation Study: Removing `D_nor`\n",
    "\n",
    "To illustrate the value of retention data, re-run sequential unlearning with `D_nor` disabled.\n",
    "You should observe:\n",
    "- Regurgitation on the target books still decreases, proving the SSU losses work.\n",
    "- Perplexity on general data collapses, showing catastrophic forgetting without the retention anchor.\n",
    "\n",
    "After the run, compare the perplexity deltas and summarize them using:\n",
    "> Including D_nor stabilizes the model and reduces unlearning-induced degradation in general perplexity by **X%** compared to a naive SSU-only objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96570568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:27:45.734900Z",
     "iopub.status.busy": "2025-11-25T19:27:45.734631Z",
     "iopub.status.idle": "2025-11-25T19:27:45.739254Z",
     "shell.execute_reply": "2025-11-25T19:27:45.738579Z"
    },
    "papermill": {
     "duration": 0.073653,
     "end_time": "2025-11-25T19:27:45.740470",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.666817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_ablation_without_retention(start_step=1, end_step=None, **kwargs):\n",
    "    \"\"\"Helper to rerun SSU without D_nor for ablation studies.\"\"\"\n",
    "    original_flag = Config.USE_RETENTION_DATA\n",
    "    try:\n",
    "        Config.USE_RETENTION_DATA = False\n",
    "        print(\"\\n>>> Running ablation: retention data disabled\")\n",
    "        return run_sequential_unlearning(\n",
    "            start_step=start_step,\n",
    "            end_step=end_step,\n",
    "            run_evaluation=kwargs.get('run_evaluation', True),\n",
    "            general_validation_text=kwargs.get('general_validation_text'),\n",
    "        )\n",
    "    finally:\n",
    "        Config.USE_RETENTION_DATA = original_flag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308eb56d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:27:45.971005Z",
     "iopub.status.busy": "2025-11-25T19:27:45.970289Z",
     "iopub.status.idle": "2025-11-25T19:27:56.475759Z",
     "shell.execute_reply": "2025-11-25T19:27:56.474920Z"
    },
    "papermill": {
     "duration": 10.56259,
     "end_time": "2025-11-25T19:27:56.476882",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.914292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quick sanity generation with the latest unlearned model\n",
    "if 'final_unlearned_model' in globals():\n",
    "    final_unlearned_model.eval()\n",
    "    sample_device = next(final_unlearned_model.parameters()).device\n",
    "    demo_prompts = [\n",
    "        \"The quick brown fox\",\n",
    "        \"Once upon a time in a quiet village\",\n",
    "    ]\n",
    "    for idx, prompt in enumerate(demo_prompts, 1):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(sample_device)\n",
    "        with torch.no_grad():\n",
    "            gen_tokens = final_unlearned_model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                max_new_tokens=80,\n",
    "            )\n",
    "        gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "        print(f\"Prompt {idx}: {prompt}\\nGenerated: {gen_text}\\n\")\n",
    "else:\n",
    "    print(\"Run run_sequential_unlearning() first to instantiate final_unlearned_model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41805.428388,
   "end_time": "2025-11-25T19:27:59.352216",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-25T07:51:13.923828",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02acda10491b45038fb1e2f0942f2d97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "035d2008bce64747a7998e423ca9780c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0b1245a70b07496f99ff028b8af99b60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_77593008769b44efaad701ba8d75623e",
       "placeholder": "​",
       "style": "IPY_MODEL_c81d93a6daca4058a5e8f6309fd74fe2",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00G/2.00G [00:05&lt;00:00, 1.14GB/s]"
      }
     },
     "0b898674b20f4ee5ac35bcd785dda99c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18051c7b18764452879cd40c1b463bf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_73a35915f078455dbd10d2ab0242c5ba",
       "max": 899,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2b883a7534e246ad9ddf9e146ab1d283",
       "tabbable": null,
       "tooltip": null,
       "value": 899
      }
     },
     "199d756ec48c4065adaec3775c7edf0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0b898674b20f4ee5ac35bcd785dda99c",
       "placeholder": "​",
       "style": "IPY_MODEL_56f7ddaa2d644e4c8c3c241740387424",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "1a8ed895689c4df58ec830e6547dc57f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1aaebcc0fc2b47d18465d45068f97000": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ae6913021e44ce89002774972685e77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b20f1c5b6fd4706b2e1b7a43b95a899": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d127b2d8e164a44b4df0f74fa3e0dc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cdb79a490a604b3d8e1f83879f0714e9",
        "IPY_MODEL_18051c7b18764452879cd40c1b463bf5",
        "IPY_MODEL_a7ba6c9967c84f5d8a3c2dba113b0612"
       ],
       "layout": "IPY_MODEL_6e428fbb42c54140a9c76db28cfb2ee7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "237a447a4e2340d68279e2c0817770c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_59b9e1cf793d47fbb2b4bd3bb8401ce6",
       "placeholder": "​",
       "style": "IPY_MODEL_aeda55431b0042da9241e652eb6fd943",
       "tabbable": null,
       "tooltip": null,
       "value": " 215/215 [00:00&lt;00:00, 29.8kB/s]"
      }
     },
     "27a44111278e43a58bc4e7c49b7628f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2b883a7534e246ad9ddf9e146ab1d283": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3662d196631e4f95bf6ed838cadfd415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3bae2e8aac054a1d866692fe8e199a13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dfc147ebc52408c82cd8a1fa1ee5718": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_474bcc5066974eceaf41a8abf56b2d35",
       "placeholder": "​",
       "style": "IPY_MODEL_035d2008bce64747a7998e423ca9780c",
       "tabbable": null,
       "tooltip": null,
       "value": "added_tokens.json: 100%"
      }
     },
     "474bcc5066974eceaf41a8abf56b2d35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "497c508200a44fd89b393b76ef030768": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3dfc147ebc52408c82cd8a1fa1ee5718",
        "IPY_MODEL_982c1ac3a17b4384b15836ebab6b3749",
        "IPY_MODEL_fb9d09b3cfdc4937841934332fb7ff39"
       ],
       "layout": "IPY_MODEL_e827fceba3224d96b3a5c4340d4dc41d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4aa0de0e584c4a438934dc7ae9ddb11f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4b265d591e84450aaf386f1be0404706": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4bb15f9566854555bce69b81c8abb861": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "508353b4c9a148eb830c7da8750e9b21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9962764b9a0941dda6c9ab8f4ab0fdb8",
        "IPY_MODEL_5d7bae2fe3034ec3a31973330caaef4f",
        "IPY_MODEL_baa861b91611413cb56720ad32bcaf7f"
       ],
       "layout": "IPY_MODEL_d1d8a177373847088379d9b7d1c7cd01",
       "tabbable": null,
       "tooltip": null
      }
     },
     "52ae1bbb5bd144aaa35d93981a7aa6e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5566f70e96534b60ab9922892e40258a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56f7ddaa2d644e4c8c3c241740387424": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "59b9e1cf793d47fbb2b4bd3bb8401ce6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b889923a5134cb9971851cf1abab7d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d7bae2fe3034ec3a31973330caaef4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fc74398114cd450e896a62c4a9a8c723",
       "max": 33384568,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_02acda10491b45038fb1e2f0942f2d97",
       "tabbable": null,
       "tooltip": null,
       "value": 33384568
      }
     },
     "60125ddc02a5435d911a7493eea0b7be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6279fd9f074642618f38593c1540760c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6a1a9c40f4a54f3caffde47f037b94b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9a49c3eae3b24632a90a5a2a56839658",
       "placeholder": "​",
       "style": "IPY_MODEL_c4c80ecba06c42ee91411fef0f1a4f77",
       "tabbable": null,
       "tooltip": null,
       "value": " 662/662 [00:00&lt;00:00, 99.8kB/s]"
      }
     },
     "6e428fbb42c54140a9c76db28cfb2ee7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f0b988963a94ef59e55a0295bb2c2c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8a8d8cb2a23f4df09614934db1409acb",
        "IPY_MODEL_87ed6deaf6f747249cb2a5449b6c6cf7",
        "IPY_MODEL_89eadef8bb084e9586f58ac1b503f6b3"
       ],
       "layout": "IPY_MODEL_52ae1bbb5bd144aaa35d93981a7aa6e0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "73a35915f078455dbd10d2ab0242c5ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75d83b93087d4dbb87622ec9e8d9aec3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ee874e010c344cd68f85c41f75c75cb6",
       "max": 1999811208,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f05ca2fb9ce740a28f99ea950bca221e",
       "tabbable": null,
       "tooltip": null,
       "value": 1999811208
      }
     },
     "761b1e746bfc47a0b4454ddeedc1cae3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77593008769b44efaad701ba8d75623e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7897a88c25454680bb17248667fe28e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_761b1e746bfc47a0b4454ddeedc1cae3",
       "max": 215,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dc5baad2d58f47d9bccd84085098026d",
       "tabbable": null,
       "tooltip": null,
       "value": 215
      }
     },
     "797c731971a74ef69de9f1d8518bc671": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7a1e751d86f543ec9c65a9c2f5dda843": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80bc8506f2aa4475816063b1217d01da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "86c324a3f4bd4e4cad39cf8c7b906f69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "87ed6deaf6f747249cb2a5449b6c6cf7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_feae82167567425fbec91e30cff92c55",
       "max": 4689074,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f64d3402e2904bb5bc33a719dae771e1",
       "tabbable": null,
       "tooltip": null,
       "value": 4689074
      }
     },
     "89493e077229431a9a31a0140ae05be2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_199d756ec48c4065adaec3775c7edf0b",
        "IPY_MODEL_9d6468b19bb7479181372718388b7752",
        "IPY_MODEL_6a1a9c40f4a54f3caffde47f037b94b8"
       ],
       "layout": "IPY_MODEL_df7a0a529ec04fbc90c771e070dbd810",
       "tabbable": null,
       "tooltip": null
      }
     },
     "89eadef8bb084e9586f58ac1b503f6b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6279fd9f074642618f38593c1540760c",
       "placeholder": "​",
       "style": "IPY_MODEL_9ab5c73caef14f86955a253830ee19f3",
       "tabbable": null,
       "tooltip": null,
       "value": " 4.69M/4.69M [00:01&lt;00:00, 128kB/s]"
      }
     },
     "8a8d8cb2a23f4df09614934db1409acb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b20f1c5b6fd4706b2e1b7a43b95a899",
       "placeholder": "​",
       "style": "IPY_MODEL_8e19a1765f8c425bad00a344ee8a9a02",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.model: 100%"
      }
     },
     "8e19a1765f8c425bad00a344ee8a9a02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "90aa3edb75974a08b8ce8d33c8b28a15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9293eb120dd14f9899e058eb6bf21024": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "982c1ac3a17b4384b15836ebab6b3749": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4bb15f9566854555bce69b81c8abb861",
       "max": 35,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4aa0de0e584c4a438934dc7ae9ddb11f",
       "tabbable": null,
       "tooltip": null,
       "value": 35
      }
     },
     "9962764b9a0941dda6c9ab8f4ab0fdb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a1164b45ca95415a9d7eef81a3082c8b",
       "placeholder": "​",
       "style": "IPY_MODEL_4b265d591e84450aaf386f1be0404706",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.json: 100%"
      }
     },
     "9a49c3eae3b24632a90a5a2a56839658": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ab5c73caef14f86955a253830ee19f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9b21795e4d3244eca583850d39247dd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d6468b19bb7479181372718388b7752": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d79162cca6a14914be26c5ff9b5e0fe5",
       "max": 662,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_797c731971a74ef69de9f1d8518bc671",
       "tabbable": null,
       "tooltip": null,
       "value": 662
      }
     },
     "a1164b45ca95415a9d7eef81a3082c8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a6ee944603ae404b861e7492529369a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1a8ed895689c4df58ec830e6547dc57f",
       "placeholder": "​",
       "style": "IPY_MODEL_3662d196631e4f95bf6ed838cadfd415",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.16M/1.16M [00:00&lt;00:00, 3.17MB/s]"
      }
     },
     "a7ba6c9967c84f5d8a3c2dba113b0612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5b889923a5134cb9971851cf1abab7d1",
       "placeholder": "​",
       "style": "IPY_MODEL_f38cac89760d4828a415674755fb75a8",
       "tabbable": null,
       "tooltip": null,
       "value": " 899/899 [00:00&lt;00:00, 139kB/s]"
      }
     },
     "aeda55431b0042da9241e652eb6fd943": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b4e357b575fe4f79b9711a15f8e7798d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3bae2e8aac054a1d866692fe8e199a13",
       "placeholder": "​",
       "style": "IPY_MODEL_d695d551cd294506bdb085945b09858d",
       "tabbable": null,
       "tooltip": null,
       "value": "generation_config.json: 100%"
      }
     },
     "b9d9a19df93443d6aa26c8f030d5a9f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e9ac06f21ef846a3ab9fe0bedce218be",
       "max": 1156999,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_27a44111278e43a58bc4e7c49b7628f1",
       "tabbable": null,
       "tooltip": null,
       "value": 1156999
      }
     },
     "ba3b14fd8bf4473a959e0982d7dcb399": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b4e357b575fe4f79b9711a15f8e7798d",
        "IPY_MODEL_7897a88c25454680bb17248667fe28e9",
        "IPY_MODEL_237a447a4e2340d68279e2c0817770c1"
       ],
       "layout": "IPY_MODEL_dea76b4846784a55bbcdef3c02ebb3ed",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ba91e5eafcd948739f39a55cdbe0d5d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "baa861b91611413cb56720ad32bcaf7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9293eb120dd14f9899e058eb6bf21024",
       "placeholder": "​",
       "style": "IPY_MODEL_86c324a3f4bd4e4cad39cf8c7b906f69",
       "tabbable": null,
       "tooltip": null,
       "value": " 33.4M/33.4M [00:00&lt;00:00, 70.9MB/s]"
      }
     },
     "bb85e4d0feb84dbc8c24453b764769f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f55aa3fba28f431988a35588fdea02e6",
        "IPY_MODEL_75d83b93087d4dbb87622ec9e8d9aec3",
        "IPY_MODEL_0b1245a70b07496f99ff028b8af99b60"
       ],
       "layout": "IPY_MODEL_7a1e751d86f543ec9c65a9c2f5dda843",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c4c80ecba06c42ee91411fef0f1a4f77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c81d93a6daca4058a5e8f6309fd74fe2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cb8943fc75194bad8d71a2f4263dbdac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1ae6913021e44ce89002774972685e77",
       "placeholder": "​",
       "style": "IPY_MODEL_80bc8506f2aa4475816063b1217d01da",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "cdb79a490a604b3d8e1f83879f0714e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1aaebcc0fc2b47d18465d45068f97000",
       "placeholder": "​",
       "style": "IPY_MODEL_5566f70e96534b60ab9922892e40258a",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "d1d8a177373847088379d9b7d1c7cd01": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d695d551cd294506bdb085945b09858d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d79162cca6a14914be26c5ff9b5e0fe5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc5baad2d58f47d9bccd84085098026d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dea76b4846784a55bbcdef3c02ebb3ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df7a0a529ec04fbc90c771e070dbd810": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e827fceba3224d96b3a5c4340d4dc41d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9ac06f21ef846a3ab9fe0bedce218be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee874e010c344cd68f85c41f75c75cb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f05ca2fb9ce740a28f99ea950bca221e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f38cac89760d4828a415674755fb75a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f4f43fa4839f49c980312dc3fc378685": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f55aa3fba28f431988a35588fdea02e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_60125ddc02a5435d911a7493eea0b7be",
       "placeholder": "​",
       "style": "IPY_MODEL_9b21795e4d3244eca583850d39247dd7",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "f64d3402e2904bb5bc33a719dae771e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fb9d09b3cfdc4937841934332fb7ff39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f4f43fa4839f49c980312dc3fc378685",
       "placeholder": "​",
       "style": "IPY_MODEL_ba91e5eafcd948739f39a55cdbe0d5d7",
       "tabbable": null,
       "tooltip": null,
       "value": " 35.0/35.0 [00:00&lt;00:00, 4.47kB/s]"
      }
     },
     "fc74398114cd450e896a62c4a9a8c723": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd9ac64136b14ce8ad1ca035536ece8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cb8943fc75194bad8d71a2f4263dbdac",
        "IPY_MODEL_b9d9a19df93443d6aa26c8f030d5a9f3",
        "IPY_MODEL_a6ee944603ae404b861e7492529369a4"
       ],
       "layout": "IPY_MODEL_90aa3edb75974a08b8ce8d33c8b28a15",
       "tabbable": null,
       "tooltip": null
      }
     },
     "feae82167567425fbec91e30cff92c55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
