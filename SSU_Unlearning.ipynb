{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052809e5",
   "metadata": {
    "papermill": {
     "duration": 0.005385,
     "end_time": "2025-11-25T07:51:17.427946",
     "exception": false,
     "start_time": "2025-11-25T07:51:17.422561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Stable Sequential Unlearning (SSU) Framework\n",
    "\n",
    "Complete implementation following the paper methodology:\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Initial Fine-tuning (Step 0)**: Fine-tune vanilla model on all copyrighted books (D_f) to make it memorize them\n",
    "2. **Sequential Unlearning (Steps 1-N)**: Unlearn books one at a time using SSU methodology\n",
    "   - Each step unlearns one book (D_f^t)\n",
    "   - Uses composite loss (L_fgt + L_rnd) and weight saliency\n",
    "   - Applies task vector negation\n",
    "\n",
    "## Datasets:\n",
    "- **D_f**: All copyrighted books (10 books from Project Gutenberg)\n",
    "- **D_f^t**: Book to unlearn at time step t\n",
    "- **D_prev**: Previously unlearned books (aggregated from previous steps)\n",
    "- **D_nor**: Retention data (200 chunks from 100 other books) - for evaluation\n",
    "\n",
    "Works on both local and Kaggle environments with automatic retry logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481067ca",
   "metadata": {
    "papermill": {
     "duration": 0.00394,
     "end_time": "2025-11-25T07:51:17.436097",
     "exception": false,
     "start_time": "2025-11-25T07:51:17.432157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c631588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:51:17.445197Z",
     "iopub.status.busy": "2025-11-25T07:51:17.444933Z",
     "iopub.status.idle": "2025-11-25T07:52:30.248465Z",
     "shell.execute_reply": "2025-11-25T07:52:30.247206Z"
    },
    "papermill": {
     "duration": 72.80991,
     "end_time": "2025-11-25T07:52:30.249989",
     "exception": false,
     "start_time": "2025-11-25T07:51:17.440079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q torch transformers peft datasets accelerate requests protobuf==3.20.3 rouge-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d67ca3",
   "metadata": {
    "papermill": {
     "duration": 0.017911,
     "end_time": "2025-11-25T07:52:30.292651",
     "exception": false,
     "start_time": "2025-11-25T07:52:30.274740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2892a846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:52:30.328466Z",
     "iopub.status.busy": "2025-11-25T07:52:30.328187Z",
     "iopub.status.idle": "2025-11-25T07:53:06.914435Z",
     "shell.execute_reply": "2025-11-25T07:53:06.913682Z"
    },
    "papermill": {
     "duration": 36.623607,
     "end_time": "2025-11-25T07:53:06.933255",
     "exception": false,
     "start_time": "2025-11-25T07:52:30.309648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohammad\\Documents\\GitHub\\DL\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.compile disabled globally (for Tesla P100 compatibility)\n"
     ]
    }
   ],
   "source": [
    "# Disable torch.compile globally to avoid CUDA capability issues\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch._dynamo\n",
    "\n",
    "from typing import Dict, Sequence\n",
    "from types import SimpleNamespace\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Set environment variable to disable torch compilation\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "\n",
    "# Also disable dynamo\n",
    "torch._dynamo.config.disable = True\n",
    "\n",
    "print(\"torch.compile disabled globally (for Tesla P100 compatibility)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29997444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:06.969170Z",
     "iopub.status.busy": "2025-11-25T07:53:06.968607Z",
     "iopub.status.idle": "2025-11-25T07:53:06.976373Z",
     "shell.execute_reply": "2025-11-25T07:53:06.975604Z"
    },
    "papermill": {
     "duration": 0.027117,
     "end_time": "2025-11-25T07:53:06.977499",
     "exception": false,
     "start_time": "2025-11-25T07:53:06.950382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n"
     ]
    }
   ],
   "source": [
    "# Configuration Class\n",
    "class Config:\n",
    "    # Model Configuration - Use smaller model to avoid download issues\n",
    "    MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "    \n",
    "    # Alternative options:\n",
    "    # MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # 3.8B, non-gated\n",
    "    # MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small, fast, non-gated\n",
    "    \n",
    "    TOKENIZER_NAME = MODEL_NAME\n",
    "    \n",
    "    # HuggingFace Authentication\n",
    "    USE_HF_TOKEN = True  # Set True for gated models\n",
    "    \n",
    "    # PEFT/LoRA Configuration - Match paper values\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "    # Sequential Unlearning Configuration\n",
    "    NUM_UNLEARNING_STEPS = 10  # Number of sequential unlearning steps (paper uses 10)\n",
    "    \n",
    "    # Fine-Tuning Hyperparameters\n",
    "    BATCH_SIZE = 1  # Reduced further to avoid OOM\n",
    "    GRADIENT_ACCUMULATION_STEPS = 16\n",
    "    LEARNING_RATE = 5e-5  # Base LR, will be overridden per step (1e-5 for steps 1-5, 1e-6 for 6-10)\n",
    "    NUM_EPOCHS_FT = 1  # 1 epoch for initial fine-tuning (as per paper)\n",
    "    NUM_EPOCHS_INITIAL_FT = 1  # Initial fine-tuning on all books (D_f)\n",
    "    WARMUP_STEPS = 10\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    \n",
    "    # SSU Methodology Parameters - Match paper values\n",
    "    EPSILON_1 = 1.0  # Weight for Forgetting Loss (L_fgt) - λ1\n",
    "    EPSILON_2 = 0.1  # Weight for Random Labeling Loss (L_rnd) - λ2\n",
    "    # GAMMA removed - now computed dynamically as mu + sigma from gradients\n",
    "    \n",
    "    # Data Configuration\n",
    "    CHUNK_SIZE = 256\n",
    "    NUM_CHUNKS_PER_STEP = 50\n",
    "    USE_REAL_BOOKS = True  # Use real books from Project Gutenberg\n",
    "    DATA_DIR = \"gutenberg_books\"\n",
    "    \n",
    "    # Project Gutenberg Book IDs - Exact 10 books from paper (in order)\n",
    "    ALL_BOOK_IDS = [\n",
    "        1661,   # Sherlock Holmes - Step 1\n",
    "        84,     # Frankenstein - Step 2\n",
    "        1342,   # Pride and Prejudice - Step 3\n",
    "        11,     # Alice in Wonderland - Step 4\n",
    "        2701,   # Moby Dick - Step 5\n",
    "        74,     # The Adventures of Tom Sawyer - Step 6\n",
    "        98,     # A Tale of Two Cities - Step 7\n",
    "        5200,   # Metamorphosis - Step 8\n",
    "        6130,   # The Iliad - Step 9\n",
    "        174,    # The Picture of Dorian Gray - Step 10\n",
    "    ]\n",
    "    \n",
    "    # Books to unlearn at each time step (sequential) - 10 steps\n",
    "    GUTENBERG_BOOK_IDS = {\n",
    "        1: [1661],   # Sherlock Holmes\n",
    "        2: [84],     # Frankenstein\n",
    "        3: [1342],   # Pride and Prejudice\n",
    "        4: [11],     # Alice in Wonderland\n",
    "        5: [2701],   # Moby Dick\n",
    "        6: [74],     # The Adventures of Tom Sawyer\n",
    "        7: [98],     # A Tale of Two Cities\n",
    "        8: [5200],   # Metamorphosis\n",
    "        9: [6130],   # The Iliad\n",
    "        10: [174],   # The Picture of Dorian Gray\n",
    "    }\n",
    "    \n",
    "    # Retention data (D_nor) - 200 chunks from 100 other books (paper specification)\n",
    "    USE_RETENTION_DATA = True  # D_nor used for evaluation and random labels\n",
    "    NUM_RETENTION_BOOKS = 100  # Paper uses 100 books\n",
    "    NUM_RETENTION_CHUNKS = 200  # Paper uses 200 chunks\n",
    "    \n",
    "    # Storage / disk usage configuration\n",
    "    SAVE_MEMORIZED_MODEL = False  # Avoid saving large memorized checkpoint unless needed\n",
    "    SAVE_STEP_MODELS = False      # Only persist final model by default\n",
    "    DELETE_PREVIOUS_STEP_MODELS = True  # Remove older step checkpoints when new ones are saved\n",
    "    KEEP_DOWNLOADED_BOOKS = False  # Remove raw .txt files after chunking to save space\n",
    "    CLEANUP_FINAL_MODEL_DIR = False  # Optional: remove final model dir after exporting elsewhere\n",
    "    \n",
    "    # Evaluation Configuration\n",
    "    EVAL_NUM_SAMPLES = 10  # Number of samples per prompt for regurgitation evaluation (paper uses 10)\n",
    "    EVAL_MAX_PAIRS = 10  # Number of prompt pairs per book for evaluation\n",
    "    \n",
    "    OUTPUT_DIR = \"ssu_unlearned_models\"\n",
    "\n",
    "print(\"Configuration loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1eeb34",
   "metadata": {
    "papermill": {
     "duration": 0.023724,
     "end_time": "2025-11-25T07:53:07.024836",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.001112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Environment Detection & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cfcc96e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.061633Z",
     "iopub.status.busy": "2025-11-25T07:53:07.061367Z",
     "iopub.status.idle": "2025-11-25T07:53:07.593520Z",
     "shell.execute_reply": "2025-11-25T07:53:07.592641Z"
    },
    "papermill": {
     "duration": 0.552754,
     "end_time": "2025-11-25T07:53:07.594860",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.042106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: Local\n",
      "Successfully logged in to HuggingFace.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Detect if running on Kaggle\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running on: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "\n",
    "# HuggingFace Authentication (if needed)\n",
    "if Config.USE_HF_TOKEN:\n",
    "    from huggingface_hub import login\n",
    "    \n",
    "    hf_token = None\n",
    "    \n",
    "    # Try Kaggle Secrets\n",
    "    if IS_KAGGLE:\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            user_secrets = UserSecretsClient()\n",
    "            hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "            print(\"Found HuggingFace token in Kaggle Secrets.\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try environment variable\n",
    "    if not hf_token:\n",
    "        hf_token = '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'\n",
    "    \n",
    "    if hf_token:\n",
    "        try:\n",
    "            login(token=hf_token, add_to_git_credential=False)\n",
    "            print(\"Successfully logged in to HuggingFace.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not login: {e}\")\n",
    "    else:\n",
    "        print(\"WARNING: No HuggingFace token found. Gated models will fail.\")\n",
    "else:\n",
    "    print(\"Using non-gated model - no authentication needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9d2455",
   "metadata": {
    "papermill": {
     "duration": 0.017311,
     "end_time": "2025-11-25T07:53:07.630223",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.612912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. SSU Model & Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93131985",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.666342Z",
     "iopub.status.busy": "2025-11-25T07:53:07.666048Z",
     "iopub.status.idle": "2025-11-25T07:53:07.703723Z",
     "shell.execute_reply": "2025-11-25T07:53:07.702890Z"
    },
    "papermill": {
     "duration": 0.057489,
     "end_time": "2025-11-25T07:53:07.704812",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.647323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data utilities loaded!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Dummy book text for simulation\n",
    "DUMMY_BOOK_TEXT = \"\"\"\n",
    "In the beginning God created the heavens and the earth. Now the earth was formless and empty, darkness was over the surface of the deep, and the Spirit of God was hovering over the waters. And God said, \"Let there be light,\" and there was light. God saw that the light was good, and he separated the light from the darkness. God called the light \"day,\" and the darkness he called \"night.\" And there was evening, and there was morning—the first day. \n",
    "\n",
    "And God said, \"Let there be a vault between the waters to separate water from water.\" So God made the vault and separated the water under the vault from the water above it. And it was so. God called the vault \"sky.\" And there was evening, and there was morning—the second day.\n",
    "\n",
    "And God said, \"Let the water under the sky be gathered to one place, and let dry ground appear.\" And it was so. God called the dry ground \"land,\" and the gathered waters he called \"seas.\" And God saw that it was good. Then God said, \"Let the land produce vegetation: seed-bearing plants and trees on the land that bear fruit with seed in it, according to their various kinds.\" And it was so. The land produced vegetation: plants bearing seed according to their kinds and trees bearing fruit with seed in it according to their kinds. And God saw that it was good. And there was evening, and there was morning—the third day.\n",
    "\n",
    "And God said, \"Let there be lights in the vault of the sky to separate the day from the night, and let them serve as signs to mark sacred times, and days and years, and let them be lights in the vault of the sky to give light on the earth.\" And it was so. God made two great lights—the greater light to govern the day and the lesser light to govern the night. He also made the stars. God set them in the vault of the sky to give light on the earth, to govern the day and the night, and to separate light from darkness. And God saw that it was good. And there was evening, and there was morning—the fourth day.\n",
    "\n",
    "And God said, \"Let the water teem with living creatures, and let birds fly above the earth across the vault of the sky.\" So God created the great creatures of the sea and every living thing with which the water teems and that moves about in it, according to their kinds, and every winged bird according to its kind. And God saw that it was good. God blessed them and said, \"Be fruitful and increase in number and fill the water in the seas, and let the birds increase on the earth.\" And there was evening, and there was morning—the fifth day.\n",
    "\n",
    "And God said, \"Let the land produce living creatures according to their kinds: the livestock, the creatures that move along the ground, and the wild animals, each according to its kind.\" And it was so. God made the wild animals according to their kinds, the livestock according to their kinds, and all the creatures that move along the ground according to their kinds. And God saw that it was good. Then God said, \"Let us make mankind in our image, in our likeness, so that they may rule over the fish in the sea and the birds in the sky, over the livestock and all the wild animals, and over all the creatures that move along the ground.\" So God created mankind in his own image, in the image of God he created them; male and female he created them. God blessed them and said to them, \"Be fruitful and increase in number; fill the earth and subdue it. Rule over the fish in the sea and the birds in the sky and over every living creature that moves on the ground.\" Then God said, \"I give you every seed-bearing plant on the face of the whole earth and every tree that has fruit with seed in it. They will be yours for food. And to all the beasts of the earth and all the birds in the sky and all the creatures that move along the ground—everything that has the breath of life in it—I give every green plant for food.\" And it was so. God saw all that he had made, and it was very good. And there was evening, and there was morning—the sixth day.\n",
    "\n",
    "Thus the heavens and the earth were completed in all their vast array. By the seventh day God had finished the work he had been doing; so on the seventh day he rested from all his work. Then God blessed the seventh day and made it holy, because on it he rested from all the work of creating that he had done.\n",
    "\"\"\" * 50\n",
    "\n",
    "\n",
    "def generate_simulated_data(text, chunk_size, num_chunks, tokenizer_name):\n",
    "    \"\"\"Simulates a list of text chunks for one book (D_f^t).\n",
    "    \n",
    "    This function splits long book text into smaller chunks that fit within the model's\n",
    "    maximum sequence length. It does this by:\n",
    "    1. Splitting text into small word chunks (to avoid tokenization warnings)\n",
    "    2. Tokenizing each small chunk separately\n",
    "    3. Combining tokenized chunks to create final chunks of the desired size\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Get model max length - this is the MAXIMUM tokens the model can handle\n",
    "    # TinyLlama has max_length of 2048 tokens\n",
    "    max_length = getattr(tokenizer, 'model_max_length', 32000)\n",
    "\n",
    "    # Fix for huge model_max_length causing OverflowError\n",
    "    if max_length > 100000:\n",
    "        max_length = 8192  # Set a reasonable limit\n",
    "    \n",
    "    # Use a safe chunk size that's smaller than model max length\n",
    "    # We use chunk_size from config (256) but ensure it's under the model limit\n",
    "    safe_chunk_size = min(chunk_size, max_length - 10)  # Leave some margin for safety\n",
    "    \n",
    "    # IMPORTANT: We need to split text into VERY small pieces before tokenizing\n",
    "    # Why? Because 1 word can become 1-3 tokens, and we want to stay under max_length\n",
    "    # Strategy: Tokenize in small batches (500-800 words max) to avoid warnings\n",
    "    words = text.split()\n",
    "    \n",
    "    # Conservative estimate: ~1.5 tokens per word on average\n",
    "    # So for max_length=2048, we want max ~1300 words per tokenization batch\n",
    "    # But to be extra safe, we'll use even smaller: 500 words per batch\n",
    "    words_per_batch = min(500, max_length // 3)  # Very conservative: 500 words max per batch\n",
    "    \n",
    "    # Step 1: Tokenize text in small batches to avoid warnings\n",
    "    all_token_ids = []\n",
    "    for i in range(0, len(words), words_per_batch):\n",
    "        batch_text = ' '.join(words[i:i + words_per_batch])\n",
    "        # Tokenize with truncation - this ensures we never exceed max_length\n",
    "        tokenized = tokenizer(\n",
    "            batch_text, \n",
    "            return_tensors='pt', \n",
    "            truncation=True,  # This truncates if too long\n",
    "            max_length=max_length,  # Use model's actual max length\n",
    "            add_special_tokens=True\n",
    "        )['input_ids'][0]\n",
    "        all_token_ids.extend(tokenized.tolist())\n",
    "    \n",
    "    # Step 2: If we don't have enough tokens, repeat the sequence\n",
    "    if len(all_token_ids) < safe_chunk_size * num_chunks:\n",
    "        repeat_factor = (safe_chunk_size * num_chunks // len(all_token_ids)) + 1\n",
    "        all_token_ids = (all_token_ids * repeat_factor)[:safe_chunk_size * num_chunks * 2]\n",
    "    \n",
    "    # Step 3: Split into chunks of the desired size\n",
    "    chunks = []\n",
    "    for i in range(0, len(all_token_ids) - safe_chunk_size + 1, safe_chunk_size):\n",
    "        chunk = all_token_ids[i:i + safe_chunk_size]\n",
    "        if len(chunk) == safe_chunk_size:\n",
    "            chunks.append(chunk)\n",
    "        if len(chunks) >= num_chunks:\n",
    "            break\n",
    "    \n",
    "    # Step 4: If we still don't have enough, pad the last chunk\n",
    "    while len(chunks) < num_chunks:\n",
    "        if chunks:\n",
    "            # Repeat last chunk or pad\n",
    "            last_chunk = chunks[-1][:safe_chunk_size]\n",
    "            if len(last_chunk) < safe_chunk_size:\n",
    "                last_chunk = last_chunk + all_token_ids[:safe_chunk_size - len(last_chunk)]\n",
    "            chunks.append(last_chunk[:safe_chunk_size])\n",
    "        else:\n",
    "            # If no chunks at all, create a dummy chunk\n",
    "            chunks.append(all_token_ids[:safe_chunk_size])\n",
    "    \n",
    "    chunks = chunks[:num_chunks]\n",
    "    \n",
    "    # Step 5: Decode back to text (this is what the dataset will use)\n",
    "    text_chunks = [tokenizer.decode(c, skip_special_tokens=True) for c in chunks]\n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "def maybe_delete_file(file_path):\n",
    "    \"\"\"Delete a file if KEEP_DOWNLOADED_BOOKS is False.\"\"\"\n",
    "    if Config.KEEP_DOWNLOADED_BOOKS:\n",
    "        return\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "def cleanup_dir(path):\n",
    "    \"\"\"Remove a directory tree if it exists.\"\"\"\n",
    "    if path and os.path.exists(path):\n",
    "        shutil.rmtree(path, ignore_errors=True)\n",
    "\n",
    "\n",
    "class SequentialUnlearningDataset(Dataset):\n",
    "    \"\"\"Custom Dataset that supports SSU forget data and retention data.\"\"\"\n",
    "    def __init__(self, tokenizer, data_texts, mode=\"forget\", random_label_ids=None):\n",
    "        if mode not in {\"forget\", \"retain\"}:\n",
    "            raise ValueError(f\"Unsupported dataset mode: {mode}\")\n",
    "        self.mode = mode\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_texts = data_texts\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            data_texts, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=Config.CHUNK_SIZE, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.input_ids = tokenized['input_ids']\n",
    "        self.attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "        self.random_label_ids = random_label_ids\n",
    "        self.random_indices = None\n",
    "        if self.mode == \"forget\":\n",
    "            if self.random_label_ids is None:\n",
    "                raise ValueError(\"random_label_ids is required for forget mode\")\n",
    "            self.random_indices = list(range(self.random_label_ids.size(0)))\n",
    "            random.shuffle(self.random_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx].clone()\n",
    "        attention_mask = self.attention_mask[idx].clone()\n",
    "        sample = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "        }\n",
    "\n",
    "        if self.mode == \"forget\":\n",
    "            labels_fgt = input_ids.clone()\n",
    "            # Sample labels_rnd from D_nor (random_label_ids), not from D_f\n",
    "            rnd_idx = self.random_indices[idx % len(self.random_indices)]\n",
    "            labels_rnd = self.random_label_ids[rnd_idx].clone()\n",
    "            sample.update({\n",
    "                'labels_fgt': labels_fgt,\n",
    "                'labels_rnd': labels_rnd,\n",
    "            })\n",
    "        else:\n",
    "            sample['labels'] = input_ids.clone()\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "class SSUDataCollator:\n",
    "    \"\"\"Custom data collator supporting mixed forget/retain batches.\"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        \n",
    "        if not features:\n",
    "            raise ValueError(\"Empty features list passed to data collator\")\n",
    "        \n",
    "        def to_tensor(x):\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                return x\n",
    "            return torch.tensor(x, dtype=torch.long)\n",
    "        \n",
    "        input_ids = [to_tensor(f['input_ids']) for f in features]\n",
    "        attention_mask = [to_tensor(f['attention_mask']) for f in features]\n",
    "        \n",
    "        batch = {}\n",
    "        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
    "        batch['input_ids'] = pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=pad_token_id,\n",
    "        )\n",
    "        batch['attention_mask'] = pad_sequence(\n",
    "            attention_mask,\n",
    "            batch_first=True,\n",
    "            padding_value=0,\n",
    "        )\n",
    "        max_len = batch['input_ids'].size(1)\n",
    "        batch_size = len(features)\n",
    "        device = batch['input_ids'].device\n",
    "        dtype = batch['input_ids'].dtype\n",
    "        \n",
    "        has_retain = any('labels' in f for f in features)\n",
    "        has_forget = any('labels_fgt' in f for f in features)\n",
    "        \n",
    "        # IMPORTANT FIX: Always create 'labels' field (Trainer expects it)\n",
    "        # For forget-only batches, labels will be all -100\n",
    "        labels = torch.full((batch_size, max_len), -100, dtype=dtype, device=device)\n",
    "        for idx, f in enumerate(features):\n",
    "            if 'labels' in f:\n",
    "                data = to_tensor(f['labels']).to(device)\n",
    "                labels[idx, :data.shape[0]] = data\n",
    "        batch['labels'] = labels\n",
    "        \n",
    "        if has_forget:\n",
    "            labels_fgt = torch.full((batch_size, max_len), -100, dtype=dtype, device=device)\n",
    "            labels_rnd = torch.full((batch_size, max_len), -100, dtype=dtype, device=device)\n",
    "            for idx, f in enumerate(features):\n",
    "                if 'labels_fgt' in f:\n",
    "                    data_fgt = to_tensor(f['labels_fgt']).to(device)\n",
    "                    labels_fgt[idx, :data_fgt.shape[0]] = data_fgt\n",
    "                if 'labels_rnd' in f:\n",
    "                    data_rnd = to_tensor(f['labels_rnd']).to(device)\n",
    "                    labels_rnd[idx, :data_rnd.shape[0]] = data_rnd\n",
    "            batch['labels_fgt'] = labels_fgt\n",
    "            batch['labels_rnd'] = labels_rnd\n",
    "        \n",
    "        return batch\n",
    "\n",
    "def download_gutenberg_book(book_id, output_dir):\n",
    "    \"\"\"Download a book from Project Gutenberg by ID.\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to downloaded book file, or None if download failed.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    book_file = os.path.join(output_dir, f\"{book_id}.txt\")\n",
    "    \n",
    "    if os.path.exists(book_file):\n",
    "        print(f\"Book {book_id} already exists, skipping download.\")\n",
    "        return book_file\n",
    "    \n",
    "    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "    try:\n",
    "        print(f\"Downloading book {book_id} from Project Gutenberg...\")\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(book_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        print(f\"Downloaded book {book_id} successfully.\")\n",
    "        return book_file\n",
    "    except:\n",
    "        url_alt = f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "        try:\n",
    "            response = requests.get(url_alt, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            with open(book_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Downloaded book {book_id} successfully.\")\n",
    "            return book_file\n",
    "        except Exception as e:\n",
    "            print(f\"Error: Could not download book {book_id} from Project Gutenberg.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def load_book_text(book_file):\n",
    "    \"\"\"Load and clean text from a book file.\"\"\"\n",
    "    if not book_file or not os.path.exists(book_file):\n",
    "        return None\n",
    "    with open(book_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    # Remove Project Gutenberg headers/footers\n",
    "    start_markers = [\"*** START OF\", \"***START OF\", \"START OF THE PROJECT\"]\n",
    "    end_markers = [\"*** END OF\", \"***END OF\", \"END OF THE PROJECT\"]\n",
    "    for marker in start_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            text = text[text.find('\\n', idx) + 1:]\n",
    "            break\n",
    "    for marker in end_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "            break\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_all_books_for_initial_finetuning():\n",
    "    \"\"\"Downloads all books for initial fine-tuning (D_f) - makes model memorize them.\"\"\"\n",
    "    print(\"\\n=== Downloading all books for initial fine-tuning (D_f) ===\")\n",
    "    all_books_dir = os.path.join(Config.DATA_DIR, \"all_books\")\n",
    "    os.makedirs(all_books_dir, exist_ok=True)\n",
    "    \n",
    "    book_texts = []\n",
    "    for book_id in Config.ALL_BOOK_IDS:\n",
    "        book_file = download_gutenberg_book(book_id, all_books_dir)\n",
    "        if not book_file:\n",
    "            raise RuntimeError(f\"Failed to download book {book_id} for initial fine-tuning\")\n",
    "        text = load_book_text(book_file)\n",
    "        if not text or len(text) < 1000:\n",
    "            raise RuntimeError(f\"Book {book_id} text is invalid or too short for initial fine-tuning\")\n",
    "        book_texts.append(text)\n",
    "        maybe_delete_file(book_file)\n",
    "        print(f\"Loaded book {book_id} ({len(text)} chars)\")\n",
    "    \n",
    "    if not book_texts:\n",
    "        raise RuntimeError(\"No books downloaded for initial fine-tuning\")\n",
    "    \n",
    "    return book_texts\n",
    "\n",
    "\n",
    "def get_unlearning_datasets(random_label_ids=None):\n",
    "    \"\"\"Generates sequential datasets D_f^1, D_f^2, ... for each time step.\n",
    "    \n",
    "    Args:\n",
    "        random_label_ids: Tensor of input_ids from D_nor to use for labels_rnd.\n",
    "                          Required for forget mode datasets.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    for t in range(Config.NUM_UNLEARNING_STEPS):\n",
    "        print(f\"\\n--- Preparing dataset D_f^{t+1} for time step {t+1} ---\")\n",
    "        \n",
    "        if Config.USE_REAL_BOOKS:\n",
    "            # Download books for this specific time step\n",
    "            time_step_dir = os.path.join(Config.DATA_DIR, f\"time_step_{t+1}\")\n",
    "            os.makedirs(time_step_dir, exist_ok=True)\n",
    "            \n",
    "            book_ids = Config.GUTENBERG_BOOK_IDS.get(t + 1, [])\n",
    "            book_texts = []\n",
    "            for book_id in book_ids:\n",
    "                book_file = download_gutenberg_book(book_id, time_step_dir)\n",
    "                if not book_file:\n",
    "                    raise RuntimeError(f\"Failed to download book {book_id} for step {t+1}\")\n",
    "                text = load_book_text(book_file)\n",
    "                if not text or len(text) < 1000:\n",
    "                    raise RuntimeError(f\"Book {book_id} text is invalid or too short for step {t+1}\")\n",
    "                book_texts.append(text)\n",
    "                maybe_delete_file(book_file)\n",
    "                print(f\"Loaded book {book_id} for step {t+1}\")\n",
    "            \n",
    "            if not book_texts:\n",
    "                raise RuntimeError(f\"No valid books for step {t+1}\")\n",
    "        else:\n",
    "            raise RuntimeError(\"USE_REAL_BOOKS must be True for paper reproduction\")\n",
    "        \n",
    "        all_chunks = []\n",
    "        for book_text in book_texts:\n",
    "            chunks = generate_simulated_data(\n",
    "                book_text,\n",
    "                Config.CHUNK_SIZE,\n",
    "                Config.NUM_CHUNKS_PER_STEP // len(book_texts) + 1,\n",
    "                Config.TOKENIZER_NAME\n",
    "            )\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        data_t = all_chunks[:Config.NUM_CHUNKS_PER_STEP]\n",
    "        print(f\"Created {len(data_t)} chunks for time step {t+1}\")\n",
    "        \n",
    "        # Create forget dataset with random_label_ids from D_nor\n",
    "        dataset_t = SequentialUnlearningDataset(\n",
    "            tokenizer, \n",
    "            data_t, \n",
    "            mode=\"forget\",\n",
    "            random_label_ids=random_label_ids\n",
    "        )\n",
    "        datasets.append(dataset_t)\n",
    "        \n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_retention_dataset():\n",
    "    \"\"\"Generates retention dataset D_nor (non-targeted data to keep).\n",
    "    \n",
    "    Returns dataset with 200 chunks from 100 books disjoint from ALL_BOOK_IDS.\n",
    "    \"\"\"\n",
    "    if not Config.USE_RETENTION_DATA:\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n=== Preparing retention dataset D_nor ===\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Generate 100 book IDs that are NOT in the unlearning set (ALL_BOOK_IDS)\n",
    "    # Use a range of common Gutenberg IDs, excluding those in ALL_BOOK_IDS\n",
    "    excluded_ids = set(Config.ALL_BOOK_IDS)\n",
    "    candidate_ids = []\n",
    "    # Common Gutenberg book IDs (expanded list)\n",
    "    common_ids = [\n",
    "        1232, 145, 76, 2591, 30254, 844, 345, 520, 6130, 174,\n",
    "        1342, 11, 2701, 74, 98, 5200, 6130, 174, 1661, 84,\n",
    "        100, 200, 300, 400, 500, 600, 700, 800, 900, 1000,\n",
    "        1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000,\n",
    "        2100, 2200, 2300, 2400, 2500, 2600, 2800, 2900, 3000, 3100,\n",
    "        3200, 3300, 3400, 3500, 3600, 3700, 3800, 3900, 4000, 4100,\n",
    "        4200, 4300, 4400, 4500, 4600, 4700, 4800, 4900, 5000, 5100,\n",
    "        5300, 5400, 5500, 5600, 5700, 5800, 5900, 6000, 6100, 6200,\n",
    "        6300, 6400, 6500, 6600, 6700, 6800, 6900, 7000, 7100, 7200,\n",
    "        7300, 7400, 7500, 7600, 7700, 7800, 7900, 8000, 8100, 8200,\n",
    "    ]\n",
    "    \n",
    "    for bid in common_ids:\n",
    "        if bid not in excluded_ids and len(candidate_ids) < Config.NUM_RETENTION_BOOKS:\n",
    "            candidate_ids.append(bid)\n",
    "    \n",
    "    # If we don't have enough, generate more sequential IDs\n",
    "    next_id = 10000\n",
    "    while len(candidate_ids) < Config.NUM_RETENTION_BOOKS:\n",
    "        if next_id not in excluded_ids:\n",
    "            candidate_ids.append(next_id)\n",
    "        next_id += 1\n",
    "        if next_id > 100000:  # Safety limit\n",
    "            break\n",
    "    \n",
    "    retention_book_ids = candidate_ids[:Config.NUM_RETENTION_BOOKS]\n",
    "    retention_dir = os.path.join(Config.DATA_DIR, \"retention_books\")\n",
    "    os.makedirs(retention_dir, exist_ok=True)\n",
    "    \n",
    "    all_chunks = []\n",
    "    successful_books = 0\n",
    "    for book_id in retention_book_ids:\n",
    "        book_file = download_gutenberg_book(book_id, retention_dir)\n",
    "        if book_file:\n",
    "            text = load_book_text(book_file)\n",
    "            if text and len(text) > 1000:\n",
    "                chunks = generate_simulated_data(\n",
    "                    text,\n",
    "                    Config.CHUNK_SIZE,\n",
    "                    Config.NUM_RETENTION_CHUNKS // Config.NUM_RETENTION_BOOKS + 1,\n",
    "                    Config.TOKENIZER_NAME\n",
    "                )\n",
    "                all_chunks.extend(chunks)\n",
    "                successful_books += 1\n",
    "                if len(all_chunks) >= Config.NUM_RETENTION_CHUNKS:\n",
    "                    break\n",
    "        maybe_delete_file(book_file)\n",
    "    \n",
    "    if not all_chunks:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to download any retention books. \"\n",
    "            f\"Expected {Config.NUM_RETENTION_BOOKS} books with {Config.NUM_RETENTION_CHUNKS} chunks total. \"\n",
    "            f\"Only {successful_books} books downloaded successfully.\"\n",
    "        )\n",
    "    \n",
    "    retention_chunks = all_chunks[:Config.NUM_RETENTION_CHUNKS]\n",
    "    print(f\"Created {len(retention_chunks)} retention chunks from {successful_books} books\")\n",
    "    \n",
    "    return SequentialUnlearningDataset(tokenizer, retention_chunks, mode=\"retain\")\n",
    "\n",
    "print(\"Data utilities loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1018cf78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.741138Z",
     "iopub.status.busy": "2025-11-25T07:53:07.740892Z",
     "iopub.status.idle": "2025-11-25T07:53:07.750636Z",
     "shell.execute_reply": "2025-11-25T07:53:07.749943Z"
    },
    "papermill": {
     "duration": 0.029217,
     "end_time": "2025-11-25T07:53:07.751647",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.722430",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSU model utilities loaded!\n"
     ]
    }
   ],
   "source": [
    "class SSUTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer implementing SSU loss with Weight Saliency.\n",
    "    \n",
    "    SSU loss: L = λ1 * L_fgt + λ2 * L_rnd (Equation 4 from paper)\n",
    "    No retention LM loss - D_nor is only used for evaluation and random labels.\n",
    "    \"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Compute SSU loss: λ1 * L_fgt + λ2 * L_rnd (Equation 4).\"\"\"\n",
    "        inputs_copy = inputs.copy()\n",
    "        _ = inputs_copy.pop('labels', None)  # Not used in SSU loss\n",
    "        labels_fgt = inputs_copy.pop('labels_fgt', None)\n",
    "        labels_rnd = inputs_copy.pop('labels_rnd', None)\n",
    "        \n",
    "        if labels_fgt is None or labels_rnd is None:\n",
    "            raise ValueError(\"SSU requires labels_fgt and labels_rnd for forget batches\")\n",
    "        \n",
    "        # Compute L_fgt and L_rnd\n",
    "        outputs_fgt = model(**inputs_copy, labels=labels_fgt)\n",
    "        outputs_rnd = model(**inputs_copy, labels=labels_rnd)\n",
    "        \n",
    "        # SSU loss: L = λ1 * L_fgt + λ2 * L_rnd\n",
    "        loss = Config.EPSILON_1 * outputs_fgt.loss + Config.EPSILON_2 * outputs_rnd.loss\n",
    "        outputs_to_return = outputs_fgt\n",
    "        \n",
    "        return (loss, outputs_to_return) if return_outputs else loss\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Override optimizer_step to apply dynamic weight saliency masking.\n",
    "        \n",
    "        Gamma (γ) is computed dynamically as: γ = μ + σ\n",
    "        where μ is the mean and σ is the standard deviation of gradient magnitudes.\n",
    "        \"\"\"\n",
    "        # Apply weight saliency mask before optimizer step\n",
    "        if self.accelerator.sync_gradients:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.grad is not None and param.requires_grad:\n",
    "                    if \"lora\" in name.lower():\n",
    "                        grad = param.grad.data\n",
    "                        grad_abs = grad.abs()\n",
    "                        \n",
    "                        # Compute dynamic gamma: γ = μ + σ\n",
    "                        flat = grad_abs.view(-1)\n",
    "                        mu = flat.mean()\n",
    "                        sigma = flat.std()\n",
    "                        gamma = mu + sigma  # 1 std above mean\n",
    "                        \n",
    "                        # Saliency Mask: m_s = I(|grad| >= gamma)\n",
    "                        m_s = (grad_abs >= gamma).float()\n",
    "                        \n",
    "                        # Apply mask to gradients (only update parameters with high saliency)\n",
    "                        param.grad.data = grad * m_s\n",
    "        \n",
    "        # Call parent optimizer_step to perform the actual update\n",
    "        super().optimizer_step()\n",
    "\n",
    "\n",
    "def create_lora_model(model):\n",
    "    \"\"\"Adds LoRA adapters to the base model.\"\"\"\n",
    "    peft_config = LoraConfig(\n",
    "        r=Config.LORA_R,\n",
    "        lora_alpha=Config.LORA_ALPHA,\n",
    "        lora_dropout=Config.LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=Config.TARGET_MODULES,\n",
    "    )\n",
    "    return get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "def apply_task_vector_negation(base_model, fine_tuned_model, name_prefix):\n",
    "    \"\"\"Task Vector Negation: theta_u^t = 2 * theta_u^{t-1} - theta_ft^t\"\"\"\n",
    "    print(f\"\\n--- Applying Task Vector Negation for {name_prefix} ---\")\n",
    "    \n",
    "    device = next(base_model.parameters()).device\n",
    "    new_unlearned_model = base_model.__class__(config=base_model.config).to(device)\n",
    "    \n",
    "    base_state = base_model.state_dict()\n",
    "    ft_state = fine_tuned_model.state_dict()\n",
    "    \n",
    "    new_state = {}\n",
    "    for name, param in new_unlearned_model.named_parameters():\n",
    "        if name in base_state and name in ft_state:\n",
    "            new_state[name] = 2 * base_state[name] - ft_state[name]\n",
    "        else:\n",
    "            new_state[name] = base_state.get(name, param.data)\n",
    "    \n",
    "    new_unlearned_model.load_state_dict(new_state, strict=False)\n",
    "    print(f\"Task Vector Negation complete for {name_prefix}.\")\n",
    "    return new_unlearned_model\n",
    "\n",
    "print(\"SSU model utilities loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a033f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.787698Z",
     "iopub.status.busy": "2025-11-25T07:53:07.787490Z",
     "iopub.status.idle": "2025-11-25T07:53:07.801704Z",
     "shell.execute_reply": "2025-11-25T07:53:07.801153Z"
    },
    "papermill": {
     "duration": 0.033761,
     "end_time": "2025-11-25T07:53:07.802734",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.768973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 9. Evaluation Utilities\n",
    "def _prepare_prompt_pairs(tokenizer, book_text: str, prompt_tokens: int = 100, continuation_tokens: int = 100, max_pairs: int = 16):\n",
    "    \"\"\"Prepare prompt-continuation pairs from book text.\n",
    "    \n",
    "    Args:\n",
    "        prompt_tokens: Number of tokens for prompt (default 100 per paper)\n",
    "        continuation_tokens: Number of tokens for continuation (default 100 per paper)\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        book_text,\n",
    "        return_tensors='pt',\n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "    )['input_ids'][0]\n",
    "    total_needed = prompt_tokens + continuation_tokens\n",
    "    pairs = []\n",
    "    for start in range(0, max(0, len(tokenized) - total_needed), total_needed):\n",
    "        prompt_ids = tokenized[start:start + prompt_tokens]\n",
    "        cont_ids = tokenized[start + prompt_tokens:start + total_needed]\n",
    "        if len(prompt_ids) < prompt_tokens or len(cont_ids) < continuation_tokens:\n",
    "            continue\n",
    "        prompt_text = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "        cont_text = tokenizer.decode(cont_ids, skip_special_tokens=True)\n",
    "        pairs.append((prompt_text, cont_text))\n",
    "        if len(pairs) >= max_pairs:\n",
    "            break\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def evaluate_regurgitation(model, tokenizer, book_text: str, max_pairs: int = 10, \n",
    "                          prompt_tokens: int = 100, continuation_tokens: int = 100,\n",
    "                          num_samples: int = 10, verbose: bool = True) -> Dict[str, float]:\n",
    "    \"\"\"Estimate regurgitation via ROUGE-L and ROUGE-1 between generated text and the book.\n",
    "    \n",
    "    For each prompt, generates N samples and takes the maximum ROUGE score per prompt.\n",
    "    Matches paper's evaluation protocol.\n",
    "    \"\"\"\n",
    "    model_device = next(model.parameters()).device\n",
    "    pairs = _prepare_prompt_pairs(tokenizer, book_text, prompt_tokens, continuation_tokens, max_pairs)\n",
    "    if not pairs:\n",
    "        return {\"rouge1\": 0.0, \"rougeL\": 0.0, \"num_pairs\": 0}\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rougeL\"], use_stemmer=False)\n",
    "    max_rouge1_scores = []\n",
    "    max_rougeL_scores = []\n",
    "    \n",
    "    model.eval()\n",
    "    total_generations = len(pairs) * num_samples\n",
    "    generation_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for pair_idx, (prompt, reference) in enumerate(pairs):\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to(model_device)\n",
    "            \n",
    "            # Generate N samples per prompt and take max ROUGE\n",
    "            prompt_rouge1_scores = []\n",
    "            prompt_rougeL_scores = []\n",
    "            \n",
    "            for sample_idx in range(num_samples):\n",
    "                if verbose and generation_count % 10 == 0:\n",
    "                    print(f\"  Generating sample {generation_count + 1}/{total_generations} (pair {pair_idx + 1}/{len(pairs)}, sample {sample_idx + 1}/{num_samples})\", end='\\r')\n",
    "                \n",
    "                generated = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=continuation_tokens,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.6,\n",
    "                    temperature=0.6,\n",
    "                    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "                )\n",
    "                gen_continuation = generated[0][inputs['input_ids'].shape[1]:]\n",
    "                hypothesis = tokenizer.decode(gen_continuation, skip_special_tokens=True)\n",
    "                \n",
    "                scores = scorer.score(reference, hypothesis)\n",
    "                prompt_rouge1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "                prompt_rougeL_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "                \n",
    "                generation_count += 1\n",
    "            \n",
    "            # Take maximum per prompt\n",
    "            max_rouge1_scores.append(max(prompt_rouge1_scores))\n",
    "            max_rougeL_scores.append(max(prompt_rougeL_scores))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"  Completed pair {pair_idx + 1}/{len(pairs)} - Max ROUGE-L: {max_rougeL_scores[-1]:.4f}                    \")\n",
    "    \n",
    "    avg_rouge1 = float(sum(max_rouge1_scores) / len(max_rouge1_scores)) if max_rouge1_scores else 0.0\n",
    "    avg_rougeL = float(sum(max_rougeL_scores) / len(max_rougeL_scores)) if max_rougeL_scores else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"rouge1\": avg_rouge1,\n",
    "        \"rougeL\": avg_rougeL,\n",
    "        \"num_pairs\": len(pairs)\n",
    "    }\n",
    "\n",
    "\n",
    "def _normalize_corpus(text_source: Sequence[str] | str, tokenizer, max_samples: int = 32) -> str:\n",
    "    if isinstance(text_source, str):\n",
    "        return text_source\n",
    "    if isinstance(text_source, SequentialUnlearningDataset):\n",
    "        samples = [tokenizer.decode(text_source.input_ids[i], skip_special_tokens=True) for i in range(min(len(text_source), max_samples))]\n",
    "        return \"\\n\\n\".join(samples)\n",
    "    if isinstance(text_source, list):\n",
    "        return \"\\n\\n\".join(text_source[:max_samples])\n",
    "    raise ValueError(\"Unsupported text source type for perplexity evaluation\")\n",
    "\n",
    "\n",
    "def evaluate_perplexity(model, tokenizer, text_source, stride: int = 256) -> float:\n",
    "    \"\"\"Compute perplexity on the provided text corpus.\"\"\"\n",
    "    corpus = _normalize_corpus(text_source, tokenizer)\n",
    "    tokenized = tokenizer(\n",
    "        corpus,\n",
    "        return_tensors='pt',\n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "    )['input_ids'][0]\n",
    "    if tokenized.size(0) <= 1:\n",
    "        return float('inf')\n",
    "    model_device = next(model.parameters()).device\n",
    "    nll_sum = 0.0\n",
    "    token_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for start in range(0, tokenized.size(0) - 1, stride):\n",
    "            end = min(start + stride, tokenized.size(0))\n",
    "            input_ids = tokenized[start:end].unsqueeze(0).to(model_device)\n",
    "            labels = input_ids.clone()\n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            n_tokens = labels.size(1) - 1\n",
    "            if n_tokens <= 0:\n",
    "                continue\n",
    "            nll_sum += outputs.loss.item() * n_tokens\n",
    "            token_count += n_tokens\n",
    "    if token_count == 0:\n",
    "        return float('inf')\n",
    "    return float(math.exp(nll_sum / token_count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42fc802",
   "metadata": {
    "papermill": {
     "duration": 0.017487,
     "end_time": "2025-11-25T07:53:07.837451",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.819964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Model Loading with Retry Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487a8517",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:07.873031Z",
     "iopub.status.busy": "2025-11-25T07:53:07.872817Z",
     "iopub.status.idle": "2025-11-25T07:53:26.089800Z",
     "shell.execute_reply": "2025-11-25T07:53:26.089054Z"
    },
    "papermill": {
     "duration": 18.236121,
     "end_time": "2025-11-25T07:53:26.090943",
     "exception": false,
     "start_time": "2025-11-25T07:53:07.854822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: google/gemma-3-1b-it\n",
      "Loading model (attempt 1/3)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded google/gemma-3-1b-it!\n",
      "Model loaded and frozen!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "\n",
    "def load_model_with_retry(model_name, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load model with automatic retry on network errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading model (attempt {attempt + 1}/{max_retries})...\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Determine device\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "                dtype = torch.bfloat16\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "                dtype = torch.float32\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                torch_dtype=dtype,\n",
    "                device_map=device,  # Use single device instead of \"auto\"\n",
    "                trust_remote_code=True,\n",
    "                attn_implementation=\"eager\",\n",
    "            )\n",
    "            \n",
    "            # Ensure model is on the correct device\n",
    "            base_model = base_model.to(device)\n",
    "            \n",
    "            print(f\"Successfully loaded {model_name}!\")\n",
    "            return base_model, tokenizer\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"Attempt {attempt + 1} failed: {error_msg[:200]}...\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                if \"IncompleteRead\" in error_msg or \"Connection\" in error_msg or \"timeout\" in error_msg.lower():\n",
    "                    print(f\"Network error detected. Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(\"Non-network error. Retrying...\")\n",
    "                    time.sleep(2)\n",
    "            else:\n",
    "                print(\"\\nAll retry attempts failed!\")\n",
    "                print(\"\\nTROUBLESHOOTING:\")\n",
    "                print(\"1. Check your internet connection\")\n",
    "                print(\"2. Try a smaller model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "                print(\"3. For gated models, ensure HF_TOKEN is set\")\n",
    "                raise\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "\n",
    "# Load the model\n",
    "print(f\"Loading base model: {Config.MODEL_NAME}\")\n",
    "base_model, tokenizer = load_model_with_retry(Config.MODEL_NAME)\n",
    "base_model.requires_grad_(False)\n",
    "print(\"Model loaded and frozen!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8347102a",
   "metadata": {
    "papermill": {
     "duration": 0.018378,
     "end_time": "2025-11-25T07:53:26.128399",
     "exception": false,
     "start_time": "2025-11-25T07:53:26.110021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Main SSU Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b70fa5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:53:26.165986Z",
     "iopub.status.busy": "2025-11-25T07:53:26.165775Z",
     "iopub.status.idle": "2025-11-25T07:57:53.503885Z",
     "shell.execute_reply": "2025-11-25T07:57:53.502958Z"
    },
    "papermill": {
     "duration": 267.358603,
     "end_time": "2025-11-25T07:57:53.505032",
     "exception": false,
     "start_time": "2025-11-25T07:53:26.146429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\n",
      "============================================================\n",
      "\n",
      "=== Downloading all books for initial fine-tuning (D_f) ===\n",
      "Downloading book 1661 from Project Gutenberg...\n",
      "Downloaded book 1661 successfully.\n",
      "Loaded book 1661 (559224 chars)\n",
      "Downloading book 84 from Project Gutenberg...\n",
      "Downloaded book 84 successfully.\n",
      "Loaded book 84 (418285 chars)\n",
      "Downloading book 1342 from Project Gutenberg...\n",
      "Downloaded book 1342 successfully.\n",
      "Loaded book 1342 (718465 chars)\n",
      "Downloading book 11 from Project Gutenberg...\n",
      "Downloaded book 11 successfully.\n",
      "Loaded book 11 (143127 chars)\n",
      "Downloading book 2701 from Project Gutenberg...\n",
      "Downloaded book 2701 successfully.\n",
      "Loaded book 2701 (1214953 chars)\n",
      "Downloading book 74 from Project Gutenberg...\n",
      "Downloaded book 74 successfully.\n",
      "Loaded book 74 (390259 chars)\n",
      "Downloading book 98 from Project Gutenberg...\n",
      "Downloaded book 98 successfully.\n",
      "Loaded book 98 (753543 chars)\n",
      "Downloading book 5200 from Project Gutenberg...\n",
      "Downloaded book 5200 successfully.\n",
      "Loaded book 5200 (118363 chars)\n",
      "Downloading book 6130 from Project Gutenberg...\n",
      "Downloaded book 6130 successfully.\n",
      "Loaded book 6130 (1093073 chars)\n",
      "Downloading book 174 from Project Gutenberg...\n",
      "Downloaded book 174 successfully.\n",
      "Loaded book 174 (427643 chars)\n",
      "\n",
      "============================================================\n",
      "STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\n",
      "============================================================\n",
      "Fine-tuning vanilla model on all copyrighted books to memorize them...\n",
      "Created 510 chunks from all books\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting initial fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 57:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.007200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial fine-tuning complete. Model has memorized all books.\n",
      "\n",
      "Skipping on-disk save of memorized model (SAVE_MEMORIZED_MODEL=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "def initial_finetuning(model, tokenizer, all_books_texts):\n",
    "    \"\"\"\n",
    "    Step 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n",
    "    This is what the paper does BEFORE unlearning.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Fine-tuning vanilla model on all copyrighted books to memorize them...\")\n",
    "    \n",
    "    # Generate chunks from all books\n",
    "    all_chunks = []\n",
    "    for book_text in all_books_texts:\n",
    "        chunks = generate_simulated_data(\n",
    "            book_text,\n",
    "            Config.CHUNK_SIZE,\n",
    "            Config.NUM_CHUNKS_PER_STEP * Config.NUM_UNLEARNING_STEPS // len(all_books_texts) + 1,\n",
    "            Config.TOKENIZER_NAME\n",
    "        )\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"Created {len(all_chunks)} chunks from all books\")\n",
    "    \n",
    "    # For initial fine-tuning, use standard dataset (not SSU dual labels)\n",
    "    from torch.utils.data import Dataset as TorchDataset\n",
    "    class StandardDataset(TorchDataset):\n",
    "        def __init__(self, tokenizer, data_texts):\n",
    "            tokenized = tokenizer(\n",
    "                data_texts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=Config.CHUNK_SIZE,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.input_ids = tokenized['input_ids']\n",
    "            self.attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                'input_ids': self.input_ids[idx].clone(),\n",
    "                'attention_mask': self.attention_mask[idx].clone(),\n",
    "                'labels': self.input_ids[idx].clone()  # Standard labels for next token prediction\n",
    "            }\n",
    "    \n",
    "    initial_dataset = StandardDataset(tokenizer, all_chunks)\n",
    "    \n",
    "    # Ensure model is on correct device before creating LoRA\n",
    "    device = next(model.parameters()).device\n",
    "    if device.type == \"meta\" or str(device) == \"meta\":\n",
    "        # If model is on meta device, move to actual device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        # Convert device object to string if needed\n",
    "        device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n",
    "    \n",
    "    # Create LoRA model for initial fine-tuning (PEFT handles device automatically)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "    \n",
    "    lora_model = create_lora_model(model)\n",
    "    lora_model.print_trainable_parameters()\n",
    "    \n",
    "    # Disable cache for gradient checkpointing\n",
    "    if hasattr(lora_model.config, \"use_cache\"):\n",
    "        lora_model.config.use_cache = False\n",
    "    \n",
    "    # PEFT models inherit device from base model, no need to call .to()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{Config.OUTPUT_DIR}/initial_ft_checkpoints\",\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=Config.LEARNING_RATE,\n",
    "        num_train_epochs=Config.NUM_EPOCHS_INITIAL_FT,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        fp16=False,\n",
    "        bf16=torch.cuda.is_available() and device == \"cuda\",\n",
    "        dataloader_pin_memory=False,  # Fix device issues\n",
    "        label_names=[\"labels\"],\n",
    "        gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    # Use standard Trainer for initial fine-tuning (not SSU)\n",
    "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Causal LM, not masked LM\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=initial_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting initial fine-tuning...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Merge LoRA weights into base model\n",
    "    memorized_model = lora_model.merge_and_unload()\n",
    "    memorized_model.requires_grad_(False)\n",
    "    \n",
    "    print(\"Initial fine-tuning complete. Model has memorized all books.\")\n",
    "    return memorized_model\n",
    "\n",
    "\n",
    "def run_initial_finetuning():\n",
    "    \"\"\"STEP 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n",
    "    \n",
    "    Run this cell once to create the memorized model. After this completes,\n",
    "    you can run the sequential unlearning steps without re-running this.\n",
    "    \"\"\"\n",
    "    # Ensure all required dependencies are available\n",
    "    missing = []\n",
    "    try:\n",
    "        _ = Config.OUTPUT_DIR\n",
    "    except NameError:\n",
    "        missing.append(\"Config (cell 4)\")\n",
    "    \n",
    "    try:\n",
    "        _ = base_model\n",
    "    except NameError:\n",
    "        missing.append(\"base_model (cell 11)\")\n",
    "    \n",
    "    try:\n",
    "        _ = tokenizer\n",
    "    except NameError:\n",
    "        missing.append(\"tokenizer (cell 11)\")\n",
    "    \n",
    "    if missing:\n",
    "        raise NameError(\n",
    "            f\"The following are not defined: {', '.join(missing)}. \"\n",
    "            f\"Please run the required cells first to set up the dependencies.\"\n",
    "        )\n",
    "    \n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Check if memorized model already exists\n",
    "    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n",
    "    if os.path.exists(memorized_model_path):\n",
    "        print(f\"Memorized model already exists at {memorized_model_path}\")\n",
    "        print(\"Loading existing memorized model...\")\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        memorized_model = AutoModelForCausalLM.from_pretrained(\n",
    "            memorized_model_path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device\n",
    "        )\n",
    "        memorized_model.requires_grad_(False)\n",
    "        print(\"Loaded existing memorized model.\")\n",
    "        return memorized_model\n",
    "    \n",
    "    # STEP 0: Initial fine-tuning on all books (D_f)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n",
    "    print(\"=\"*60)\n",
    "    all_books_texts = get_all_books_for_initial_finetuning()\n",
    "    memorized_model = initial_finetuning(base_model, tokenizer, all_books_texts)\n",
    "    \n",
    "    # Save the memorized model (optional)\n",
    "    if Config.SAVE_MEMORIZED_MODEL:\n",
    "        memorized_model.save_pretrained(memorized_model_path)\n",
    "        tokenizer.save_pretrained(memorized_model_path)\n",
    "        print(f\"\\nMemorized model saved to {memorized_model_path}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping on-disk save of memorized model (SAVE_MEMORIZED_MODEL=False)\")\n",
    "    \n",
    "    return memorized_model\n",
    "\n",
    "\n",
    "# Run initial fine-tuning (only need to run this once)\n",
    "memorized_model = run_initial_finetuning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f42950e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T07:57:53.544778Z",
     "iopub.status.busy": "2025-11-25T07:57:53.544567Z",
     "iopub.status.idle": "2025-11-25T19:27:45.245035Z",
     "shell.execute_reply": "2025-11-25T19:27:45.244316Z"
    },
    "papermill": {
     "duration": 41391.722088,
     "end_time": "2025-11-25T19:27:45.246567",
     "exception": false,
     "start_time": "2025-11-25T07:57:53.524479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from memorized model (in-memory) for step 1\n",
      "\n",
      "=== Preparing retention dataset D_nor ===\n",
      "Downloading book 1232 from Project Gutenberg...\n",
      "Downloaded book 1232 successfully.\n",
      "Downloading book 145 from Project Gutenberg...\n",
      "Downloaded book 145 successfully.\n",
      "Downloading book 76 from Project Gutenberg...\n",
      "Downloaded book 76 successfully.\n",
      "Downloading book 2591 from Project Gutenberg...\n",
      "Downloaded book 2591 successfully.\n",
      "Downloading book 30254 from Project Gutenberg...\n",
      "Downloaded book 30254 successfully.\n",
      "Downloading book 844 from Project Gutenberg...\n",
      "Downloaded book 844 successfully.\n",
      "Downloading book 345 from Project Gutenberg...\n",
      "Downloaded book 345 successfully.\n",
      "Downloading book 520 from Project Gutenberg...\n",
      "Downloaded book 520 successfully.\n",
      "Downloading book 100 from Project Gutenberg...\n",
      "Downloaded book 100 successfully.\n",
      "Downloading book 200 from Project Gutenberg...\n",
      "Downloaded book 200 successfully.\n",
      "Downloading book 300 from Project Gutenberg...\n",
      "Downloaded book 300 successfully.\n",
      "Downloading book 400 from Project Gutenberg...\n",
      "Downloaded book 400 successfully.\n",
      "Downloading book 500 from Project Gutenberg...\n",
      "Downloaded book 500 successfully.\n",
      "Downloading book 600 from Project Gutenberg...\n",
      "Downloaded book 600 successfully.\n",
      "Downloading book 700 from Project Gutenberg...\n",
      "Downloaded book 700 successfully.\n",
      "Downloading book 800 from Project Gutenberg...\n",
      "Downloaded book 800 successfully.\n",
      "Downloading book 900 from Project Gutenberg...\n",
      "Error: Could not download book 900 from Project Gutenberg.\n",
      "Downloading book 1000 from Project Gutenberg...\n",
      "Downloaded book 1000 successfully.\n",
      "Downloading book 1100 from Project Gutenberg...\n",
      "Downloaded book 1100 successfully.\n",
      "Downloading book 1200 from Project Gutenberg...\n",
      "Downloaded book 1200 successfully.\n",
      "Downloading book 1300 from Project Gutenberg...\n",
      "Downloaded book 1300 successfully.\n",
      "Downloading book 1400 from Project Gutenberg...\n",
      "Downloaded book 1400 successfully.\n",
      "Downloading book 1500 from Project Gutenberg...\n",
      "Downloaded book 1500 successfully.\n",
      "Downloading book 1600 from Project Gutenberg...\n",
      "Downloaded book 1600 successfully.\n",
      "Downloading book 1700 from Project Gutenberg...\n",
      "Downloaded book 1700 successfully.\n",
      "Downloading book 1800 from Project Gutenberg...\n",
      "Downloaded book 1800 successfully.\n",
      "Downloading book 1900 from Project Gutenberg...\n",
      "Downloaded book 1900 successfully.\n",
      "Downloading book 2000 from Project Gutenberg...\n",
      "Downloaded book 2000 successfully.\n",
      "Downloading book 2100 from Project Gutenberg...\n",
      "Downloaded book 2100 successfully.\n",
      "Downloading book 2200 from Project Gutenberg...\n",
      "Error: Could not download book 2200 from Project Gutenberg.\n",
      "Downloading book 2300 from Project Gutenberg...\n",
      "Downloaded book 2300 successfully.\n",
      "Downloading book 2400 from Project Gutenberg...\n",
      "Downloaded book 2400 successfully.\n",
      "Downloading book 2500 from Project Gutenberg...\n",
      "Downloaded book 2500 successfully.\n",
      "Downloading book 2600 from Project Gutenberg...\n",
      "Downloaded book 2600 successfully.\n",
      "Downloading book 2800 from Project Gutenberg...\n",
      "Downloaded book 2800 successfully.\n",
      "Downloading book 2900 from Project Gutenberg...\n",
      "Downloaded book 2900 successfully.\n",
      "Downloading book 3000 from Project Gutenberg...\n",
      "Downloaded book 3000 successfully.\n",
      "Downloading book 3100 from Project Gutenberg...\n",
      "Downloaded book 3100 successfully.\n",
      "Downloading book 3200 from Project Gutenberg...\n",
      "Downloaded book 3200 successfully.\n",
      "Downloading book 3300 from Project Gutenberg...\n",
      "Downloaded book 3300 successfully.\n",
      "Downloading book 3400 from Project Gutenberg...\n",
      "Downloaded book 3400 successfully.\n",
      "Downloading book 3500 from Project Gutenberg...\n",
      "Downloaded book 3500 successfully.\n",
      "Downloading book 3600 from Project Gutenberg...\n",
      "Downloaded book 3600 successfully.\n",
      "Downloading book 3700 from Project Gutenberg...\n",
      "Downloaded book 3700 successfully.\n",
      "Downloading book 3800 from Project Gutenberg...\n",
      "Downloaded book 3800 successfully.\n",
      "Downloading book 3900 from Project Gutenberg...\n",
      "Downloaded book 3900 successfully.\n",
      "Downloading book 4000 from Project Gutenberg...\n",
      "Downloaded book 4000 successfully.\n",
      "Downloading book 4100 from Project Gutenberg...\n",
      "Downloaded book 4100 successfully.\n",
      "Downloading book 4200 from Project Gutenberg...\n",
      "Downloaded book 4200 successfully.\n",
      "Downloading book 4300 from Project Gutenberg...\n",
      "Downloaded book 4300 successfully.\n",
      "Downloading book 4400 from Project Gutenberg...\n",
      "Downloaded book 4400 successfully.\n",
      "Downloading book 4500 from Project Gutenberg...\n",
      "Downloaded book 4500 successfully.\n",
      "Downloading book 4600 from Project Gutenberg...\n",
      "Downloaded book 4600 successfully.\n",
      "Downloading book 4700 from Project Gutenberg...\n",
      "Downloaded book 4700 successfully.\n",
      "Downloading book 4800 from Project Gutenberg...\n",
      "Downloaded book 4800 successfully.\n",
      "Downloading book 4900 from Project Gutenberg...\n",
      "Downloaded book 4900 successfully.\n",
      "Downloading book 5000 from Project Gutenberg...\n",
      "Downloaded book 5000 successfully.\n",
      "Downloading book 5100 from Project Gutenberg...\n",
      "Downloaded book 5100 successfully.\n",
      "Downloading book 5300 from Project Gutenberg...\n",
      "Downloaded book 5300 successfully.\n",
      "Downloading book 5400 from Project Gutenberg...\n",
      "Downloaded book 5400 successfully.\n",
      "Downloading book 5500 from Project Gutenberg...\n",
      "Downloaded book 5500 successfully.\n",
      "Downloading book 5600 from Project Gutenberg...\n",
      "Downloaded book 5600 successfully.\n",
      "Downloading book 5700 from Project Gutenberg...\n",
      "Downloaded book 5700 successfully.\n",
      "Downloading book 5800 from Project Gutenberg...\n",
      "Downloaded book 5800 successfully.\n",
      "Downloading book 5900 from Project Gutenberg...\n",
      "Downloaded book 5900 successfully.\n",
      "Downloading book 6000 from Project Gutenberg...\n",
      "Downloaded book 6000 successfully.\n",
      "Downloading book 6100 from Project Gutenberg...\n",
      "Downloaded book 6100 successfully.\n",
      "Downloading book 6200 from Project Gutenberg...\n",
      "Downloaded book 6200 successfully.\n",
      "Downloading book 6300 from Project Gutenberg...\n",
      "Downloaded book 6300 successfully.\n",
      "Created 200 retention chunks from 67 books\n",
      "\n",
      "--- Preparing dataset D_f^1 for time step 1 ---\n",
      "Downloading book 1661 from Project Gutenberg...\n",
      "Downloaded book 1661 successfully.\n",
      "Loaded book 1661 for step 1\n",
      "Created 50 chunks for time step 1\n",
      "\n",
      "--- Preparing dataset D_f^2 for time step 2 ---\n",
      "Downloading book 84 from Project Gutenberg...\n",
      "Downloaded book 84 successfully.\n",
      "Loaded book 84 for step 2\n",
      "Created 50 chunks for time step 2\n",
      "\n",
      "--- Preparing dataset D_f^3 for time step 3 ---\n",
      "Downloading book 1342 from Project Gutenberg...\n",
      "Downloaded book 1342 successfully.\n",
      "Loaded book 1342 for step 3\n",
      "Created 50 chunks for time step 3\n",
      "\n",
      "--- Preparing dataset D_f^4 for time step 4 ---\n",
      "Downloading book 11 from Project Gutenberg...\n",
      "Downloaded book 11 successfully.\n",
      "Loaded book 11 for step 4\n",
      "Created 50 chunks for time step 4\n",
      "\n",
      "--- Preparing dataset D_f^5 for time step 5 ---\n",
      "Downloading book 2701 from Project Gutenberg...\n",
      "Downloaded book 2701 successfully.\n",
      "Loaded book 2701 for step 5\n",
      "Created 50 chunks for time step 5\n",
      "\n",
      "--- Preparing dataset D_f^6 for time step 6 ---\n",
      "Downloading book 74 from Project Gutenberg...\n",
      "Downloaded book 74 successfully.\n",
      "Loaded book 74 for step 6\n",
      "Created 50 chunks for time step 6\n",
      "\n",
      "--- Preparing dataset D_f^7 for time step 7 ---\n",
      "Downloading book 98 from Project Gutenberg...\n",
      "Downloaded book 98 successfully.\n",
      "Loaded book 98 for step 7\n",
      "Created 50 chunks for time step 7\n",
      "\n",
      "--- Preparing dataset D_f^8 for time step 8 ---\n",
      "Downloading book 5200 from Project Gutenberg...\n",
      "Downloaded book 5200 successfully.\n",
      "Loaded book 5200 for step 8\n",
      "Created 50 chunks for time step 8\n",
      "\n",
      "--- Preparing dataset D_f^9 for time step 9 ---\n",
      "Downloading book 6130 from Project Gutenberg...\n",
      "Downloaded book 6130 successfully.\n",
      "Loaded book 6130 for step 9\n",
      "Created 50 chunks for time step 9\n",
      "\n",
      "--- Preparing dataset D_f^10 for time step 10 ---\n",
      "Downloading book 174 from Project Gutenberg...\n",
      "Downloaded book 174 successfully.\n",
      "Loaded book 174 for step 10\n",
      "Created 50 chunks for time step 10\n",
      "\n",
      "Generated 10 sequential unlearning datasets.\n",
      "\n",
      "=== Baseline Evaluation ===\n",
      "Note: Evaluation generates 10 samples per prompt, this may take a while...\n",
      "\n",
      "Evaluating book 1661...\n",
      "Downloading book 1661 from Project Gutenberg...\n",
      "Downloaded book 1661 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2055                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1965                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2086                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1905                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1467                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1268                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2400                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1757                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1325                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2797                    \n",
      "Book 1661 baseline ROUGE-L: 0.1902, ROUGE-1: 0.2800\n",
      "\n",
      "Evaluating book 84...\n",
      "Downloading book 84 from Project Gutenberg...\n",
      "Downloaded book 84 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1667                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1548                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2159                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1446                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1871                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2022                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1549                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1943                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1943                    \n",
      "Book 84 baseline ROUGE-L: 0.1766, ROUGE-1: 0.2498\n",
      "\n",
      "Evaluating book 1342...\n",
      "Downloading book 1342 from Project Gutenberg...\n",
      "Downloaded book 1342 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1295                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1974                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1935                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1474                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2368                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2152                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1677                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1958                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1566                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1830                    \n",
      "Book 1342 baseline ROUGE-L: 0.1823, ROUGE-1: 0.2853\n",
      "\n",
      "Evaluating book 11...\n",
      "Downloading book 11 from Project Gutenberg...\n",
      "Downloaded book 11 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2577                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2484                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1905                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1176                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1757                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1752                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1939                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2130                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1512                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1410                    \n",
      "Book 11 baseline ROUGE-L: 0.1864, ROUGE-1: 0.2848\n",
      "\n",
      "Evaluating book 2701...\n",
      "Downloading book 2701 from Project Gutenberg...\n",
      "Downloaded book 2701 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.6042                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.5567                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.3656                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.4078                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.4854                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.4118                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.6122                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1867                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1333                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1522                    \n",
      "Book 2701 baseline ROUGE-L: 0.3916, ROUGE-1: 0.4319\n",
      "\n",
      "Evaluating book 74...\n",
      "Downloading book 74 from Project Gutenberg...\n",
      "Downloaded book 74 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.0896                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2121                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1940                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1517                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2275                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1796                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2386                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1477                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1975                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1677                    \n",
      "Book 74 baseline ROUGE-L: 0.1806, ROUGE-1: 0.2391\n",
      "\n",
      "Evaluating book 98...\n",
      "Downloading book 98 from Project Gutenberg...\n",
      "Downloaded book 98 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.4217                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2683                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1850                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1677                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1739                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2195                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1863                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2286                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1783                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2471                    \n",
      "Book 98 baseline ROUGE-L: 0.2276, ROUGE-1: 0.3116\n",
      "\n",
      "Evaluating book 5200...\n",
      "Downloading book 5200 from Project Gutenberg...\n",
      "Downloaded book 5200 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1905                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1401                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1893                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1628                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1282                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1768                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1606                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1429                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1500                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1829                    \n",
      "Book 5200 baseline ROUGE-L: 0.1624, ROUGE-1: 0.2633\n",
      "\n",
      "Evaluating book 6130...\n",
      "Downloading book 6130 from Project Gutenberg...\n",
      "Downloaded book 6130 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.4640                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1818                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2400                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1745                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2281                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1734                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2000                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2105                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1807                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2194                    \n",
      "Book 6130 baseline ROUGE-L: 0.2272, ROUGE-1: 0.3149\n",
      "\n",
      "Evaluating book 174...\n",
      "Downloading book 174 from Project Gutenberg...\n",
      "Downloaded book 174 successfully.\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2099                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.3195                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2381                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1031                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1525                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1529                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1392                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1404                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1282                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1605                    \n",
      "Book 174 baseline ROUGE-L: 0.1744, ROUGE-1: 0.2418\n",
      "Retention baseline perplexity: 103.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohammad\\Documents\\GitHub\\DL\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General baseline perplexity: 16.92\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 1 (Unlearning D_f^1)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^1...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_1...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 08:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_1 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_1.\n",
      "Skipped saving step_1 checkpoint (SAVE_STEP_MODELS=False)\n",
      "\n",
      "Evaluating current book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1375                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1557                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2312                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2000                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1615                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1579                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1705                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1622                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1375                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2029                    \n",
      "Step 1 current book 1661 ROUGE-L: 0.1717, ROUGE-1: 0.2690\n",
      "Step 1 retention perplexity: 103.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 general perplexity: 16.91\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 2 (Unlearning D_f^2)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^2...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_2...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 08:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_2 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_2.\n",
      "Skipped saving step_2 checkpoint (SAVE_STEP_MODELS=False)\n",
      "\n",
      "Evaluating current book 84...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1739                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1775                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2414                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1850                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1557                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1921                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1560                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2041                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2045                    \n",
      "Step 2 current book 84 ROUGE-L: 0.1841, ROUGE-1: 0.2605\n",
      "\n",
      "Evaluating prev book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2000                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1988                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1871                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2130                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1519                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1274                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1593                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1529                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1317                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2319                    \n",
      "Step 2 prev book 1661 ROUGE-L: 0.1754, ROUGE-1: 0.2918\n",
      "Step 2 retention perplexity: 103.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 general perplexity: 16.91\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 3 (Unlearning D_f^3)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^3...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_3...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 08:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_3 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_3.\n",
      "Skipped saving step_3 checkpoint (SAVE_STEP_MODELS=False)\n",
      "\n",
      "Evaluating current book 1342...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1579                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1899                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2073                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1517                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2353                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1857                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1687                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1944                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1714                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1905                    \n",
      "Step 3 current book 1342 ROUGE-L: 0.1853, ROUGE-1: 0.2719\n",
      "\n",
      "Evaluating prev book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1923                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1988                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2209                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2024                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1750                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1548                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1681                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1852                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1446                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2177                    \n",
      "Step 3 prev book 1661 ROUGE-L: 0.1860, ROUGE-1: 0.2855\n",
      "\n",
      "Evaluating prev book 84...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1646                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1765                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2260                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1839                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1807                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1818                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1538                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2182                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1818                    \n",
      "Step 3 prev book 84 ROUGE-L: 0.1819, ROUGE-1: 0.2624\n",
      "Step 3 retention perplexity: 103.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 general perplexity: 16.90\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 4 (Unlearning D_f^4)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^4...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_4...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 08:17, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_4 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_4.\n",
      "Skipped saving step_4 checkpoint (SAVE_STEP_MODELS=False)\n",
      "\n",
      "Evaluating current book 11...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2517                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2345                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1954                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1266                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1646                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1589                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2156                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2061                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1387                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1447                    \n",
      "Step 4 current book 11 ROUGE-L: 0.1837, ROUGE-1: 0.2822\n",
      "\n",
      "Evaluating prev book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1728                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1707                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2345                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1852                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1728                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1316                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2394                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1750                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1366                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2400                    \n",
      "Step 4 prev book 1661 ROUGE-L: 0.1859, ROUGE-1: 0.2768\n",
      "\n",
      "Evaluating prev book 84...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1657                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1734                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2069                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1647                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1548                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2011                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1517                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2057                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1829                    \n",
      "Step 4 prev book 84 ROUGE-L: 0.1758, ROUGE-1: 0.2440\n",
      "\n",
      "Evaluating prev book 1342...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1622                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1750                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2099                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1699                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2545                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2041                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1628                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2112                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1472                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1892                    \n",
      "Step 4 prev book 1342 ROUGE-L: 0.1886, ROUGE-1: 0.2786\n",
      "Step 4 retention perplexity: 103.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 general perplexity: 16.89\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 5 (Unlearning D_f^5)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^5...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_5...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 08:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_5 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_5.\n",
      "Skipped saving step_5 checkpoint (SAVE_STEP_MODELS=False)\n",
      "\n",
      "Evaluating current book 2701...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.6105                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.5833                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.3673                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.4158                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.4727                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2222                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.5400                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1757                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1290                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1600                    \n",
      "Step 5 current book 2701 ROUGE-L: 0.3677, ROUGE-1: 0.4142\n",
      "\n",
      "Evaluating prev book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2013                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1744                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2209                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2048                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1548                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1538                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1681                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1711                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1350                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2190                    \n",
      "Step 5 prev book 1661 ROUGE-L: 0.1803, ROUGE-1: 0.2734\n",
      "\n",
      "Evaluating prev book 84...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1786                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1637                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2346                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1647                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1754                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2045                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1538                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2090                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1921                    \n",
      "Step 5 prev book 84 ROUGE-L: 0.1828, ROUGE-1: 0.2697\n",
      "\n",
      "Evaluating prev book 1342...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1497                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1656                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1951                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1569                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2439                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2312                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1744                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1875                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1503                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1846                    \n",
      "Step 5 prev book 1342 ROUGE-L: 0.1839, ROUGE-1: 0.2772\n",
      "\n",
      "Evaluating prev book 11...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2895                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2208                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2130                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1438                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1772                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1500                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2125                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2118                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1420                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1595                    \n",
      "Step 5 prev book 11 ROUGE-L: 0.1920, ROUGE-1: 0.3192\n",
      "Step 5 retention perplexity: 103.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5 general perplexity: 16.88\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 6 (Unlearning D_f^6)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^6...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_6...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 08:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_6 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_6.\n",
      "Skipped saving step_6 checkpoint (SAVE_STEP_MODELS=False)\n",
      "\n",
      "Evaluating current book 74...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1353                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2273                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2462                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1528                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2275                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2092                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2312                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1585                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1667                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1699                    \n",
      "Step 6 current book 74 ROUGE-L: 0.1925, ROUGE-1: 0.2678\n",
      "\n",
      "Evaluating prev book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1818                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2197                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2386                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2235                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1646                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1410                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2059                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1750                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1384                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2361                    \n",
      "Step 6 prev book 1661 ROUGE-L: 0.1925, ROUGE-1: 0.2838\n",
      "\n",
      "Evaluating prev book 84...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1795                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1618                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1882                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1512                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1647                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1932                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1655                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1988                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2069                    \n",
      "Step 6 prev book 84 ROUGE-L: 0.1761, ROUGE-1: 0.2601\n",
      "\n",
      "Evaluating prev book 1342...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1517                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1835                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2166                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1486                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2368                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2138                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1734                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1750                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1463                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2083                    \n",
      "Step 6 prev book 1342 ROUGE-L: 0.1854, ROUGE-1: 0.2825\n",
      "\n",
      "Evaluating prev book 11...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2949                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2331                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1905                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1290                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1622                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1538                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2061                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2143                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1472                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1605                    \n",
      "Step 6 prev book 11 ROUGE-L: 0.1892, ROUGE-1: 0.2987\n",
      "\n",
      "Evaluating prev book 2701...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.5833                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.5773                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.3621                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.4000                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.4717                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.3529                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.5455                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1733                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1356                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1447                    \n",
      "Step 6 prev book 2701 ROUGE-L: 0.3746, ROUGE-1: 0.4323\n",
      "Step 6 retention perplexity: 103.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6 general perplexity: 16.88\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 7 (Unlearning D_f^7)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^7...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_7...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 08:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_7 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_7.\n",
      "Skipped saving step_7 checkpoint (SAVE_STEP_MODELS=False)\n",
      "\n",
      "Evaluating current book 98...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.4048                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.4734                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1932                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1667                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1733                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2346                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1852                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2073                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1739                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2410                    \n",
      "Step 7 current book 98 ROUGE-L: 0.2453, ROUGE-1: 0.3192\n",
      "\n",
      "Evaluating prev book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1646                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1882                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2138                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2367                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1633                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1507                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2400                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2162                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1455                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2297                    \n",
      "Step 7 prev book 1661 ROUGE-L: 0.1949, ROUGE-1: 0.2789\n",
      "\n",
      "Evaluating prev book 84...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1728                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1765                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2034                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1765                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1618                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1965                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1806                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2118                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1724                    \n",
      "Step 7 prev book 84 ROUGE-L: 0.1804, ROUGE-1: 0.2700\n",
      "\n",
      "Evaluating prev book 1342...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1486                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1761                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2112                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1606                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2420                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2289                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1637                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1963                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1657                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1931                    \n",
      "Step 7 prev book 1342 ROUGE-L: 0.1886, ROUGE-1: 0.2800\n",
      "\n",
      "Evaluating prev book 11...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2581                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2353                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1420                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1342                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1579                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1549                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1916                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2000                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1321                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1697                    \n",
      "Step 7 prev book 11 ROUGE-L: 0.1776, ROUGE-1: 0.2780\n",
      "\n",
      "Evaluating prev book 2701...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.6105                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.5773                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.4158                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.4078                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2632                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2034                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.5859                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1733                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1290                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1795                    \n",
      "Step 7 prev book 2701 ROUGE-L: 0.3546, ROUGE-1: 0.3964\n",
      "\n",
      "Evaluating prev book 74...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1212                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2326                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1667                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1644                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2317                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2025                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2420                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1437                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1467                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1569                    \n",
      "Step 7 prev book 74 ROUGE-L: 0.1808, ROUGE-1: 0.2478\n",
      "Step 7 retention perplexity: 103.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7 general perplexity: 16.88\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 8 (Unlearning D_f^8)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^8...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_8...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 07:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_8 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_8.\n",
      "Skipped saving step_8 checkpoint (SAVE_STEP_MODELS=False)\n",
      "\n",
      "Evaluating current book 5200...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2169                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1266                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2105                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2000                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1392                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1486                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1637                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1912                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1356                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1839                    \n",
      "Step 8 current book 5200 ROUGE-L: 0.1716, ROUGE-1: 0.2809\n",
      "\n",
      "Evaluating prev book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1899                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1808                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2118                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2367                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1739                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1307                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1626                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1750                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1341                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2500                    \n",
      "Step 8 prev book 1661 ROUGE-L: 0.1845, ROUGE-1: 0.2782\n",
      "\n",
      "Evaluating prev book 84...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1875                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1366                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2484                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1637                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1951                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1965                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1655                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1985                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1775                    \n",
      "Step 8 prev book 84 ROUGE-L: 0.1821, ROUGE-1: 0.2607\n",
      "\n",
      "Evaluating prev book 1342...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1644                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1935                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2078                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1486                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2375                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2125                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1647                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1951                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1807                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1745                    \n",
      "Step 8 prev book 1342 ROUGE-L: 0.1879, ROUGE-1: 0.2709\n",
      "\n",
      "Evaluating prev book 11...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2727                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2237                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2036                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1351                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1438                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1867                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2339                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2410                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1491                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1761                    \n",
      "Step 8 prev book 11 ROUGE-L: 0.1966, ROUGE-1: 0.3118\n",
      "\n",
      "Evaluating prev book 2701...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.6105                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.5625                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.3248                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.4211                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2586                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2453                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.5714                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1830                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1280                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1684                    \n",
      "Step 8 prev book 2701 ROUGE-L: 0.3474, ROUGE-1: 0.3957\n",
      "\n",
      "Evaluating prev book 74...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.0517                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2137                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1791                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1507                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2156                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1807                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2471                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1667                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1635                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1733                    \n",
      "Step 8 prev book 74 ROUGE-L: 0.1742, ROUGE-1: 0.2274\n",
      "\n",
      "Evaluating prev book 98...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.3851                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.4551                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1943                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1697                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1982                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2485                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2138                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1935                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1892                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2381                    \n",
      "Step 8 prev book 98 ROUGE-L: 0.2486, ROUGE-1: 0.3191\n",
      "Step 8 retention perplexity: 103.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 8 general perplexity: 16.88\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 9 (Unlearning D_f^9)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^9...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_9...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 08:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_9 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_9.\n",
      "Skipped saving step_9 checkpoint (SAVE_STEP_MODELS=False)\n",
      "\n",
      "Evaluating current book 6130...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.4833                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1639                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2056                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1688                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2073                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1667                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1916                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2331                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2169                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2420                    \n",
      "Step 9 current book 6130 ROUGE-L: 0.2279, ROUGE-1: 0.3117\n",
      "\n",
      "Evaluating prev book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1793                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1916                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2485                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1916                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1605                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1366                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2063                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1948                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1463                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2282                    \n",
      "Step 9 prev book 1661 ROUGE-L: 0.1884, ROUGE-1: 0.2880\n",
      "\n",
      "Evaluating prev book 84...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1786                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1657                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2022                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1557                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1657                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1954                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1935                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2159                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1829                    \n",
      "Step 9 prev book 84 ROUGE-L: 0.1807, ROUGE-1: 0.2727\n",
      "\n",
      "Evaluating prev book 1342...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1630                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1745                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2061                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1605                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2258                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2118                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1807                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1892                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1807                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1879                    \n",
      "Step 9 prev book 1342 ROUGE-L: 0.1880, ROUGE-1: 0.2747\n",
      "\n",
      "Evaluating prev book 11...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2987                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2264                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2118                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1472                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1635                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1605                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2143                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2143                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1538                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1863                    \n",
      "Step 9 prev book 11 ROUGE-L: 0.1977, ROUGE-1: 0.3108\n",
      "\n",
      "Evaluating prev book 2701...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.6042                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.5567                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.3590                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.4330                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.4815                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.3922                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.5859                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1733                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1333                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1684                    \n",
      "Step 9 prev book 2701 ROUGE-L: 0.3887, ROUGE-1: 0.4325\n",
      "\n",
      "Evaluating prev book 74...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.0986                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2256                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1778                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1528                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2156                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1987                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2339                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1446                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1711                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1745                    \n",
      "Step 9 prev book 74 ROUGE-L: 0.1793, ROUGE-1: 0.2490\n",
      "\n",
      "Evaluating prev book 98...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.3558                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2699                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2048                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1954                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1739                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2125                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2073                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2256                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2048                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2303                    \n",
      "Step 9 prev book 98 ROUGE-L: 0.2280, ROUGE-1: 0.3014\n",
      "\n",
      "Evaluating prev book 5200...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1916                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1605                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1754                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1977                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1350                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1600                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1657                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1745                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1273                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1864                    \n",
      "Step 9 prev book 5200 ROUGE-L: 0.1674, ROUGE-1: 0.2691\n",
      "Step 9 retention perplexity: 103.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 9 general perplexity: 16.88\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 10 (Unlearning D_f^10)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^10...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_10...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 07:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_10 ---\n",
      "LoRA weights negated.\n",
      "Task Vector Negation complete for step_10.\n",
      "Unlearned model step_10 saved.\n",
      "\n",
      "Evaluating current book 174...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2111                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.3205                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2395                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1143                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1681                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1384                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1625                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1420                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1361                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1401                    \n",
      "Step 10 current book 174 ROUGE-L: 0.1773, ROUGE-1: 0.2442\n",
      "\n",
      "Evaluating prev book 1661...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1341                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1916                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2130                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2339                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1852                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1341                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1681                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1911                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1358                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2378                    \n",
      "Step 10 prev book 1661 ROUGE-L: 0.1825, ROUGE-1: 0.2715\n",
      "\n",
      "Evaluating prev book 84...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1513                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1786                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1625                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.2360                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1765                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1795                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1705                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2027                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2105                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1734                    \n",
      "Step 10 prev book 84 ROUGE-L: 0.1841, ROUGE-1: 0.2647\n",
      "\n",
      "Evaluating prev book 1342...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1379                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1739                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2209                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1633                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2264                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2207                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1657                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2121                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1647                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2238                    \n",
      "Step 10 prev book 1342 ROUGE-L: 0.1909, ROUGE-1: 0.2754\n",
      "\n",
      "Evaluating prev book 11...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.2658                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2264                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1637                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1472                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1688                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1486                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2209                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1905                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1471                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1491                    \n",
      "Step 10 prev book 11 ROUGE-L: 0.1828, ROUGE-1: 0.2977\n",
      "\n",
      "Evaluating prev book 2701...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.5510                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.5567                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.3368                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.4038                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2500                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.3883                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.5455                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1757                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1167                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1573                    \n",
      "Step 10 prev book 2701 ROUGE-L: 0.3482, ROUGE-1: 0.3959\n",
      "\n",
      "Evaluating prev book 74...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1203                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.2188                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1832                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1589                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2156                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2013                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2486                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1739                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1600                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1605                    \n",
      "Step 10 prev book 74 ROUGE-L: 0.1841, ROUGE-1: 0.2557\n",
      "\n",
      "Evaluating prev book 98...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.4505                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.4671                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.1871                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1829                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1892                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2236                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1818                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2235                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1772                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2353                    \n",
      "Step 10 prev book 98 ROUGE-L: 0.2518, ROUGE-1: 0.3337\n",
      "\n",
      "Evaluating prev book 5200...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.1928                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1491                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2118                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1882                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.1290                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.1573                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.1765                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.1931                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.1441                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.1765                    \n",
      "Step 10 prev book 5200 ROUGE-L: 0.1718, ROUGE-1: 0.2746\n",
      "\n",
      "Evaluating prev book 6130...\n",
      "  Completed pair 1/10 - Max ROUGE-L: 0.5299                    \n",
      "  Completed pair 2/10 - Max ROUGE-L: 0.1833                    \n",
      "  Completed pair 3/10 - Max ROUGE-L: 0.2149                    \n",
      "  Completed pair 4/10 - Max ROUGE-L: 0.1699                    \n",
      "  Completed pair 5/10 - Max ROUGE-L: 0.2099                    \n",
      "  Completed pair 6/10 - Max ROUGE-L: 0.2159                    \n",
      "  Completed pair 7/10 - Max ROUGE-L: 0.2118                    \n",
      "  Completed pair 8/10 - Max ROUGE-L: 0.2249                    \n",
      "  Completed pair 9/10 - Max ROUGE-L: 0.2099                    \n",
      "  Completed pair 10/10 - Max ROUGE-L: 0.2152                    \n",
      "Step 10 prev book 6130 ROUGE-L: 0.2386, ROUGE-1: 0.3216\n",
      "Step 10 retention perplexity: 103.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10 general perplexity: 16.88\n",
      "\n",
      "============================================================\n",
      "SEQUENTIAL UNLEARNING COMPLETE\n",
      "============================================================\n",
      "Final Unlearned Model: ssu_unlearned_models/step_10_unlearned_model\n",
      "\n",
      "Testing generation with final unlearned model...\n",
      "Reloading model for clean inference...\n",
      "Prompt: The quick brown fox\n",
      "Generated: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "This is a classic pangram – a sentence that contains every letter of the alphabet.\n",
      "\n",
      "It's often used\n"
     ]
    }
   ],
   "source": [
    "## 8. Sequential Unlearning Steps\n",
    "\n",
    "def run_sequential_unlearning(start_step=1, end_step=None, general_validation_text=None, run_evaluation=True):\n",
    "    # Suppress dynamo errors\n",
    "    import torch._dynamo\n",
    "    torch._dynamo.config.suppress_errors = True\n",
    "    \"\"\"Run sequential unlearning steps with optional evaluation reporting.\"\"\"\n",
    "    # Ensure all required dependencies are available\n",
    "    missing = []\n",
    "    try:\n",
    "        _ = Config.OUTPUT_DIR\n",
    "    except NameError:\n",
    "        missing.append(\"Config (cell 4)\")\n",
    "    \n",
    "    try:\n",
    "        _ = tokenizer\n",
    "    except NameError:\n",
    "        missing.append(\"tokenizer (cell 11)\")\n",
    "    \n",
    "    if missing:\n",
    "        raise NameError(\n",
    "            f\"The following are not defined: {', '.join(missing)}. \"\n",
    "            f\"Please run the required cells first.\"\n",
    "        )\n",
    "    \n",
    "    if end_step is None:\n",
    "        end_step = Config.NUM_UNLEARNING_STEPS\n",
    "    \n",
    "    general_validation_text = general_validation_text or DUMMY_BOOK_TEXT\n",
    "    \n",
    "    # Load or start from memorized model (check if file exists, not variable)\n",
    "    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n",
    "    memorized_model_on_disk = os.path.exists(memorized_model_path)\n",
    "    memorized_model_in_memory = 'memorized_model' in globals()\n",
    "    if not memorized_model_on_disk and not memorized_model_in_memory:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Memorized model not found at {memorized_model_path} and no in-memory model available. \"\n",
    "            f\"Please run initial fine-tuning first.\"\n",
    "        )\n",
    "    \n",
    "    # Determine starting model\n",
    "    from transformers import AutoModelForCausalLM\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    \n",
    "    if start_step == 1:\n",
    "        if memorized_model_on_disk:\n",
    "            current_model = AutoModelForCausalLM.from_pretrained(\n",
    "                memorized_model_path,\n",
    "                torch_dtype=dtype,\n",
    "                device_map=device\n",
    "            )\n",
    "            print(f\"Starting from memorized model (disk) for step 1\")\n",
    "        else:\n",
    "            current_model = memorized_model.to(device)\n",
    "            print(f\"Starting from memorized model (in-memory) for step 1\")\n",
    "        current_model.requires_grad_(False)\n",
    "    else:\n",
    "        # Load model from previous step\n",
    "        prev_step_path = f\"{Config.OUTPUT_DIR}/step_{start_step-1}_unlearned_model\"\n",
    "        if not os.path.exists(prev_step_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Previous step model not found at {prev_step_path}. \"\n",
    "                f\"Resume is only possible if step checkpoints were saved.\"\n",
    "            )\n",
    "        current_model = AutoModelForCausalLM.from_pretrained(\n",
    "            prev_step_path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device\n",
    "        )\n",
    "        current_model.requires_grad_(False)\n",
    "        print(f\"Starting from step {start_step-1} model for step {start_step}\")\n",
    "    \n",
    "    # Get retention dataset (D_nor) first - needed for random_label_ids\n",
    "    retention_dataset = get_retention_dataset()\n",
    "    \n",
    "    # Extract random_label_ids from D_nor for use in forget datasets\n",
    "    random_label_ids = retention_dataset.input_ids if retention_dataset is not None else None\n",
    "    if random_label_ids is None:\n",
    "        raise RuntimeError(\"retention_dataset is required for SSU (provides random_label_ids)\")\n",
    "    \n",
    "    # Load sequential datasets for unlearning with random_label_ids\n",
    "    unlearning_datasets = get_unlearning_datasets(random_label_ids=random_label_ids)\n",
    "    print(f\"\\nGenerated {len(unlearning_datasets)} sequential unlearning datasets.\")\n",
    "    \n",
    "    all_target_book_ids = [book_id for ids in Config.GUTENBERG_BOOK_IDS.values() for book_id in ids]\n",
    "    evaluation_log = {\"baseline\": {}, \"steps\": []}\n",
    "    eval_book_cache = {}\n",
    "    eval_book_dir = os.path.join(Config.DATA_DIR, \"evaluation_books\")\n",
    "    os.makedirs(eval_book_dir, exist_ok=True)\n",
    "    \n",
    "    def get_book_text_for_eval(book_id):\n",
    "        if book_id in eval_book_cache:\n",
    "            return eval_book_cache[book_id]\n",
    "        book_file = download_gutenberg_book(book_id, eval_book_dir)\n",
    "        if not book_file:\n",
    "            raise RuntimeError(f\"Failed to download book {book_id} for evaluation\")\n",
    "        text = load_book_text(book_file)\n",
    "        if not text or len(text) < 1000:\n",
    "            raise RuntimeError(f\"Book {book_id} text is invalid or too short for evaluation\")\n",
    "        eval_book_cache[book_id] = text\n",
    "        maybe_delete_file(book_file)\n",
    "        return text\n",
    "    \n",
    "    if run_evaluation:\n",
    "        print(\"\\n=== Baseline Evaluation ===\")\n",
    "        print(f\"Note: Evaluation generates {Config.EVAL_NUM_SAMPLES} samples per prompt, this may take a while...\")\n",
    "        baseline_regurg = {}\n",
    "        for book_id in all_target_book_ids:\n",
    "            print(f\"\\nEvaluating book {book_id}...\")\n",
    "            metrics = evaluate_regurgitation(\n",
    "                current_model, \n",
    "                tokenizer, \n",
    "                get_book_text_for_eval(book_id),\n",
    "                max_pairs=Config.EVAL_MAX_PAIRS,\n",
    "                num_samples=Config.EVAL_NUM_SAMPLES,\n",
    "                verbose=True\n",
    "            )\n",
    "            baseline_regurg[book_id] = metrics\n",
    "            print(f\"Book {book_id} baseline ROUGE-L: {metrics['rougeL']:.4f}, ROUGE-1: {metrics['rouge1']:.4f}\")\n",
    "        baseline_perplexity = {}\n",
    "        if retention_dataset is not None:\n",
    "            baseline_perplexity['retention'] = evaluate_perplexity(current_model, tokenizer, retention_dataset)\n",
    "            print(f\"Retention baseline perplexity: {baseline_perplexity['retention']:.2f}\")\n",
    "        baseline_perplexity['general'] = evaluate_perplexity(current_model, tokenizer, general_validation_text)\n",
    "        print(f\"General baseline perplexity: {baseline_perplexity['general']:.2f}\")\n",
    "        evaluation_log['baseline'] = {\n",
    "            'regurgitation': baseline_regurg,\n",
    "            'perplexity': baseline_perplexity,\n",
    "        }\n",
    "    \n",
    "    # Track latest saved checkpoint to delete previous ones if configured\n",
    "    last_saved_checkpoint = None\n",
    "    \n",
    "    # STEP 1-N: Sequential Unlearning Loop\n",
    "    for t in range(start_step - 1, min(end_step, Config.NUM_UNLEARNING_STEPS)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"UNLEARNING STEP {t+1} (Unlearning D_f^{t+1})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        dataset_t = unlearning_datasets[t]\n",
    "        step_prefix = f\"step_{t+1}\"\n",
    "        \n",
    "        # SSU training uses only D_f^t (dataset_t), not D_nor\n",
    "        # D_nor is only used for evaluation and random labels\n",
    "        train_dataset = dataset_t\n",
    "        \n",
    "        # Fine-Tuning Stage\n",
    "        print(f\"Preparing LoRA model for fine-tuning on D_f^{t+1}...\")\n",
    "        \n",
    "        # Ensure model is on correct device\n",
    "        device = next(current_model.parameters()).device\n",
    "        if device.type == \"meta\" or str(device) == \"meta\":\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            current_model = current_model.to(device)\n",
    "        else:\n",
    "            # Convert device object to string if needed\n",
    "            device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n",
    "        \n",
    "        if hasattr(current_model, \"enable_input_require_grads\"):\n",
    "            current_model.enable_input_require_grads()\n",
    "        else:\n",
    "            def make_inputs_require_grad(module, input, output):\n",
    "                output.requires_grad_(True)\n",
    "            current_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "        \n",
    "        lora_model = create_lora_model(current_model)\n",
    "        # PEFT models inherit device from base model, no need to call .to()\n",
    "        lora_model.print_trainable_parameters()\n",
    "        \n",
    "        # Disable cache for gradient checkpointing\n",
    "        if hasattr(lora_model.config, \"use_cache\"):\n",
    "            lora_model.config.use_cache = False\n",
    "        \n",
    "        # Dynamic learning rate: 1e-5 for steps 1-5, 1e-6 for steps 6-10\n",
    "        lr = 1e-5 if (t + 1) <= 5 else 1e-6\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{Config.OUTPUT_DIR}/{step_prefix}_ft_checkpoints\",\n",
    "            per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "            gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "            warmup_steps=Config.WARMUP_STEPS,\n",
    "            learning_rate=lr,\n",
    "            num_train_epochs=Config.NUM_EPOCHS_FT,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"no\",\n",
    "            report_to=\"none\",\n",
    "            fp16=False,\n",
    "            bf16=torch.cuda.is_available() and device == \"cuda\",\n",
    "            dataloader_pin_memory=False,  # Fix device issues\n",
    "            label_names=[\"labels\"],\n",
    "            gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "            remove_unused_columns=False,  # Keep custom labels\n",
    "            weight_decay=Config.WEIGHT_DECAY,\n",
    "        )\n",
    "\n",
    "        # Use custom data collator that preserves labels_fgt and labels_rnd\n",
    "        data_collator = SSUDataCollator(tokenizer=tokenizer)\n",
    "        \n",
    "        trainer = SSUTrainer(\n",
    "            model=lora_model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        print(f\"Starting fine-tuning with SSU Loss for {step_prefix}...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Task Vector Negation Stage: theta_new = theta_old - Delta_LoRA\n",
    "        print(f\"\\n--- Applying Task Vector Negation for {step_prefix} ---\")\n",
    "        \n",
    "        # 1. Negate LoRA weights: W_new = W_old - (W_ft - W_old) = W_old - W_lora\n",
    "        # We achieve this by multiplying LoRA weights by -1, then merging.\n",
    "        with torch.no_grad():\n",
    "            for name, param in lora_model.named_parameters():\n",
    "                if \"lora\" in name:\n",
    "                    param.data = -1 * param.data\n",
    "        \n",
    "        print(\"LoRA weights negated.\")\n",
    "        \n",
    "        # 2. Merge negated weights into base model\n",
    "        current_model = lora_model.merge_and_unload()\n",
    "        current_model.requires_grad_(False)\n",
    "        \n",
    "        print(f\"Task Vector Negation complete for {step_prefix}.\")\n",
    "        \n",
    "        # Save the unlearned model (optionally keep only final checkpoint)\n",
    "        save_path = f\"{Config.OUTPUT_DIR}/{step_prefix}_unlearned_model\"\n",
    "        is_final_step = (t + 1) == min(end_step, Config.NUM_UNLEARNING_STEPS)\n",
    "        save_this_step = Config.SAVE_STEP_MODELS or is_final_step\n",
    "        if save_this_step:\n",
    "            current_model.save_pretrained(save_path)\n",
    "            tokenizer.save_pretrained(save_path)\n",
    "            print(f\"Unlearned model {step_prefix} saved.\")\n",
    "            if Config.DELETE_PREVIOUS_STEP_MODELS and last_saved_checkpoint and last_saved_checkpoint != save_path:\n",
    "                print(f\"Deleting previous checkpoint: {last_saved_checkpoint}\")\n",
    "                cleanup_dir(last_saved_checkpoint)\n",
    "            last_saved_checkpoint = save_path\n",
    "        else:\n",
    "            # Remove any stale checkpoint directory to avoid disk bloat\n",
    "            cleanup_dir(save_path)\n",
    "            print(f\"Skipped saving {step_prefix} checkpoint (SAVE_STEP_MODELS=False)\")\n",
    "        \n",
    "        if run_evaluation:\n",
    "            step_metrics = {\n",
    "                'step': t + 1,\n",
    "                'current_books': {},  # D_f^t (current step)\n",
    "                'prev_books': {},     # D_prev (previously unlearned)\n",
    "                'perplexity': {},\n",
    "            }\n",
    "            \n",
    "            # Evaluate on current books (D_f^t)\n",
    "            current_book_ids = Config.GUTENBERG_BOOK_IDS.get(t + 1, [])\n",
    "            for book_id in current_book_ids:\n",
    "                print(f\"\\nEvaluating current book {book_id}...\")\n",
    "                metrics = evaluate_regurgitation(\n",
    "                    current_model, \n",
    "                    tokenizer, \n",
    "                    get_book_text_for_eval(book_id),\n",
    "                    max_pairs=Config.EVAL_MAX_PAIRS,\n",
    "                    num_samples=Config.EVAL_NUM_SAMPLES,\n",
    "                    verbose=True\n",
    "                )\n",
    "                step_metrics['current_books'][book_id] = metrics\n",
    "                print(f\"Step {t+1} current book {book_id} ROUGE-L: {metrics['rougeL']:.4f}, ROUGE-1: {metrics['rouge1']:.4f}\")\n",
    "            \n",
    "            # Evaluate on previous books (D_prev) - union of steps 1..t\n",
    "            prev_book_ids = []\n",
    "            for prev_step in range(1, t + 1):\n",
    "                prev_book_ids.extend(Config.GUTENBERG_BOOK_IDS.get(prev_step, []))\n",
    "            for book_id in prev_book_ids:\n",
    "                print(f\"\\nEvaluating prev book {book_id}...\")\n",
    "                metrics = evaluate_regurgitation(\n",
    "                    current_model, \n",
    "                    tokenizer, \n",
    "                    get_book_text_for_eval(book_id),\n",
    "                    max_pairs=Config.EVAL_MAX_PAIRS,\n",
    "                    num_samples=Config.EVAL_NUM_SAMPLES,\n",
    "                    verbose=True\n",
    "                )\n",
    "                step_metrics['prev_books'][book_id] = metrics\n",
    "                print(f\"Step {t+1} prev book {book_id} ROUGE-L: {metrics['rougeL']:.4f}, ROUGE-1: {metrics['rouge1']:.4f}\")\n",
    "            \n",
    "            # Evaluate perplexity on D_nor (retention) and general text\n",
    "            if retention_dataset is not None:\n",
    "                retention_ppl = evaluate_perplexity(current_model, tokenizer, retention_dataset)\n",
    "                step_metrics['perplexity']['retention'] = retention_ppl\n",
    "                print(f\"Step {t+1} retention perplexity: {retention_ppl:.2f}\")\n",
    "            general_ppl = evaluate_perplexity(current_model, tokenizer, general_validation_text)\n",
    "            step_metrics['perplexity']['general'] = general_ppl\n",
    "            print(f\"Step {t+1} general perplexity: {general_ppl:.2f}\")\n",
    "            evaluation_log['steps'].append(step_metrics)\n",
    "        \n",
    "        # current_model is already updated via merge_and_unload\n",
    "\n",
    "    # Final Evaluation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SEQUENTIAL UNLEARNING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    final_step = min(end_step, Config.NUM_UNLEARNING_STEPS)\n",
    "    final_model_dir = f\"{Config.OUTPUT_DIR}/step_{final_step}_unlearned_model\"\n",
    "    print(f\"Final Unlearned Model: {final_model_dir}\")\n",
    "    \n",
    "    # Test generation\n",
    "    prompt = \"The quick brown fox\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    print(\"\\nTesting generation with final unlearned model...\")\n",
    "    \n",
    "    # Reload model to remove training hooks/state that interfere with inference\n",
    "    print(\"Reloading model for clean inference...\")\n",
    "    del current_model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    # Re-determine device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    current_model = AutoModelForCausalLM.from_pretrained(\n",
    "        final_model_dir,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=device,\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "    current_model.requires_grad_(False)\n",
    "    current_model.eval()\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Completely disable torch.compile to avoid compilation errors\n",
    "    torch._dynamo.reset()\n",
    "    # This completely disables dynamo/torch.compile\n",
    "    original_disable_state = torch._dynamo.config.disable\n",
    "    torch._dynamo.config.disable = True\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            output_tokens = current_model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=30, \n",
    "                do_sample=True, \n",
    "                top_p=0.9, \n",
    "                temperature=0.7\n",
    "            )\n",
    "    finally:\n",
    "        # Restore original state\n",
    "        torch._dynamo.config.disable = original_disable_state\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    \n",
    "    if run_evaluation:\n",
    "        run_sequential_unlearning.last_evaluation = evaluation_log\n",
    "    \n",
    "    if Config.CLEANUP_FINAL_MODEL_DIR:\n",
    "        print(f\"Removing final model directory per configuration: {final_model_dir}\")\n",
    "        cleanup_dir(final_model_dir)\n",
    "    \n",
    "    return current_model\n",
    "\n",
    "\n",
    "# Run sequential unlearning steps\n",
    "# You can specify start_step and end_step to resume from a specific step\n",
    "# Example: run_sequential_unlearning(start_step=2, end_step=3) to run only steps 2-3\n",
    "final_unlearned_model = run_sequential_unlearning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deaf902",
   "metadata": {
    "papermill": {
     "duration": 0.05637,
     "end_time": "2025-11-25T19:27:45.360932",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.304562",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 10. Verification & Testing\n",
    "\n",
    "**Automated checks** ensure the core SSU components work before running long jobs:\n",
    "- Dataset construction keeps the correct masking for `labels`, `labels_fgt`, and `labels_rnd`.\n",
    "- The custom trainer mixes retention and forgetting losses without shape errors.\n",
    "- A miniature pipeline run (concatenated forget + retain batches) executes an optimizer step without crashing.\n",
    "\n",
    "**Manual checks** after training runs:\n",
    "- Generate from a known passage of a removed book and confirm the model no longer regurgitates it.\n",
    "- Measure perplexity on `D_nor` before vs. after each step to ensure general capability stays stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d6ac2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:27:45.478565Z",
     "iopub.status.busy": "2025-11-25T19:27:45.478141Z",
     "iopub.status.idle": "2025-11-25T19:27:45.493451Z",
     "shell.execute_reply": "2025-11-25T19:27:45.492750Z"
    },
    "papermill": {
     "duration": 0.078354,
     "end_time": "2025-11-25T19:27:45.494613",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.416259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automated sanity checks for dataset, trainer, and pipeline wiring\n",
    "\n",
    "\n",
    "def run_automated_tests():\n",
    "    tok = tokenizer\n",
    "    sample_texts = [\"Sanity sample text \" + str(i) for i in range(4)]\n",
    "\n",
    "    # Create dummy random_label_ids for forget dataset\n",
    "    dummy_tokenized = tok(\n",
    "        sample_texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=Config.CHUNK_SIZE,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    dummy_random_label_ids = dummy_tokenized['input_ids']\n",
    "\n",
    "    # Dataset + collator check\n",
    "    forget_ds = SequentialUnlearningDataset(tok, sample_texts, mode=\"forget\", random_label_ids=dummy_random_label_ids)\n",
    "    retain_ds = SequentialUnlearningDataset(tok, sample_texts, mode=\"retain\")\n",
    "    collator = SSUDataCollator(tok)\n",
    "    batch = collator([forget_ds[0], retain_ds[0]])\n",
    "    assert 'labels' in batch and 'labels_fgt' in batch and 'labels_rnd' in batch\n",
    "    assert batch['labels'][0].eq(-100).all(), \"Forget sample should mask retention labels\"\n",
    "    assert not batch['labels'][1].eq(-100).all(), \"Retain sample should keep labels\"\n",
    "    assert not batch['labels_fgt'][0].eq(-100).all(), \"Forget sample must keep L_fgt\"\n",
    "    assert not batch['labels_rnd'][0].eq(-100).all(), \"Forget sample must keep L_rnd\"\n",
    "\n",
    "    # Trainer loss check - SSU loss should be λ1 * L_fgt + λ2 * L_rnd (no retention loss)\n",
    "    class MockModel(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.losses = [torch.tensor(0.4), torch.tensor(0.2)]  # fgt, rnd\n",
    "        def forward(self, *args, **kwargs):\n",
    "            loss_val = self.losses.pop(0) if self.losses else torch.tensor(0.0)\n",
    "            return SimpleNamespace(loss=loss_val)\n",
    "    mock_model = MockModel()\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(Config.OUTPUT_DIR, \"test_runs\"),\n",
    "        per_device_train_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        report_to=[],\n",
    "        logging_steps=1000,\n",
    "    )\n",
    "    trainer = SSUTrainer(model=mock_model, args=args, train_dataset=None)\n",
    "    dummy_inputs = {\n",
    "        'input_ids': torch.ones((1, 4), dtype=torch.long),\n",
    "        'attention_mask': torch.ones((1, 4), dtype=torch.long),\n",
    "        'labels': torch.ones((1, 4), dtype=torch.long),  # Not used in SSU loss\n",
    "        'labels_fgt': torch.ones((1, 4), dtype=torch.long),\n",
    "        'labels_rnd': torch.ones((1, 4), dtype=torch.long),\n",
    "    }\n",
    "    loss = trainer.compute_loss(mock_model, dummy_inputs)\n",
    "    expected = Config.EPSILON_1 * 0.4 + Config.EPSILON_2 * 0.2\n",
    "    assert abs(loss.item() - expected) < 1e-6, f\"SSU loss incorrect: expected {expected}, got {loss.item()}\"\n",
    "\n",
    "    # Mini pipeline batch check (forget-only batch, no retain)\n",
    "    trainer = SSUTrainer(\n",
    "        model=mock_model,\n",
    "        args=args,\n",
    "        train_dataset=forget_ds,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "    data_loader = trainer.get_train_dataloader()\n",
    "    batch = next(iter(data_loader))\n",
    "    assert 'labels' in batch and 'labels_fgt' in batch and 'labels_rnd' in batch, \"Batch missing required labels\"\n",
    "\n",
    "    print(\"All automated SSU sanity tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd4025a",
   "metadata": {
    "papermill": {
     "duration": 0.057137,
     "end_time": "2025-11-25T19:27:45.610182",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.553045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 11. Ablation Study: Removing `D_nor`\n",
    "\n",
    "To illustrate the value of retention data, re-run sequential unlearning with `D_nor` disabled.\n",
    "You should observe:\n",
    "- Regurgitation on the target books still decreases, proving the SSU losses work.\n",
    "- Perplexity on general data collapses, showing catastrophic forgetting without the retention anchor.\n",
    "\n",
    "After the run, compare the perplexity deltas and summarize them using:\n",
    "> Including D_nor stabilizes the model and reduces unlearning-induced degradation in general perplexity by **X%** compared to a naive SSU-only objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96570568",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:27:45.734900Z",
     "iopub.status.busy": "2025-11-25T19:27:45.734631Z",
     "iopub.status.idle": "2025-11-25T19:27:45.739254Z",
     "shell.execute_reply": "2025-11-25T19:27:45.738579Z"
    },
    "papermill": {
     "duration": 0.073653,
     "end_time": "2025-11-25T19:27:45.740470",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.666817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_ablation_without_retention(start_step=1, end_step=None, **kwargs):\n",
    "    \"\"\"Helper to rerun SSU without D_nor for ablation studies.\"\"\"\n",
    "    original_flag = Config.USE_RETENTION_DATA\n",
    "    try:\n",
    "        Config.USE_RETENTION_DATA = False\n",
    "        print(\"\\n>>> Running ablation: retention data disabled\")\n",
    "        return run_sequential_unlearning(\n",
    "            start_step=start_step,\n",
    "            end_step=end_step,\n",
    "            run_evaluation=kwargs.get('run_evaluation', True),\n",
    "            general_validation_text=kwargs.get('general_validation_text'),\n",
    "        )\n",
    "    finally:\n",
    "        Config.USE_RETENTION_DATA = original_flag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "308eb56d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-25T19:27:45.971005Z",
     "iopub.status.busy": "2025-11-25T19:27:45.970289Z",
     "iopub.status.idle": "2025-11-25T19:27:56.475759Z",
     "shell.execute_reply": "2025-11-25T19:27:56.474920Z"
    },
    "papermill": {
     "duration": 10.56259,
     "end_time": "2025-11-25T19:27:56.476882",
     "exception": false,
     "start_time": "2025-11-25T19:27:45.914292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: The quick brown fox\n",
      "Generated: <bos>The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "This sentence is a well-known pangram – it contains every letter of the English alphabet at least once. It's frequently used for testing typewriters, fonts, and keyboards.\n",
      "\n",
      "Is there anything else you'd like to know about the sentence \"The quick brown fox jumps over the lazy dog\"?\n",
      "<end_of_turn>\n",
      "\n",
      "Prompt 2: Once upon a time in a quiet village\n",
      "Generated: <bos>Once upon a time in a quiet village nestled beside a sparkling river lived a young woman named Elara. She was known throughout the land for her remarkable skill with herbs and her gentle spirit.\n",
      "\n",
      "Elara's days were spent tending her small garden, collecting herbs, and brewing potions for the villagers. However, she felt a persistent yearning for something more, a feeling that she didn't quite understand.  \n",
      "\n",
      "One day, a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity generation with the latest unlearned model\n",
    "if 'final_unlearned_model' in globals():\n",
    "    final_unlearned_model.eval()\n",
    "    sample_device = next(final_unlearned_model.parameters()).device\n",
    "    demo_prompts = [\n",
    "        \"The quick brown fox\",\n",
    "        \"Once upon a time in a quiet village\",\n",
    "    ]\n",
    "    for idx, prompt in enumerate(demo_prompts, 1):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(sample_device)\n",
    "        with torch.no_grad():\n",
    "            gen_tokens = final_unlearned_model.generate(\n",
    "                **inputs,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                max_new_tokens=80,\n",
    "            )\n",
    "        gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "        print(f\"Prompt {idx}: {prompt}\\nGenerated: {gen_text}\\n\")\n",
    "else:\n",
    "    print(\"Run run_sequential_unlearning() first to instantiate final_unlearned_model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41805.428388,
   "end_time": "2025-11-25T19:27:59.352216",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-25T07:51:13.923828",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "02acda10491b45038fb1e2f0942f2d97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "035d2008bce64747a7998e423ca9780c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0b1245a70b07496f99ff028b8af99b60": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_77593008769b44efaad701ba8d75623e",
       "placeholder": "​",
       "style": "IPY_MODEL_c81d93a6daca4058a5e8f6309fd74fe2",
       "tabbable": null,
       "tooltip": null,
       "value": " 2.00G/2.00G [00:05&lt;00:00, 1.14GB/s]"
      }
     },
     "0b898674b20f4ee5ac35bcd785dda99c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "18051c7b18764452879cd40c1b463bf5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_73a35915f078455dbd10d2ab0242c5ba",
       "max": 899,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2b883a7534e246ad9ddf9e146ab1d283",
       "tabbable": null,
       "tooltip": null,
       "value": 899
      }
     },
     "199d756ec48c4065adaec3775c7edf0b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0b898674b20f4ee5ac35bcd785dda99c",
       "placeholder": "​",
       "style": "IPY_MODEL_56f7ddaa2d644e4c8c3c241740387424",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "1a8ed895689c4df58ec830e6547dc57f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1aaebcc0fc2b47d18465d45068f97000": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ae6913021e44ce89002774972685e77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b20f1c5b6fd4706b2e1b7a43b95a899": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1d127b2d8e164a44b4df0f74fa3e0dc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cdb79a490a604b3d8e1f83879f0714e9",
        "IPY_MODEL_18051c7b18764452879cd40c1b463bf5",
        "IPY_MODEL_a7ba6c9967c84f5d8a3c2dba113b0612"
       ],
       "layout": "IPY_MODEL_6e428fbb42c54140a9c76db28cfb2ee7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "237a447a4e2340d68279e2c0817770c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_59b9e1cf793d47fbb2b4bd3bb8401ce6",
       "placeholder": "​",
       "style": "IPY_MODEL_aeda55431b0042da9241e652eb6fd943",
       "tabbable": null,
       "tooltip": null,
       "value": " 215/215 [00:00&lt;00:00, 29.8kB/s]"
      }
     },
     "27a44111278e43a58bc4e7c49b7628f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2b883a7534e246ad9ddf9e146ab1d283": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3662d196631e4f95bf6ed838cadfd415": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3bae2e8aac054a1d866692fe8e199a13": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3dfc147ebc52408c82cd8a1fa1ee5718": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_474bcc5066974eceaf41a8abf56b2d35",
       "placeholder": "​",
       "style": "IPY_MODEL_035d2008bce64747a7998e423ca9780c",
       "tabbable": null,
       "tooltip": null,
       "value": "added_tokens.json: 100%"
      }
     },
     "474bcc5066974eceaf41a8abf56b2d35": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "497c508200a44fd89b393b76ef030768": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3dfc147ebc52408c82cd8a1fa1ee5718",
        "IPY_MODEL_982c1ac3a17b4384b15836ebab6b3749",
        "IPY_MODEL_fb9d09b3cfdc4937841934332fb7ff39"
       ],
       "layout": "IPY_MODEL_e827fceba3224d96b3a5c4340d4dc41d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4aa0de0e584c4a438934dc7ae9ddb11f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4b265d591e84450aaf386f1be0404706": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4bb15f9566854555bce69b81c8abb861": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "508353b4c9a148eb830c7da8750e9b21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9962764b9a0941dda6c9ab8f4ab0fdb8",
        "IPY_MODEL_5d7bae2fe3034ec3a31973330caaef4f",
        "IPY_MODEL_baa861b91611413cb56720ad32bcaf7f"
       ],
       "layout": "IPY_MODEL_d1d8a177373847088379d9b7d1c7cd01",
       "tabbable": null,
       "tooltip": null
      }
     },
     "52ae1bbb5bd144aaa35d93981a7aa6e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5566f70e96534b60ab9922892e40258a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56f7ddaa2d644e4c8c3c241740387424": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "59b9e1cf793d47fbb2b4bd3bb8401ce6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5b889923a5134cb9971851cf1abab7d1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d7bae2fe3034ec3a31973330caaef4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fc74398114cd450e896a62c4a9a8c723",
       "max": 33384568,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_02acda10491b45038fb1e2f0942f2d97",
       "tabbable": null,
       "tooltip": null,
       "value": 33384568
      }
     },
     "60125ddc02a5435d911a7493eea0b7be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6279fd9f074642618f38593c1540760c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6a1a9c40f4a54f3caffde47f037b94b8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9a49c3eae3b24632a90a5a2a56839658",
       "placeholder": "​",
       "style": "IPY_MODEL_c4c80ecba06c42ee91411fef0f1a4f77",
       "tabbable": null,
       "tooltip": null,
       "value": " 662/662 [00:00&lt;00:00, 99.8kB/s]"
      }
     },
     "6e428fbb42c54140a9c76db28cfb2ee7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f0b988963a94ef59e55a0295bb2c2c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8a8d8cb2a23f4df09614934db1409acb",
        "IPY_MODEL_87ed6deaf6f747249cb2a5449b6c6cf7",
        "IPY_MODEL_89eadef8bb084e9586f58ac1b503f6b3"
       ],
       "layout": "IPY_MODEL_52ae1bbb5bd144aaa35d93981a7aa6e0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "73a35915f078455dbd10d2ab0242c5ba": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75d83b93087d4dbb87622ec9e8d9aec3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ee874e010c344cd68f85c41f75c75cb6",
       "max": 1999811208,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f05ca2fb9ce740a28f99ea950bca221e",
       "tabbable": null,
       "tooltip": null,
       "value": 1999811208
      }
     },
     "761b1e746bfc47a0b4454ddeedc1cae3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77593008769b44efaad701ba8d75623e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7897a88c25454680bb17248667fe28e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_761b1e746bfc47a0b4454ddeedc1cae3",
       "max": 215,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_dc5baad2d58f47d9bccd84085098026d",
       "tabbable": null,
       "tooltip": null,
       "value": 215
      }
     },
     "797c731971a74ef69de9f1d8518bc671": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7a1e751d86f543ec9c65a9c2f5dda843": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80bc8506f2aa4475816063b1217d01da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "86c324a3f4bd4e4cad39cf8c7b906f69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "87ed6deaf6f747249cb2a5449b6c6cf7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_feae82167567425fbec91e30cff92c55",
       "max": 4689074,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f64d3402e2904bb5bc33a719dae771e1",
       "tabbable": null,
       "tooltip": null,
       "value": 4689074
      }
     },
     "89493e077229431a9a31a0140ae05be2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_199d756ec48c4065adaec3775c7edf0b",
        "IPY_MODEL_9d6468b19bb7479181372718388b7752",
        "IPY_MODEL_6a1a9c40f4a54f3caffde47f037b94b8"
       ],
       "layout": "IPY_MODEL_df7a0a529ec04fbc90c771e070dbd810",
       "tabbable": null,
       "tooltip": null
      }
     },
     "89eadef8bb084e9586f58ac1b503f6b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6279fd9f074642618f38593c1540760c",
       "placeholder": "​",
       "style": "IPY_MODEL_9ab5c73caef14f86955a253830ee19f3",
       "tabbable": null,
       "tooltip": null,
       "value": " 4.69M/4.69M [00:01&lt;00:00, 128kB/s]"
      }
     },
     "8a8d8cb2a23f4df09614934db1409acb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1b20f1c5b6fd4706b2e1b7a43b95a899",
       "placeholder": "​",
       "style": "IPY_MODEL_8e19a1765f8c425bad00a344ee8a9a02",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.model: 100%"
      }
     },
     "8e19a1765f8c425bad00a344ee8a9a02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "90aa3edb75974a08b8ce8d33c8b28a15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9293eb120dd14f9899e058eb6bf21024": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "982c1ac3a17b4384b15836ebab6b3749": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4bb15f9566854555bce69b81c8abb861",
       "max": 35,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4aa0de0e584c4a438934dc7ae9ddb11f",
       "tabbable": null,
       "tooltip": null,
       "value": 35
      }
     },
     "9962764b9a0941dda6c9ab8f4ab0fdb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a1164b45ca95415a9d7eef81a3082c8b",
       "placeholder": "​",
       "style": "IPY_MODEL_4b265d591e84450aaf386f1be0404706",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.json: 100%"
      }
     },
     "9a49c3eae3b24632a90a5a2a56839658": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ab5c73caef14f86955a253830ee19f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9b21795e4d3244eca583850d39247dd7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9d6468b19bb7479181372718388b7752": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d79162cca6a14914be26c5ff9b5e0fe5",
       "max": 662,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_797c731971a74ef69de9f1d8518bc671",
       "tabbable": null,
       "tooltip": null,
       "value": 662
      }
     },
     "a1164b45ca95415a9d7eef81a3082c8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a6ee944603ae404b861e7492529369a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1a8ed895689c4df58ec830e6547dc57f",
       "placeholder": "​",
       "style": "IPY_MODEL_3662d196631e4f95bf6ed838cadfd415",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.16M/1.16M [00:00&lt;00:00, 3.17MB/s]"
      }
     },
     "a7ba6c9967c84f5d8a3c2dba113b0612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5b889923a5134cb9971851cf1abab7d1",
       "placeholder": "​",
       "style": "IPY_MODEL_f38cac89760d4828a415674755fb75a8",
       "tabbable": null,
       "tooltip": null,
       "value": " 899/899 [00:00&lt;00:00, 139kB/s]"
      }
     },
     "aeda55431b0042da9241e652eb6fd943": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b4e357b575fe4f79b9711a15f8e7798d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3bae2e8aac054a1d866692fe8e199a13",
       "placeholder": "​",
       "style": "IPY_MODEL_d695d551cd294506bdb085945b09858d",
       "tabbable": null,
       "tooltip": null,
       "value": "generation_config.json: 100%"
      }
     },
     "b9d9a19df93443d6aa26c8f030d5a9f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e9ac06f21ef846a3ab9fe0bedce218be",
       "max": 1156999,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_27a44111278e43a58bc4e7c49b7628f1",
       "tabbable": null,
       "tooltip": null,
       "value": 1156999
      }
     },
     "ba3b14fd8bf4473a959e0982d7dcb399": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b4e357b575fe4f79b9711a15f8e7798d",
        "IPY_MODEL_7897a88c25454680bb17248667fe28e9",
        "IPY_MODEL_237a447a4e2340d68279e2c0817770c1"
       ],
       "layout": "IPY_MODEL_dea76b4846784a55bbcdef3c02ebb3ed",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ba91e5eafcd948739f39a55cdbe0d5d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "baa861b91611413cb56720ad32bcaf7f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9293eb120dd14f9899e058eb6bf21024",
       "placeholder": "​",
       "style": "IPY_MODEL_86c324a3f4bd4e4cad39cf8c7b906f69",
       "tabbable": null,
       "tooltip": null,
       "value": " 33.4M/33.4M [00:00&lt;00:00, 70.9MB/s]"
      }
     },
     "bb85e4d0feb84dbc8c24453b764769f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f55aa3fba28f431988a35588fdea02e6",
        "IPY_MODEL_75d83b93087d4dbb87622ec9e8d9aec3",
        "IPY_MODEL_0b1245a70b07496f99ff028b8af99b60"
       ],
       "layout": "IPY_MODEL_7a1e751d86f543ec9c65a9c2f5dda843",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c4c80ecba06c42ee91411fef0f1a4f77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c81d93a6daca4058a5e8f6309fd74fe2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "cb8943fc75194bad8d71a2f4263dbdac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1ae6913021e44ce89002774972685e77",
       "placeholder": "​",
       "style": "IPY_MODEL_80bc8506f2aa4475816063b1217d01da",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "cdb79a490a604b3d8e1f83879f0714e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1aaebcc0fc2b47d18465d45068f97000",
       "placeholder": "​",
       "style": "IPY_MODEL_5566f70e96534b60ab9922892e40258a",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "d1d8a177373847088379d9b7d1c7cd01": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d695d551cd294506bdb085945b09858d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d79162cca6a14914be26c5ff9b5e0fe5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dc5baad2d58f47d9bccd84085098026d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dea76b4846784a55bbcdef3c02ebb3ed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df7a0a529ec04fbc90c771e070dbd810": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e827fceba3224d96b3a5c4340d4dc41d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e9ac06f21ef846a3ab9fe0bedce218be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ee874e010c344cd68f85c41f75c75cb6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f05ca2fb9ce740a28f99ea950bca221e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f38cac89760d4828a415674755fb75a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f4f43fa4839f49c980312dc3fc378685": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f55aa3fba28f431988a35588fdea02e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_60125ddc02a5435d911a7493eea0b7be",
       "placeholder": "​",
       "style": "IPY_MODEL_9b21795e4d3244eca583850d39247dd7",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "f64d3402e2904bb5bc33a719dae771e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fb9d09b3cfdc4937841934332fb7ff39": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f4f43fa4839f49c980312dc3fc378685",
       "placeholder": "​",
       "style": "IPY_MODEL_ba91e5eafcd948739f39a55cdbe0d5d7",
       "tabbable": null,
       "tooltip": null,
       "value": " 35.0/35.0 [00:00&lt;00:00, 4.47kB/s]"
      }
     },
     "fc74398114cd450e896a62c4a9a8c723": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd9ac64136b14ce8ad1ca035536ece8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cb8943fc75194bad8d71a2f4263dbdac",
        "IPY_MODEL_b9d9a19df93443d6aa26c8f030d5a9f3",
        "IPY_MODEL_a6ee944603ae404b861e7492529369a4"
       ],
       "layout": "IPY_MODEL_90aa3edb75974a08b8ce8d33c8b28a15",
       "tabbable": null,
       "tooltip": null
      }
     },
     "feae82167567425fbec91e30cff92c55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
