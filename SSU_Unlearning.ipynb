{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Sequential Unlearning (SSU) Framework\n",
    "\n",
    "Complete implementation following the paper methodology:\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Initial Fine-tuning (Step 0)**: Fine-tune vanilla model on all copyrighted books (D_f) to make it memorize them\n",
    "2. **Sequential Unlearning (Steps 1-N)**: Unlearn books one at a time using SSU methodology\n",
    "   - Each step unlearns one book (D_f^t)\n",
    "   - Uses composite loss (L_fgt + L_rnd) and weight saliency\n",
    "   - Applies task vector negation\n",
    "\n",
    "## Datasets:\n",
    "- **D_f**: All copyrighted books (10 books from Project Gutenberg)\n",
    "- **D_f^t**: Book to unlearn at time step t\n",
    "- **D_prev**: Previously unlearned books (aggregated from previous steps)\n",
    "- **D_nor**: Retention data (200 chunks from 100 other books) - for evaluation\n",
    "\n",
    "Works on both local and Kaggle environments with automatic retry logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T06:24:28.431647Z",
     "iopub.status.busy": "2025-11-24T06:24:28.430969Z",
     "iopub.status.idle": "2025-11-24T06:24:32.341933Z",
     "shell.execute_reply": "2025-11-24T06:24:32.341052Z",
     "shell.execute_reply.started": "2025-11-24T06:24:28.431620Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/DL/lib/python3.10/codeop.py:118\u001b[0m, in \u001b[0;36mCompile.__call__\u001b[0;34m(self, source, filename, symbol)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source, filename, symbol):\n\u001b[0;32m--> 118\u001b[0m     codeob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m _features:\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m codeob\u001b[38;5;241m.\u001b[39mco_flags \u001b[38;5;241m&\u001b[39m feature\u001b[38;5;241m.\u001b[39mcompiler_flag:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q torch transformers peft datasets accelerate requests protobuf==3.20.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T06:24:32.343921Z",
     "iopub.status.busy": "2025-11-24T06:24:32.343659Z",
     "iopub.status.idle": "2025-11-24T06:24:32.353743Z",
     "shell.execute_reply": "2025-11-24T06:24:32.352982Z",
     "shell.execute_reply.started": "2025-11-24T06:24:32.343897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded!\n"
     ]
    }
   ],
   "source": [
    "# Configuration Class\n",
    "class Config:\n",
    "    # Model Configuration - Use smaller model to avoid download issues\n",
    "    MODEL_NAME = \"google/gemma-3-1b-it\"\n",
    "    \n",
    "    # Alternative options:\n",
    "    # MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # 3.8B, non-gated\n",
    "    # MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small, fast, non-gated\n",
    "    \n",
    "    TOKENIZER_NAME = MODEL_NAME\n",
    "    \n",
    "    # HuggingFace Authentication\n",
    "    USE_HF_TOKEN = True  # Set True for gated models\n",
    "    \n",
    "    # PEFT/LoRA Configuration\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "\n",
    "    # Sequential Unlearning Configuration\n",
    "    NUM_UNLEARNING_STEPS = 3  # Number of sequential unlearning steps\n",
    "    \n",
    "    # Fine-Tuning Hyperparameters\n",
    "    BATCH_SIZE = 1  # Reduced further to avoid OOM\n",
    "    GRADIENT_ACCUMULATION_STEPS = 16\n",
    "    LEARNING_RATE = 5e-5\n",
    "    NUM_EPOCHS_FT = 1  # 1 epoch for initial fine-tuning (as per paper)\n",
    "    NUM_EPOCHS_INITIAL_FT = 1  # Initial fine-tuning on all books (D_f)\n",
    "    \n",
    "    # SSU Methodology Parameters\n",
    "    EPSILON_1 = 1.0  # Weight for Forgetting Loss (L_fgt)\n",
    "    EPSILON_2 = 0.1  # Weight for Random Labeling Loss (L_rnd)\n",
    "    GAMMA = 1e-4  # Saliency threshold\n",
    "    \n",
    "    # Data Configuration\n",
    "    CHUNK_SIZE = 256\n",
    "    NUM_CHUNKS_PER_STEP = 50\n",
    "    USE_REAL_BOOKS = True  # Use real books from Project Gutenberg\n",
    "    DATA_DIR = \"gutenberg_books\"\n",
    "    \n",
    "    # Project Gutenberg Book IDs - All books for initial fine-tuning (D_f)\n",
    "    # Paper uses 10 books total, with specific ones at certain time steps\n",
    "    ALL_BOOK_IDS = [\n",
    "        1661,   # Sherlock Holmes (used at step 1 in paper)\n",
    "        84,     # Frankenstein\n",
    "        1342,   # Pride and Prejudice (used at step 5 in paper)\n",
    "        11,     # Alice in Wonderland (used at step 8 in paper)\n",
    "        2701,   # Moby Dick\n",
    "        74,     # The Adventures of Tom Sawyer\n",
    "        98,     # A Tale of Two Cities\n",
    "        5200,   # Metamorphosis\n",
    "        6130,   # The Iliad\n",
    "        174,    # The Picture of Dorian Gray\n",
    "    ]\n",
    "    \n",
    "    # Books to unlearn at each time step (sequential)\n",
    "    GUTENBERG_BOOK_IDS = {\n",
    "        1: [1661],  # Sherlock Holmes - Step 1\n",
    "        2: [1342],  # Pride and Prejudice - Step 2\n",
    "        3: [11],    # Alice in Wonderland - Step 3\n",
    "        # Add more steps as needed\n",
    "    }\n",
    "    \n",
    "    # Retention data (D_nor) - 200 chunks from 100 other books\n",
    "    USE_RETENTION_DATA = True  # Include D_nor for retention testing\n",
    "    NUM_RETENTION_BOOKS = 10  # Reduced for demo (paper uses 100)\n",
    "    NUM_RETENTION_CHUNKS = 20  # Reduced for demo (paper uses 200)\n",
    "    \n",
    "    OUTPUT_DIR = \"ssu_unlearned_models\"\n",
    "\n",
    "print(\"Configuration loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment Detection & Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T06:24:32.354977Z",
     "iopub.status.busy": "2025-11-24T06:24:32.354692Z",
     "iopub.status.idle": "2025-11-24T06:24:32.524980Z",
     "shell.execute_reply": "2025-11-24T06:24:32.524118Z",
     "shell.execute_reply.started": "2025-11-24T06:24:32.354953Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: Kaggle\n",
      "Successfully logged in to HuggingFace.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect if running on Kaggle\n",
    "IS_KAGGLE = os.path.exists('/kaggle')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running on: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "\n",
    "# HuggingFace Authentication (if needed)\n",
    "if Config.USE_HF_TOKEN:\n",
    "    from huggingface_hub import login\n",
    "    \n",
    "    hf_token = None\n",
    "    \n",
    "    # Try Kaggle Secrets\n",
    "    if IS_KAGGLE:\n",
    "        try:\n",
    "            from kaggle_secrets import UserSecretsClient\n",
    "            user_secrets = UserSecretsClient()\n",
    "            hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "            print(\"Found HuggingFace token in Kaggle Secrets.\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Try environment variable\n",
    "    if not hf_token:\n",
    "        hf_token = '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'\n",
    "    \n",
    "    if hf_token:\n",
    "        try:\n",
    "            login(token=hf_token, add_to_git_credential=False)\n",
    "            print(\"Successfully logged in to HuggingFace.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not login: {e}\")\n",
    "    else:\n",
    "        print(\"WARNING: No HuggingFace token found. Gated models will fail.\")\n",
    "else:\n",
    "    print(\"Using non-gated model - no authentication needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SSU Model & Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T06:24:32.527498Z",
     "iopub.status.busy": "2025-11-24T06:24:32.527237Z",
     "iopub.status.idle": "2025-11-24T06:24:32.571644Z",
     "shell.execute_reply": "2025-11-24T06:24:32.570972Z",
     "shell.execute_reply.started": "2025-11-24T06:24:32.527479Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data utilities loaded!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import requests\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Dummy book text for simulation\n",
    "DUMMY_BOOK_TEXT = \"\"\"\n",
    "In the beginning God created the heavens and the earth. Now the earth was formless and empty, darkness was over the surface of the deep, and the Spirit of God was hovering over the waters. And God said, \"Let there be light,\" and there was light. God saw that the light was good, and he separated the light from the darkness. God called the light \"day,\" and the darkness he called \"night.\" And there was evening, and there was morning\u2014the first day. \n",
    "\n",
    "And God said, \"Let there be a vault between the waters to separate water from water.\" So God made the vault and separated the water under the vault from the water above it. And it was so. God called the vault \"sky.\" And there was evening, and there was morning\u2014the second day.\n",
    "\n",
    "And God said, \"Let the water under the sky be gathered to one place, and let dry ground appear.\" And it was so. God called the dry ground \"land,\" and the gathered waters he called \"seas.\" And God saw that it was good. Then God said, \"Let the land produce vegetation: seed-bearing plants and trees on the land that bear fruit with seed in it, according to their various kinds.\" And it was so. The land produced vegetation: plants bearing seed according to their kinds and trees bearing fruit with seed in it according to their kinds. And God saw that it was good. And there was evening, and there was morning\u2014the third day.\n",
    "\n",
    "And God said, \"Let there be lights in the vault of the sky to separate the day from the night, and let them serve as signs to mark sacred times, and days and years, and let them be lights in the vault of the sky to give light on the earth.\" And it was so. God made two great lights\u2014the greater light to govern the day and the lesser light to govern the night. He also made the stars. God set them in the vault of the sky to give light on the earth, to govern the day and the night, and to separate light from darkness. And God saw that it was good. And there was evening, and there was morning\u2014the fourth day.\n",
    "\n",
    "And God said, \"Let the water teem with living creatures, and let birds fly above the earth across the vault of the sky.\" So God created the great creatures of the sea and every living thing with which the water teems and that moves about in it, according to their kinds, and every winged bird according to its kind. And God saw that it was good. God blessed them and said, \"Be fruitful and increase in number and fill the water in the seas, and let the birds increase on the earth.\" And there was evening, and there was morning\u2014the fifth day.\n",
    "\n",
    "And God said, \"Let the land produce living creatures according to their kinds: the livestock, the creatures that move along the ground, and the wild animals, each according to its kind.\" And it was so. God made the wild animals according to their kinds, the livestock according to their kinds, and all the creatures that move along the ground according to their kinds. And God saw that it was good. Then God said, \"Let us make mankind in our image, in our likeness, so that they may rule over the fish in the sea and the birds in the sky, over the livestock and all the wild animals, and over all the creatures that move along the ground.\" So God created mankind in his own image, in the image of God he created them; male and female he created them. God blessed them and said to them, \"Be fruitful and increase in number; fill the earth and subdue it. Rule over the fish in the sea and the birds in the sky and over every living creature that moves on the ground.\" Then God said, \"I give you every seed-bearing plant on the face of the whole earth and every tree that has fruit with seed in it. They will be yours for food. And to all the beasts of the earth and all the birds in the sky and all the creatures that move along the ground\u2014everything that has the breath of life in it\u2014I give every green plant for food.\" And it was so. God saw all that he had made, and it was very good. And there was evening, and there was morning\u2014the sixth day.\n",
    "\n",
    "Thus the heavens and the earth were completed in all their vast array. By the seventh day God had finished the work he had been doing; so on the seventh day he rested from all his work. Then God blessed the seventh day and made it holy, because on it he rested from all the work of creating that he had done.\n",
    "\"\"\" * 50\n",
    "\n",
    "\n",
    "def generate_simulated_data(text, chunk_size, num_chunks, tokenizer_name):\n",
    "    \"\"\"Simulates a list of text chunks for one book (D_f^t).\n",
    "    \n",
    "    This function splits long book text into smaller chunks that fit within the model's\n",
    "    maximum sequence length. It does this by:\n",
    "    1. Splitting text into small word chunks (to avoid tokenization warnings)\n",
    "    2. Tokenizing each small chunk separately\n",
    "    3. Combining tokenized chunks to create final chunks of the desired size\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Get model max length - this is the MAXIMUM tokens the model can handle\n",
    "    # TinyLlama has max_length of 2048 tokens\n",
    "    max_length = getattr(tokenizer, 'model_max_length', 32000)\n",
    "\n",
    "    # Fix for huge model_max_length causing OverflowError\n",
    "    if max_length > 100000:\n",
    "        max_length = 8192  # Set a reasonable limit\n",
    "    \n",
    "    # Use a safe chunk size that's smaller than model max length\n",
    "    # We use chunk_size from config (256) but ensure it's under the model limit\n",
    "    safe_chunk_size = min(chunk_size, max_length - 10)  # Leave some margin for safety\n",
    "    \n",
    "    # IMPORTANT: We need to split text into VERY small pieces before tokenizing\n",
    "    # Why? Because 1 word can become 1-3 tokens, and we want to stay under max_length\n",
    "    # Strategy: Tokenize in small batches (500-800 words max) to avoid warnings\n",
    "    words = text.split()\n",
    "    \n",
    "    # Conservative estimate: ~1.5 tokens per word on average\n",
    "    # So for max_length=2048, we want max ~1300 words per tokenization batch\n",
    "    # But to be extra safe, we'll use even smaller: 500 words per batch\n",
    "    words_per_batch = min(500, max_length // 3)  # Very conservative: 500 words max per batch\n",
    "    \n",
    "    # Step 1: Tokenize text in small batches to avoid warnings\n",
    "    all_token_ids = []\n",
    "    for i in range(0, len(words), words_per_batch):\n",
    "        batch_text = ' '.join(words[i:i + words_per_batch])\n",
    "        # Tokenize with truncation - this ensures we never exceed max_length\n",
    "        tokenized = tokenizer(\n",
    "            batch_text, \n",
    "            return_tensors='pt', \n",
    "            truncation=True,  # This truncates if too long\n",
    "            max_length=max_length,  # Use model's actual max length\n",
    "            add_special_tokens=True\n",
    "        )['input_ids'][0]\n",
    "        all_token_ids.extend(tokenized.tolist())\n",
    "    \n",
    "    # Step 2: If we don't have enough tokens, repeat the sequence\n",
    "    if len(all_token_ids) < safe_chunk_size * num_chunks:\n",
    "        repeat_factor = (safe_chunk_size * num_chunks // len(all_token_ids)) + 1\n",
    "        all_token_ids = (all_token_ids * repeat_factor)[:safe_chunk_size * num_chunks * 2]\n",
    "    \n",
    "    # Step 3: Split into chunks of the desired size\n",
    "    chunks = []\n",
    "    for i in range(0, len(all_token_ids) - safe_chunk_size + 1, safe_chunk_size):\n",
    "        chunk = all_token_ids[i:i + safe_chunk_size]\n",
    "        if len(chunk) == safe_chunk_size:\n",
    "            chunks.append(chunk)\n",
    "        if len(chunks) >= num_chunks:\n",
    "            break\n",
    "    \n",
    "    # Step 4: If we still don't have enough, pad the last chunk\n",
    "    while len(chunks) < num_chunks:\n",
    "        if chunks:\n",
    "            # Repeat last chunk or pad\n",
    "            last_chunk = chunks[-1][:safe_chunk_size]\n",
    "            if len(last_chunk) < safe_chunk_size:\n",
    "                last_chunk = last_chunk + all_token_ids[:safe_chunk_size - len(last_chunk)]\n",
    "            chunks.append(last_chunk[:safe_chunk_size])\n",
    "        else:\n",
    "            # If no chunks at all, create a dummy chunk\n",
    "            chunks.append(all_token_ids[:safe_chunk_size])\n",
    "    \n",
    "    chunks = chunks[:num_chunks]\n",
    "    \n",
    "    # Step 5: Decode back to text (this is what the dataset will use)\n",
    "    text_chunks = [tokenizer.decode(c, skip_special_tokens=True) for c in chunks]\n",
    "    return text_chunks\n",
    "\n",
    "\n",
    "class SequentialUnlearningDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for SSU loss with dual labels.\"\"\"\n",
    "    def __init__(self, tokenizer, data_texts):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_texts = data_texts\n",
    "        \n",
    "        tokenized = tokenizer(\n",
    "            data_texts, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=Config.CHUNK_SIZE, \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.input_ids = tokenized['input_ids']\n",
    "        self.attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "        self.random_indices = list(range(len(data_texts)))\n",
    "        random.shuffle(self.random_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.input_ids[idx].clone()\n",
    "        attention_mask = self.attention_mask[idx].clone()\n",
    "        labels_fgt = input_ids.clone()\n",
    "        rnd_idx = self.random_indices[idx]\n",
    "        labels_rnd = self.input_ids[rnd_idx].clone()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels_fgt': labels_fgt,\n",
    "            'labels_rnd': labels_rnd\n",
    "        }\n",
    "\n",
    "\n",
    "class SSUDataCollator:\n",
    "    \"\"\"Custom data collator that preserves labels_fgt and labels_rnd for SSU training.\"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        \"\"\"Collate batch while preserving custom labels.\"\"\"\n",
    "        import torch\n",
    "        from torch.nn.utils.rnn import pad_sequence\n",
    "        \n",
    "        if not features:\n",
    "            raise ValueError(\"Empty features list passed to data collator\")\n",
    "        \n",
    "        # Debug: Check what keys are in the first feature\n",
    "        first_feature_keys = list(features[0].keys())\n",
    "        \n",
    "        # Check if features have the custom labels (for SSU training)\n",
    "        has_custom_labels = 'labels_fgt' in features[0] and 'labels_rnd' in features[0]\n",
    "        \n",
    "        if not has_custom_labels:\n",
    "            # Debug information\n",
    "            print(f\"Warning: Custom labels not found in features. Available keys: {first_feature_keys}\")\n",
    "            print(\"Creating labels from input_ids as fallback...\")\n",
    "        \n",
    "        # Extract custom labels if they exist\n",
    "        if has_custom_labels:\n",
    "            labels_fgt_list = [f.pop('labels_fgt') for f in features]\n",
    "            labels_rnd_list = [f.pop('labels_rnd') for f in features]\n",
    "        else:\n",
    "            # If no custom labels, create them from input_ids (fallback)\n",
    "            # This should not happen with SequentialUnlearningDataset, but handle it gracefully\n",
    "            labels_fgt_list = []\n",
    "            labels_rnd_list = []\n",
    "            for f in features:\n",
    "                input_ids = f['input_ids']\n",
    "                if isinstance(input_ids, torch.Tensor):\n",
    "                    labels_fgt_list.append(input_ids.clone())\n",
    "                    labels_rnd_list.append(input_ids.clone())\n",
    "                else:\n",
    "                    labels_fgt_list.append(torch.tensor(input_ids).clone())\n",
    "                    labels_rnd_list.append(torch.tensor(input_ids).clone())\n",
    "        \n",
    "        batch = {}\n",
    "        \n",
    "        # Helper to convert to tensor if needed\n",
    "        def to_tensor(x):\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                return x\n",
    "            return torch.tensor(x)\n",
    "        \n",
    "        # Collate input_ids (already tensors from dataset)\n",
    "        input_ids = [to_tensor(f['input_ids']) for f in features]\n",
    "        batch['input_ids'] = pad_sequence(\n",
    "            input_ids, \n",
    "            batch_first=True, \n",
    "            padding_value=self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        # Collate attention_mask\n",
    "        attention_mask = [to_tensor(f['attention_mask']) for f in features]\n",
    "        batch['attention_mask'] = pad_sequence(\n",
    "            attention_mask,\n",
    "            batch_first=True,\n",
    "            padding_value=0\n",
    "        )\n",
    "        \n",
    "        # Add back the custom labels (already tensors from dataset)\n",
    "        batch['labels_fgt'] = pad_sequence(\n",
    "            [to_tensor(l) for l in labels_fgt_list],\n",
    "            batch_first=True,\n",
    "            padding_value=-100  # -100 is ignored in loss computation\n",
    "        )\n",
    "        \n",
    "        batch['labels_rnd'] = pad_sequence(\n",
    "            [to_tensor(l) for l in labels_rnd_list],\n",
    "            batch_first=True,\n",
    "            padding_value=-100\n",
    "        )\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "def download_gutenberg_book(book_id, output_dir):\n",
    "    \"\"\"Download a book from Project Gutenberg by ID.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    book_file = os.path.join(output_dir, f\"{book_id}.txt\")\n",
    "    \n",
    "    if os.path.exists(book_file):\n",
    "        print(f\"Book {book_id} already exists, skipping download.\")\n",
    "        return book_file\n",
    "    \n",
    "    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
    "    try:\n",
    "        print(f\"Downloading book {book_id} from Project Gutenberg...\")\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        with open(book_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(response.text)\n",
    "        print(f\"Downloaded book {book_id} successfully.\")\n",
    "        return book_file\n",
    "    except:\n",
    "        url_alt = f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "        try:\n",
    "            response = requests.get(url_alt, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            with open(book_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(response.text)\n",
    "            print(f\"Downloaded book {book_id} successfully.\")\n",
    "            return book_file\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not download book {book_id}. Using dummy text.\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def load_book_text(book_file):\n",
    "    \"\"\"Load and clean text from a book file.\"\"\"\n",
    "    if not book_file or not os.path.exists(book_file):\n",
    "        return None\n",
    "    with open(book_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "    # Remove Project Gutenberg headers/footers\n",
    "    start_markers = [\"*** START OF\", \"***START OF\", \"START OF THE PROJECT\"]\n",
    "    end_markers = [\"*** END OF\", \"***END OF\", \"END OF THE PROJECT\"]\n",
    "    for marker in start_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            text = text[text.find('\\n', idx) + 1:]\n",
    "            break\n",
    "    for marker in end_markers:\n",
    "        idx = text.find(marker)\n",
    "        if idx != -1:\n",
    "            text = text[:idx]\n",
    "            break\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_all_books_for_initial_finetuning():\n",
    "    \"\"\"Downloads all books for initial fine-tuning (D_f) - makes model memorize them.\"\"\"\n",
    "    print(\"\\n=== Downloading all books for initial fine-tuning (D_f) ===\")\n",
    "    all_books_dir = os.path.join(Config.DATA_DIR, \"all_books\")\n",
    "    os.makedirs(all_books_dir, exist_ok=True)\n",
    "    \n",
    "    book_texts = []\n",
    "    for book_id in Config.ALL_BOOK_IDS:\n",
    "        book_file = download_gutenberg_book(book_id, all_books_dir)\n",
    "        if book_file:\n",
    "            text = load_book_text(book_file)\n",
    "            if text and len(text) > 1000:\n",
    "                book_texts.append(text)\n",
    "                print(f\"Loaded book {book_id} ({len(text)} chars)\")\n",
    "    \n",
    "    if not book_texts:\n",
    "        print(\"Warning: No books downloaded. Using dummy text.\")\n",
    "        book_texts = [DUMMY_BOOK_TEXT]\n",
    "    \n",
    "    return book_texts\n",
    "\n",
    "\n",
    "def get_unlearning_datasets():\n",
    "    \"\"\"Generates sequential datasets D_f^1, D_f^2, ... for each time step.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    datasets = []\n",
    "    \n",
    "    for t in range(Config.NUM_UNLEARNING_STEPS):\n",
    "        print(f\"\\n--- Preparing dataset D_f^{t+1} for time step {t+1} ---\")\n",
    "        \n",
    "        if Config.USE_REAL_BOOKS:\n",
    "            # Download books for this specific time step\n",
    "            time_step_dir = os.path.join(Config.DATA_DIR, f\"time_step_{t+1}\")\n",
    "            os.makedirs(time_step_dir, exist_ok=True)\n",
    "            \n",
    "            book_ids = Config.GUTENBERG_BOOK_IDS.get(t + 1, [])\n",
    "            book_texts = []\n",
    "            for book_id in book_ids:\n",
    "                book_file = download_gutenberg_book(book_id, time_step_dir)\n",
    "                if book_file:\n",
    "                    text = load_book_text(book_file)\n",
    "                    if text and len(text) > 1000:\n",
    "                        book_texts.append(text)\n",
    "                        print(f\"Loaded book {book_id} for step {t+1}\")\n",
    "            if not book_texts:\n",
    "                print(f\"Warning: No valid books for step {t+1}. Using dummy text.\")\n",
    "                book_texts = [DUMMY_BOOK_TEXT]\n",
    "        else:\n",
    "            book_texts = [DUMMY_BOOK_TEXT]\n",
    "        \n",
    "        all_chunks = []\n",
    "        for book_text in book_texts:\n",
    "            chunks = generate_simulated_data(\n",
    "                book_text,\n",
    "                Config.CHUNK_SIZE,\n",
    "                Config.NUM_CHUNKS_PER_STEP // len(book_texts) + 1,\n",
    "                Config.TOKENIZER_NAME\n",
    "            )\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        data_t = all_chunks[:Config.NUM_CHUNKS_PER_STEP]\n",
    "        print(f\"Created {len(data_t)} chunks for time step {t+1}\")\n",
    "        \n",
    "        dataset_t = SequentialUnlearningDataset(tokenizer, data_t)\n",
    "        datasets.append(dataset_t)\n",
    "        \n",
    "    return datasets\n",
    "\n",
    "\n",
    "def get_retention_dataset():\n",
    "    \"\"\"Generates retention dataset D_nor (non-targeted data to keep).\"\"\"\n",
    "    if not Config.USE_RETENTION_DATA:\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n=== Preparing retention dataset D_nor ===\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Use some books that are NOT in the unlearning set\n",
    "    retention_book_ids = [1232, 145, 76, 2591, 30254, 844, 345, 520, 6130, 174][:Config.NUM_RETENTION_BOOKS]\n",
    "    retention_dir = os.path.join(Config.DATA_DIR, \"retention_books\")\n",
    "    os.makedirs(retention_dir, exist_ok=True)\n",
    "    \n",
    "    all_chunks = []\n",
    "    for book_id in retention_book_ids:\n",
    "        if book_id not in Config.ALL_BOOK_IDS:  # Don't use books from D_f\n",
    "            book_file = download_gutenberg_book(book_id, retention_dir)\n",
    "            if book_file:\n",
    "                text = load_book_text(book_file)\n",
    "                if text and len(text) > 1000:\n",
    "                    chunks = generate_simulated_data(\n",
    "                        text,\n",
    "                        Config.CHUNK_SIZE,\n",
    "                        Config.NUM_RETENTION_CHUNKS // Config.NUM_RETENTION_BOOKS + 1,\n",
    "                        Config.TOKENIZER_NAME\n",
    "                    )\n",
    "                    all_chunks.extend(chunks)\n",
    "    \n",
    "    if not all_chunks:\n",
    "        print(\"Warning: No retention books downloaded. Using dummy text.\")\n",
    "        all_chunks = generate_simulated_data(\n",
    "            DUMMY_BOOK_TEXT,\n",
    "            Config.CHUNK_SIZE,\n",
    "            Config.NUM_RETENTION_CHUNKS,\n",
    "            Config.TOKENIZER_NAME\n",
    "        )\n",
    "    \n",
    "    retention_chunks = all_chunks[:Config.NUM_RETENTION_CHUNKS]\n",
    "    print(f\"Created {len(retention_chunks)} retention chunks\")\n",
    "    \n",
    "    return SequentialUnlearningDataset(tokenizer, retention_chunks)\n",
    "\n",
    "print(\"Data utilities loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T06:24:32.572783Z",
     "iopub.status.busy": "2025-11-24T06:24:32.572499Z",
     "iopub.status.idle": "2025-11-24T06:24:32.596140Z",
     "shell.execute_reply": "2025-11-24T06:24:32.595482Z",
     "shell.execute_reply.started": "2025-11-24T06:24:32.572764Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSU model utilities loaded!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "\n",
    "class SSUTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer implementing SSU loss and Weight Saliency.\"\"\"\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Compute SSU loss (L_fgt + L_rnd) with support for additional kwargs.\n",
    "        \n",
    "        This method accepts any additional keyword arguments that newer versions\n",
    "        of transformers might pass (like num_items_in_batch).\n",
    "        \"\"\"\n",
    "        # Make a copy of inputs to avoid modifying the original dict\n",
    "        inputs_copy = inputs.copy()\n",
    "        \n",
    "        # Extract our custom labels (these are added by SequentialUnlearningDataset)\n",
    "        labels_fgt = inputs_copy.pop('labels_fgt', None)\n",
    "        labels_rnd = inputs_copy.pop('labels_rnd', None)\n",
    "        \n",
    "        if labels_fgt is None or labels_rnd is None:\n",
    "            raise ValueError(\"Missing labels_fgt or labels_rnd in inputs. Check dataset.\")\n",
    "        \n",
    "        # L_fgt (Forgetting Loss)\n",
    "        outputs_fgt = model(**inputs_copy, labels=labels_fgt)\n",
    "        loss_fgt = outputs_fgt.loss \n",
    "\n",
    "        # L_rnd (Random Labeling Loss)\n",
    "        outputs_rnd = model(**inputs_copy, labels=labels_rnd)\n",
    "        loss_rnd = outputs_rnd.loss\n",
    "        \n",
    "        # Combined SSU Loss\n",
    "        loss = Config.EPSILON_1 * loss_fgt + Config.EPSILON_2 * loss_rnd\n",
    "        \n",
    "        return (loss, outputs_fgt) if return_outputs else loss\n",
    "\n",
    "    def optimizer_step(self):\n",
    "        \"\"\"Override optimizer_step to apply weight saliency masking.\"\"\"\n",
    "        # Apply weight saliency mask before optimizer step\n",
    "        if self.accelerator.sync_gradients:\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if param.grad is not None and param.requires_grad:\n",
    "                    if \"lora\" in name.lower():\n",
    "                        grad = param.grad.data\n",
    "                        \n",
    "                        # Saliency Mask: m_s = I(|grad| >= gamma)\n",
    "                        m_s = (grad.abs() >= Config.GAMMA).float()\n",
    "                        \n",
    "                        # Apply mask to gradients (only update parameters with high saliency)\n",
    "                        param.grad.data = grad * m_s\n",
    "        \n",
    "        # Call parent optimizer_step to perform the actual update\n",
    "        super().optimizer_step()\n",
    "\n",
    "\n",
    "def create_lora_model(model):\n",
    "    \"\"\"Adds LoRA adapters to the base model.\"\"\"\n",
    "    peft_config = LoraConfig(\n",
    "        r=Config.LORA_R,\n",
    "        lora_alpha=Config.LORA_ALPHA,\n",
    "        lora_dropout=Config.LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=Config.TARGET_MODULES,\n",
    "    )\n",
    "    return get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "def apply_task_vector_negation(base_model, fine_tuned_model, name_prefix):\n",
    "    \"\"\"Task Vector Negation: theta_u^t = 2 * theta_u^{t-1} - theta_ft^t\"\"\"\n",
    "    print(f\"\\n--- Applying Task Vector Negation for {name_prefix} ---\")\n",
    "    \n",
    "    device = next(base_model.parameters()).device\n",
    "    new_unlearned_model = base_model.__class__(config=base_model.config).to(device)\n",
    "    \n",
    "    base_state = base_model.state_dict()\n",
    "    ft_state = fine_tuned_model.state_dict()\n",
    "    \n",
    "    new_state = {}\n",
    "    for name, param in new_unlearned_model.named_parameters():\n",
    "        if name in base_state and name in ft_state:\n",
    "            new_state[name] = 2 * base_state[name] - ft_state[name]\n",
    "        else:\n",
    "            new_state[name] = base_state.get(name, param.data)\n",
    "    \n",
    "    new_unlearned_model.load_state_dict(new_state, strict=False)\n",
    "    print(f\"Task Vector Negation complete for {name_prefix}.\")\n",
    "    return new_unlearned_model\n",
    "\n",
    "print(\"SSU model utilities loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Loading with Retry Logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T06:24:32.597265Z",
     "iopub.status.busy": "2025-11-24T06:24:32.596964Z",
     "iopub.status.idle": "2025-11-24T06:24:35.847613Z",
     "shell.execute_reply": "2025-11-24T06:24:35.846890Z",
     "shell.execute_reply.started": "2025-11-24T06:24:32.597241Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: google/gemma-3-1b-it\n",
      "Loading model (attempt 1/3)...\n",
      "Successfully loaded google/gemma-3-1b-it!\n",
      "Model loaded and frozen!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "\n",
    "def load_model_with_retry(model_name, max_retries=3, retry_delay=5):\n",
    "    \"\"\"Load model with automatic retry on network errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Loading model (attempt {attempt + 1}/{max_retries})...\")\n",
    "            \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            # Determine device\n",
    "            if torch.cuda.is_available():\n",
    "                device = \"cuda\"\n",
    "                dtype = torch.bfloat16\n",
    "            else:\n",
    "                device = \"cpu\"\n",
    "                dtype = torch.float32\n",
    "            \n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                torch_dtype=dtype,\n",
    "                device_map=device,  # Use single device instead of \"auto\"\n",
    "                trust_remote_code=True,\n",
    "                attn_implementation=\"eager\",\n",
    "            )\n",
    "            \n",
    "            # Ensure model is on the correct device\n",
    "            base_model = base_model.to(device)\n",
    "            \n",
    "            print(f\"Successfully loaded {model_name}!\")\n",
    "            return base_model, tokenizer\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"Attempt {attempt + 1} failed: {error_msg[:200]}...\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                if \"IncompleteRead\" in error_msg or \"Connection\" in error_msg or \"timeout\" in error_msg.lower():\n",
    "                    print(f\"Network error detected. Retrying in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(\"Non-network error. Retrying...\")\n",
    "                    time.sleep(2)\n",
    "            else:\n",
    "                print(\"\\nAll retry attempts failed!\")\n",
    "                print(\"\\nTROUBLESHOOTING:\")\n",
    "                print(\"1. Check your internet connection\")\n",
    "                print(\"2. Try a smaller model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "                print(\"3. For gated models, ensure HF_TOKEN is set\")\n",
    "                raise\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "\n",
    "# Load the model\n",
    "print(f\"Loading base model: {Config.MODEL_NAME}\")\n",
    "base_model, tokenizer = load_model_with_retry(Config.MODEL_NAME)\n",
    "base_model.requires_grad_(False)\n",
    "print(\"Model loaded and frozen!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main SSU Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T06:24:35.849006Z",
     "iopub.status.busy": "2025-11-24T06:24:35.848711Z",
     "iopub.status.idle": "2025-11-24T06:24:36.947965Z",
     "shell.execute_reply": "2025-11-24T06:24:36.947031Z",
     "shell.execute_reply.started": "2025-11-24T06:24:35.848980Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memorized model already exists at ssu_unlearned_models/memorized_model\n",
      "Loading existing memorized model...\n",
      "Loaded existing memorized model.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "def initial_finetuning(model, tokenizer, all_books_texts):\n",
    "    \"\"\"\n",
    "    Step 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n",
    "    This is what the paper does BEFORE unlearning.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Fine-tuning vanilla model on all copyrighted books to memorize them...\")\n",
    "    \n",
    "    # Generate chunks from all books\n",
    "    all_chunks = []\n",
    "    for book_text in all_books_texts:\n",
    "        chunks = generate_simulated_data(\n",
    "            book_text,\n",
    "            Config.CHUNK_SIZE,\n",
    "            Config.NUM_CHUNKS_PER_STEP * Config.NUM_UNLEARNING_STEPS // len(all_books_texts) + 1,\n",
    "            Config.TOKENIZER_NAME\n",
    "        )\n",
    "        all_chunks.extend(chunks)\n",
    "    \n",
    "    print(f\"Created {len(all_chunks)} chunks from all books\")\n",
    "    \n",
    "    # For initial fine-tuning, use standard dataset (not SSU dual labels)\n",
    "    from torch.utils.data import Dataset as TorchDataset\n",
    "    class StandardDataset(TorchDataset):\n",
    "        def __init__(self, tokenizer, data_texts):\n",
    "            tokenized = tokenizer(\n",
    "                data_texts,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=Config.CHUNK_SIZE,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.input_ids = tokenized['input_ids']\n",
    "            self.attention_mask = tokenized['attention_mask']\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.input_ids)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            return {\n",
    "                'input_ids': self.input_ids[idx].clone(),\n",
    "                'attention_mask': self.attention_mask[idx].clone(),\n",
    "                'labels': self.input_ids[idx].clone()  # Standard labels for next token prediction\n",
    "            }\n",
    "    \n",
    "    initial_dataset = StandardDataset(tokenizer, all_chunks)\n",
    "    \n",
    "    # Ensure model is on correct device before creating LoRA\n",
    "    device = next(model.parameters()).device\n",
    "    if device.type == \"meta\" or str(device) == \"meta\":\n",
    "        # If model is on meta device, move to actual device\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        # Convert device object to string if needed\n",
    "        device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n",
    "    \n",
    "    # Create LoRA model for initial fine-tuning (PEFT handles device automatically)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    else:\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "    \n",
    "    lora_model = create_lora_model(model)\n",
    "    lora_model.print_trainable_parameters()\n",
    "    \n",
    "    # Disable cache for gradient checkpointing\n",
    "    if hasattr(lora_model.config, \"use_cache\"):\n",
    "        lora_model.config.use_cache = False\n",
    "    \n",
    "    # PEFT models inherit device from base model, no need to call .to()\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"{Config.OUTPUT_DIR}/initial_ft_checkpoints\",\n",
    "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=10,\n",
    "        learning_rate=Config.LEARNING_RATE,\n",
    "        num_train_epochs=Config.NUM_EPOCHS_INITIAL_FT,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        fp16=False,\n",
    "        bf16=torch.cuda.is_available() and device == \"cuda\",\n",
    "        dataloader_pin_memory=False,  # Fix device issues\n",
    "        label_names=[\"labels\"],\n",
    "        gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    # Use standard Trainer for initial fine-tuning (not SSU)\n",
    "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # Causal LM, not masked LM\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=training_args,\n",
    "        train_dataset=initial_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting initial fine-tuning...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Merge LoRA weights into base model\n",
    "    memorized_model = lora_model.merge_and_unload()\n",
    "    memorized_model.requires_grad_(False)\n",
    "    \n",
    "    print(\"Initial fine-tuning complete. Model has memorized all books.\")\n",
    "    return memorized_model\n",
    "\n",
    "\n",
    "def run_initial_finetuning():\n",
    "    \"\"\"STEP 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n",
    "    \n",
    "    Run this cell once to create the memorized model. After this completes,\n",
    "    you can run the sequential unlearning steps without re-running this.\n",
    "    \"\"\"\n",
    "    # Ensure all required dependencies are available\n",
    "    missing = []\n",
    "    try:\n",
    "        _ = Config.OUTPUT_DIR\n",
    "    except NameError:\n",
    "        missing.append(\"Config (cell 4)\")\n",
    "    \n",
    "    try:\n",
    "        _ = base_model\n",
    "    except NameError:\n",
    "        missing.append(\"base_model (cell 11)\")\n",
    "    \n",
    "    try:\n",
    "        _ = tokenizer\n",
    "    except NameError:\n",
    "        missing.append(\"tokenizer (cell 11)\")\n",
    "    \n",
    "    if missing:\n",
    "        raise NameError(\n",
    "            f\"The following are not defined: {', '.join(missing)}. \"\n",
    "            f\"Please run the required cells first to set up the dependencies.\"\n",
    "        )\n",
    "    \n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Check if memorized model already exists\n",
    "    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n",
    "    if os.path.exists(memorized_model_path):\n",
    "        print(f\"Memorized model already exists at {memorized_model_path}\")\n",
    "        print(\"Loading existing memorized model...\")\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        memorized_model = AutoModelForCausalLM.from_pretrained(\n",
    "            memorized_model_path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device\n",
    "        )\n",
    "        memorized_model.requires_grad_(False)\n",
    "        print(\"Loaded existing memorized model.\")\n",
    "        return memorized_model\n",
    "    \n",
    "    # STEP 0: Initial fine-tuning on all books (D_f)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n",
    "    print(\"=\"*60)\n",
    "    all_books_texts = get_all_books_for_initial_finetuning()\n",
    "    memorized_model = initial_finetuning(base_model, tokenizer, all_books_texts)\n",
    "    \n",
    "    # Save the memorized model\n",
    "    memorized_model.save_pretrained(memorized_model_path)\n",
    "    tokenizer.save_pretrained(memorized_model_path)\n",
    "    print(f\"\\nMemorized model saved to {memorized_model_path}\")\n",
    "    \n",
    "    return memorized_model\n",
    "\n",
    "\n",
    "# Run initial fine-tuning (only need to run this once)\n",
    "memorized_model = run_initial_finetuning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T06:24:36.949902Z",
     "iopub.status.busy": "2025-11-24T06:24:36.949576Z",
     "iopub.status.idle": "2025-11-24T06:26:46.567484Z",
     "shell.execute_reply": "2025-11-24T06:26:46.566198Z",
     "shell.execute_reply.started": "2025-11-24T06:24:36.949868Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from memorized model for step 1\n",
      "\n",
      "--- Preparing dataset D_f^1 for time step 1 ---\n",
      "Book 1661 already exists, skipping download.\n",
      "Loaded book 1661 for step 1\n",
      "Created 50 chunks for time step 1\n",
      "\n",
      "--- Preparing dataset D_f^2 for time step 2 ---\n",
      "Book 1342 already exists, skipping download.\n",
      "Loaded book 1342 for step 2\n",
      "Created 50 chunks for time step 2\n",
      "\n",
      "--- Preparing dataset D_f^3 for time step 3 ---\n",
      "Book 11 already exists, skipping download.\n",
      "Loaded book 11 for step 3\n",
      "Created 50 chunks for time step 3\n",
      "\n",
      "Generated 3 sequential unlearning datasets.\n",
      "\n",
      "=== Preparing retention dataset D_nor ===\n",
      "Book 1232 already exists, skipping download.\n",
      "Book 145 already exists, skipping download.\n",
      "Book 76 already exists, skipping download.\n",
      "Book 2591 already exists, skipping download.\n",
      "Book 30254 already exists, skipping download.\n",
      "Book 844 already exists, skipping download.\n",
      "Book 345 already exists, skipping download.\n",
      "Book 520 already exists, skipping download.\n",
      "Created 20 retention chunks\n",
      "\n",
      "============================================================\n",
      "UNLEARNING STEP 1 (Unlearning D_f^1)\n",
      "============================================================\n",
      "Preparing LoRA model for fine-tuning on D_f^1...\n",
      "trainable params: 745,472 || all params: 1,000,631,424 || trainable%: 0.0745\n",
      "Starting fine-tuning with SSU Loss for step_1...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Task Vector Negation for step_1 ---\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 2353 has 14.72 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 68.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/2748667507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;31m# You can specify start_step and end_step to resume from a specific step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;31m# Example: run_sequential_unlearning(start_step=2, end_step=3) to run only steps 2-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m \u001b[0mfinal_unlearned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_sequential_unlearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_48/2748667507.py\u001b[0m in \u001b[0;36mrun_sequential_unlearning\u001b[0;34m(start_step, end_step)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Task Vector Negation Stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mtheta_ft_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlora_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_and_unload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0munlearned_model_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_task_vector_negation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_ft_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0munlearned_model_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/4277313892.py\u001b[0m in \u001b[0;36mapply_task_vector_negation\u001b[0;34m(base_model, fine_tuned_model, name_prefix)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mnew_unlearned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mbase_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4108\u001b[0m                     \u001b[0;34m\" `dtype` by passing the correct `torch_dtype` argument.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4109\u001b[0m                 )\n\u001b[0;32m-> 4110\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 22.12 MiB is free. Process 2353 has 14.72 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 68.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "## 8. Sequential Unlearning Steps\n",
    "\n",
    "def run_sequential_unlearning(start_step=1, end_step=None):\n",
    "    \"\"\"Run sequential unlearning steps.\n",
    "    \n",
    "    Args:\n",
    "        start_step: Starting unlearning step (1-indexed). Default: 1\n",
    "        end_step: Ending unlearning step (inclusive). If None, uses Config.NUM_UNLEARNING_STEPS\n",
    "    \"\"\"\n",
    "    # Ensure all required dependencies are available\n",
    "    missing = []\n",
    "    try:\n",
    "        _ = Config.OUTPUT_DIR\n",
    "    except NameError:\n",
    "        missing.append(\"Config (cell 4)\")\n",
    "    \n",
    "    try:\n",
    "        _ = tokenizer\n",
    "    except NameError:\n",
    "        missing.append(\"tokenizer (cell 11)\")\n",
    "    \n",
    "    if missing:\n",
    "        raise NameError(\n",
    "            f\"The following are not defined: {', '.join(missing)}. \"\n",
    "            f\"Please run the required cells first.\"\n",
    "        )\n",
    "    \n",
    "    if end_step is None:\n",
    "        end_step = Config.NUM_UNLEARNING_STEPS\n",
    "    \n",
    "    # Load or start from memorized model (check if file exists, not variable)\n",
    "    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n",
    "    if not os.path.exists(memorized_model_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Memorized model not found at {memorized_model_path}. \"\n",
    "            f\"Please run cell 13 (initial fine-tuning) first.\"\n",
    "        )\n",
    "    \n",
    "    # Determine starting model\n",
    "    if start_step == 1:\n",
    "        # Load memorized model\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        current_model = AutoModelForCausalLM.from_pretrained(\n",
    "            memorized_model_path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device\n",
    "        )\n",
    "        current_model.requires_grad_(False)\n",
    "        print(f\"Starting from memorized model for step 1\")\n",
    "    else:\n",
    "        # Load model from previous step\n",
    "        prev_step_path = f\"{Config.OUTPUT_DIR}/step_{start_step-1}_unlearned_model\"\n",
    "        if not os.path.exists(prev_step_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Previous step model not found at {prev_step_path}. \"\n",
    "                f\"Please run steps 1 to {start_step-1} first.\"\n",
    "            )\n",
    "        from transformers import AutoModelForCausalLM\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "        current_model = AutoModelForCausalLM.from_pretrained(\n",
    "            prev_step_path,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=device\n",
    "        )\n",
    "        current_model.requires_grad_(False)\n",
    "        print(f\"Starting from step {start_step-1} model for step {start_step}\")\n",
    "    \n",
    "    # Load sequential datasets for unlearning\n",
    "    unlearning_datasets = get_unlearning_datasets()\n",
    "    print(f\"\\nGenerated {len(unlearning_datasets)} sequential unlearning datasets.\")\n",
    "    \n",
    "    # Get retention dataset (D_nor) if needed\n",
    "    retention_dataset = get_retention_dataset()\n",
    "    \n",
    "    # STEP 1-N: Sequential Unlearning Loop\n",
    "    for t in range(start_step - 1, min(end_step, Config.NUM_UNLEARNING_STEPS)):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"UNLEARNING STEP {t+1} (Unlearning D_f^{t+1})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        dataset_t = unlearning_datasets[t]\n",
    "        step_prefix = f\"step_{t+1}\"\n",
    "        \n",
    "        # D_prev: Previously unlearned books (for t > 1)\n",
    "        # In the paper, this ensures model doesn't re-learn old content\n",
    "        if t > 0 and Config.USE_RETENTION_DATA:\n",
    "            print(f\"Note: D_prev includes books from steps 1-{t}\")\n",
    "        \n",
    "        # Fine-Tuning Stage\n",
    "        print(f\"Preparing LoRA model for fine-tuning on D_f^{t+1}...\")\n",
    "        \n",
    "        # Ensure model is on correct device\n",
    "        device = next(current_model.parameters()).device\n",
    "        if device.type == \"meta\" or str(device) == \"meta\":\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            current_model = current_model.to(device)\n",
    "        else:\n",
    "            # Convert device object to string if needed\n",
    "            device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n",
    "        \n",
    "        if hasattr(current_model, \"enable_input_require_grads\"):\n",
    "            current_model.enable_input_require_grads()\n",
    "        else:\n",
    "            def make_inputs_require_grad(module, input, output):\n",
    "                output.requires_grad_(True)\n",
    "            current_model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "        \n",
    "        lora_model = create_lora_model(current_model)\n",
    "        # PEFT models inherit device from base model, no need to call .to()\n",
    "        lora_model.print_trainable_parameters()\n",
    "        \n",
    "        # Disable cache for gradient checkpointing\n",
    "        if hasattr(lora_model.config, \"use_cache\"):\n",
    "            lora_model.config.use_cache = False\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{Config.OUTPUT_DIR}/{step_prefix}_ft_checkpoints\",\n",
    "            per_device_train_batch_size=Config.BATCH_SIZE,\n",
    "            gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
    "            warmup_steps=10,\n",
    "            learning_rate=Config.LEARNING_RATE,\n",
    "            num_train_epochs=Config.NUM_EPOCHS_FT,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"no\",\n",
    "            report_to=\"none\",\n",
    "            fp16=False,\n",
    "            bf16=torch.cuda.is_available() and device == \"cuda\",\n",
    "            dataloader_pin_memory=False,  # Fix device issues\n",
    "            label_names=[\"labels\"],\n",
    "            gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "            remove_unused_columns=False,  # Keep custom labels\n",
    "            use_cache=False, # Disable KV cache for training\n",
    "        )\n",
    "\n",
    "        # Use custom data collator that preserves labels_fgt and labels_rnd\n",
    "        data_collator = SSUDataCollator(tokenizer=tokenizer)\n",
    "        \n",
    "        trainer = SSUTrainer(\n",
    "            model=lora_model,\n",
    "            args=training_args,\n",
    "            train_dataset=dataset_t,\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        print(f\"Starting fine-tuning with SSU Loss for {step_prefix}...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Task Vector Negation Stage: theta_new = theta_old - Delta_LoRA\n",
    "        print(f\"\\n--- Applying Task Vector Negation for {step_prefix} ---\")\n",
    "        \n",
    "        # 1. Negate LoRA weights: W_new = W_old - (W_ft - W_old) = W_old - W_lora\n",
    "        # We achieve this by multiplying LoRA weights by -1, then merging.\n",
    "        with torch.no_grad():\n",
    "            for name, param in lora_model.named_parameters():\n",
    "                if \"lora\" in name:\n",
    "                    param.data = -1 * param.data\n",
    "        \n",
    "        print(\"LoRA weights negated.\")\n",
    "        \n",
    "        # 2. Merge negated weights into base model\n",
    "        current_model = lora_model.merge_and_unload()\n",
    "        current_model.requires_grad_(False)\n",
    "        \n",
    "        print(f\"Task Vector Negation complete for {step_prefix}.\")\n",
    "        \n",
    "        # Save the unlearned model\n",
    "        current_model.save_pretrained(f\"{Config.OUTPUT_DIR}/{step_prefix}_unlearned_model\")\n",
    "        tokenizer.save_pretrained(f\"{Config.OUTPUT_DIR}/{step_prefix}_unlearned_model\")\n",
    "        print(f\"Unlearned model {step_prefix} saved.\")\n",
    "        \n",
    "        current_model = unlearned_model_t\n",
    "\n",
    "    # Final Evaluation\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SEQUENTIAL UNLEARNING COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    final_step = min(end_step, Config.NUM_UNLEARNING_STEPS)\n",
    "    print(f\"Final Unlearned Model: {Config.OUTPUT_DIR}/step_{final_step}_unlearned_model\")\n",
    "    \n",
    "    # Test generation\n",
    "    prompt = \"The quick brown fox\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    device = next(current_model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    print(\"\\nTesting generation with final unlearned model...\")\n",
    "    current_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output_tokens = current_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=30, \n",
    "            do_sample=True, \n",
    "            top_p=0.9, \n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    \n",
    "    return current_model\n",
    "\n",
    "\n",
    "# Run sequential unlearning steps\n",
    "# You can specify start_step and end_step to resume from a specific step\n",
    "# Example: run_sequential_unlearning(start_step=2, end_step=3) to run only steps 2-3\n",
    "final_unlearned_model = run_sequential_unlearning()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}