{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stable Sequential Unlearning (SSU) Framework\n",
        "\n",
        "Complete implementation following the paper methodology:\n",
        "\n",
        "## Pipeline Overview:\n",
        "1. **Initial Fine-tuning (Step 0)**: Fine-tune vanilla model on all copyrighted books (D_f) to make it memorize them\n",
        "2. **Sequential Unlearning (Steps 1-N)**: Unlearn books one at a time using SSU methodology\n",
        "   - Each step unlearns one book (D_f^t)\n",
        "   - Uses composite loss (L_fgt + L_rnd) and weight saliency\n",
        "   - Applies task vector negation\n",
        "\n",
        "## Datasets:\n",
        "- **D_f**: All copyrighted books (10 books from Project Gutenberg)\n",
        "- **D_f^t**: Book to unlearn at time step t\n",
        "- **D_prev**: Previously unlearned books (aggregated from previous steps)\n",
        "- **D_nor**: Retention data (200 chunks from 100 other books) - for evaluation\n",
        "\n",
        "Works on both local and Kaggle environments with automatic retry logic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install -q torch transformers peft datasets accelerate requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded!\n"
          ]
        }
      ],
      "source": [
        "# Configuration Class\n",
        "class Config:\n",
        "    # Model Configuration - Use smaller model to avoid download issues\n",
        "    # Alternative options:\n",
        "    # MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # 3.8B, non-gated\n",
        "    MODEL_NAME = \"google/gemma-3-270m-instruct\"\n",
        "    # MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Small, fast, non-gated\n",
        "    # MODEL_NAME = \"google/gemma-2-2b\"  # Requires auth\n",
        "    TOKENIZER_NAME = MODEL_NAME\n",
        "    \n",
        "    # HuggingFace Authentication\n",
        "    USE_HF_TOKEN = False  # Set True for gated models\n",
        "    \n",
        "    # PEFT/LoRA Configuration\n",
        "    LORA_R = 8\n",
        "    LORA_ALPHA = 16\n",
        "    LORA_DROPOUT = 0.05\n",
        "    TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
        "\n",
        "    # Sequential Unlearning Configuration\n",
        "    NUM_UNLEARNING_STEPS = 3  # Number of sequential unlearning steps\n",
        "    \n",
        "    # Fine-Tuning Hyperparameters\n",
        "    BATCH_SIZE = 2  # Reduced for stability\n",
        "    GRADIENT_ACCUMULATION_STEPS = 8\n",
        "    LEARNING_RATE = 5e-5\n",
        "    NUM_EPOCHS_FT = 1  # 1 epoch for initial fine-tuning (as per paper)\n",
        "    NUM_EPOCHS_INITIAL_FT = 1  # Initial fine-tuning on all books (D_f)\n",
        "    \n",
        "    # SSU Methodology Parameters\n",
        "    EPSILON_1 = 1.0  # Weight for Forgetting Loss (L_fgt)\n",
        "    EPSILON_2 = 0.1  # Weight for Random Labeling Loss (L_rnd)\n",
        "    GAMMA = 1e-4  # Saliency threshold\n",
        "    \n",
        "    # Data Configuration\n",
        "    CHUNK_SIZE = 256\n",
        "    NUM_CHUNKS_PER_STEP = 50\n",
        "    USE_REAL_BOOKS = True  # Use real books from Project Gutenberg\n",
        "    DATA_DIR = \"gutenberg_books\"\n",
        "    \n",
        "    # Project Gutenberg Book IDs - All books for initial fine-tuning (D_f)\n",
        "    # Paper uses 10 books total, with specific ones at certain time steps\n",
        "    ALL_BOOK_IDS = [\n",
        "        1661,   # Sherlock Holmes (used at step 1 in paper)\n",
        "        84,     # Frankenstein\n",
        "        1342,   # Pride and Prejudice (used at step 5 in paper)\n",
        "        11,     # Alice in Wonderland (used at step 8 in paper)\n",
        "        2701,   # Moby Dick\n",
        "        74,     # The Adventures of Tom Sawyer\n",
        "        98,     # A Tale of Two Cities\n",
        "        5200,   # Metamorphosis\n",
        "        6130,   # The Iliad\n",
        "        174,    # The Picture of Dorian Gray\n",
        "    ]\n",
        "    \n",
        "    # Books to unlearn at each time step (sequential)\n",
        "    GUTENBERG_BOOK_IDS = {\n",
        "        1: [1661],  # Sherlock Holmes - Step 1\n",
        "        2: [1342],  # Pride and Prejudice - Step 2\n",
        "        3: [11],    # Alice in Wonderland - Step 3\n",
        "        # Add more steps as needed\n",
        "    }\n",
        "    \n",
        "    # Retention data (D_nor) - 200 chunks from 100 other books\n",
        "    USE_RETENTION_DATA = True  # Include D_nor for retention testing\n",
        "    NUM_RETENTION_BOOKS = 10  # Reduced for demo (paper uses 100)\n",
        "    NUM_RETENTION_CHUNKS = 20  # Reduced for demo (paper uses 200)\n",
        "    \n",
        "    OUTPUT_DIR = \"ssu_unlearned_models\"\n",
        "\n",
        "print(\"Configuration loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Environment Detection & Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on: Local\n",
            "Using non-gated model - no authentication needed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Detect if running on Kaggle\n",
        "IS_KAGGLE = os.path.exists('/kaggle')\n",
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "print(f\"Running on: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
        "\n",
        "# HuggingFace Authentication (if needed)\n",
        "if Config.USE_HF_TOKEN:\n",
        "    from huggingface_hub import login\n",
        "    \n",
        "    hf_token = None\n",
        "    \n",
        "    # Try Kaggle Secrets\n",
        "    if IS_KAGGLE:\n",
        "        try:\n",
        "            from kaggle_secrets import UserSecretsClient\n",
        "            user_secrets = UserSecretsClient()\n",
        "            hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "            print(\"Found HuggingFace token in Kaggle Secrets.\")\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Try environment variable\n",
        "    if not hf_token:\n",
        "        hf_token = os.environ.get('HF_TOKEN')\n",
        "    \n",
        "    if hf_token:\n",
        "        try:\n",
        "            login(token=hf_token, add_to_git_credential=False)\n",
        "            print(\"Successfully logged in to HuggingFace.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not login: {e}\")\n",
        "    else:\n",
        "        print(\"WARNING: No HuggingFace token found. Gated models will fail.\")\n",
        "else:\n",
        "    print(\"Using non-gated model - no authentication needed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. SSU Model & Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data utilities loaded!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import os\n",
        "import requests\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Dummy book text for simulation\n",
        "DUMMY_BOOK_TEXT = \"\"\"\n",
        "In the beginning God created the heavens and the earth. Now the earth was formless and empty, darkness was over the surface of the deep, and the Spirit of God was hovering over the waters. And God said, \"Let there be light,\" and there was light. God saw that the light was good, and he separated the light from the darkness. God called the light \"day,\" and the darkness he called \"night.\" And there was evening, and there was morning—the first day. \n",
        "\n",
        "And God said, \"Let there be a vault between the waters to separate water from water.\" So God made the vault and separated the water under the vault from the water above it. And it was so. God called the vault \"sky.\" And there was evening, and there was morning—the second day.\n",
        "\n",
        "And God said, \"Let the water under the sky be gathered to one place, and let dry ground appear.\" And it was so. God called the dry ground \"land,\" and the gathered waters he called \"seas.\" And God saw that it was good. Then God said, \"Let the land produce vegetation: seed-bearing plants and trees on the land that bear fruit with seed in it, according to their various kinds.\" And it was so. The land produced vegetation: plants bearing seed according to their kinds and trees bearing fruit with seed in it according to their kinds. And God saw that it was good. And there was evening, and there was morning—the third day.\n",
        "\n",
        "And God said, \"Let there be lights in the vault of the sky to separate the day from the night, and let them serve as signs to mark sacred times, and days and years, and let them be lights in the vault of the sky to give light on the earth.\" And it was so. God made two great lights—the greater light to govern the day and the lesser light to govern the night. He also made the stars. God set them in the vault of the sky to give light on the earth, to govern the day and the night, and to separate light from darkness. And God saw that it was good. And there was evening, and there was morning—the fourth day.\n",
        "\n",
        "And God said, \"Let the water teem with living creatures, and let birds fly above the earth across the vault of the sky.\" So God created the great creatures of the sea and every living thing with which the water teems and that moves about in it, according to their kinds, and every winged bird according to its kind. And God saw that it was good. God blessed them and said, \"Be fruitful and increase in number and fill the water in the seas, and let the birds increase on the earth.\" And there was evening, and there was morning—the fifth day.\n",
        "\n",
        "And God said, \"Let the land produce living creatures according to their kinds: the livestock, the creatures that move along the ground, and the wild animals, each according to its kind.\" And it was so. God made the wild animals according to their kinds, the livestock according to their kinds, and all the creatures that move along the ground according to their kinds. And God saw that it was good. Then God said, \"Let us make mankind in our image, in our likeness, so that they may rule over the fish in the sea and the birds in the sky, over the livestock and all the wild animals, and over all the creatures that move along the ground.\" So God created mankind in his own image, in the image of God he created them; male and female he created them. God blessed them and said to them, \"Be fruitful and increase in number; fill the earth and subdue it. Rule over the fish in the sea and the birds in the sky and over every living creature that moves on the ground.\" Then God said, \"I give you every seed-bearing plant on the face of the whole earth and every tree that has fruit with seed in it. They will be yours for food. And to all the beasts of the earth and all the birds in the sky and all the creatures that move along the ground—everything that has the breath of life in it—I give every green plant for food.\" And it was so. God saw all that he had made, and it was very good. And there was evening, and there was morning—the sixth day.\n",
        "\n",
        "Thus the heavens and the earth were completed in all their vast array. By the seventh day God had finished the work he had been doing; so on the seventh day he rested from all his work. Then God blessed the seventh day and made it holy, because on it he rested from all the work of creating that he had done.\n",
        "\"\"\" * 50\n",
        "\n",
        "\n",
        "def generate_simulated_data(text, chunk_size, num_chunks, tokenizer_name):\n",
        "    \"\"\"Simulates a list of text chunks for one book (D_f^t).\n",
        "    \n",
        "    This function splits long book text into smaller chunks that fit within the model's\n",
        "    maximum sequence length. It does this by:\n",
        "    1. Splitting text into small word chunks (to avoid tokenization warnings)\n",
        "    2. Tokenizing each small chunk separately\n",
        "    3. Combining tokenized chunks to create final chunks of the desired size\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Get model max length - this is the MAXIMUM tokens the model can handle\n",
        "    # TinyLlama has max_length of 2048 tokens\n",
        "    max_length = getattr(tokenizer, 'model_max_length', 32000)\n",
        "    \n",
        "    # Use a safe chunk size that's smaller than model max length\n",
        "    # We use chunk_size from config (256) but ensure it's under the model limit\n",
        "    safe_chunk_size = min(chunk_size, max_length - 10)  # Leave some margin for safety\n",
        "    \n",
        "    # IMPORTANT: We need to split text into VERY small pieces before tokenizing\n",
        "    # Why? Because 1 word can become 1-3 tokens, and we want to stay under max_length\n",
        "    # Strategy: Tokenize in small batches (500-800 words max) to avoid warnings\n",
        "    words = text.split()\n",
        "    \n",
        "    # Conservative estimate: ~1.5 tokens per word on average\n",
        "    # So for max_length=2048, we want max ~1300 words per tokenization batch\n",
        "    # But to be extra safe, we'll use even smaller: 500 words per batch\n",
        "    words_per_batch = min(500, max_length // 3)  # Very conservative: 500 words max per batch\n",
        "    \n",
        "    # Step 1: Tokenize text in small batches to avoid warnings\n",
        "    all_token_ids = []\n",
        "    for i in range(0, len(words), words_per_batch):\n",
        "        batch_text = ' '.join(words[i:i + words_per_batch])\n",
        "        # Tokenize with truncation - this ensures we never exceed max_length\n",
        "        tokenized = tokenizer(\n",
        "            batch_text, \n",
        "            return_tensors='pt', \n",
        "            truncation=True,  # This truncates if too long\n",
        "            max_length=max_length,  # Use model's actual max length\n",
        "            add_special_tokens=True\n",
        "        )['input_ids'][0]\n",
        "        all_token_ids.extend(tokenized.tolist())\n",
        "    \n",
        "    # Step 2: If we don't have enough tokens, repeat the sequence\n",
        "    if len(all_token_ids) < safe_chunk_size * num_chunks:\n",
        "        repeat_factor = (safe_chunk_size * num_chunks // len(all_token_ids)) + 1\n",
        "        all_token_ids = (all_token_ids * repeat_factor)[:safe_chunk_size * num_chunks * 2]\n",
        "    \n",
        "    # Step 3: Split into chunks of the desired size\n",
        "    chunks = []\n",
        "    for i in range(0, len(all_token_ids) - safe_chunk_size + 1, safe_chunk_size):\n",
        "        chunk = all_token_ids[i:i + safe_chunk_size]\n",
        "        if len(chunk) == safe_chunk_size:\n",
        "            chunks.append(chunk)\n",
        "        if len(chunks) >= num_chunks:\n",
        "            break\n",
        "    \n",
        "    # Step 4: If we still don't have enough, pad the last chunk\n",
        "    while len(chunks) < num_chunks:\n",
        "        if chunks:\n",
        "            # Repeat last chunk or pad\n",
        "            last_chunk = chunks[-1][:safe_chunk_size]\n",
        "            if len(last_chunk) < safe_chunk_size:\n",
        "                last_chunk = last_chunk + all_token_ids[:safe_chunk_size - len(last_chunk)]\n",
        "            chunks.append(last_chunk[:safe_chunk_size])\n",
        "        else:\n",
        "            # If no chunks at all, create a dummy chunk\n",
        "            chunks.append(all_token_ids[:safe_chunk_size])\n",
        "    \n",
        "    chunks = chunks[:num_chunks]\n",
        "    \n",
        "    # Step 5: Decode back to text (this is what the dataset will use)\n",
        "    text_chunks = [tokenizer.decode(c, skip_special_tokens=True) for c in chunks]\n",
        "    return text_chunks\n",
        "\n",
        "\n",
        "class SequentialUnlearningDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for SSU loss with dual labels.\"\"\"\n",
        "    def __init__(self, tokenizer, data_texts):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data_texts = data_texts\n",
        "        \n",
        "        tokenized = tokenizer(\n",
        "            data_texts, \n",
        "            truncation=True, \n",
        "            padding=\"max_length\", \n",
        "            max_length=Config.CHUNK_SIZE, \n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        self.input_ids = tokenized['input_ids']\n",
        "        self.attention_mask = tokenized['attention_mask']\n",
        "        \n",
        "        self.random_indices = list(range(len(data_texts)))\n",
        "        random.shuffle(self.random_indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.input_ids[idx].clone()\n",
        "        attention_mask = self.attention_mask[idx].clone()\n",
        "        labels_fgt = input_ids.clone()\n",
        "        rnd_idx = self.random_indices[idx]\n",
        "        labels_rnd = self.input_ids[rnd_idx].clone()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels_fgt': labels_fgt,\n",
        "            'labels_rnd': labels_rnd\n",
        "        }\n",
        "\n",
        "\n",
        "class SSUDataCollator:\n",
        "    \"\"\"Custom data collator that preserves labels_fgt and labels_rnd for SSU training.\"\"\"\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __call__(self, features):\n",
        "        \"\"\"Collate batch while preserving custom labels.\"\"\"\n",
        "        import torch\n",
        "        from torch.nn.utils.rnn import pad_sequence\n",
        "        \n",
        "        if not features:\n",
        "            raise ValueError(\"Empty features list passed to data collator\")\n",
        "        \n",
        "        # Debug: Check what keys are in the first feature\n",
        "        first_feature_keys = list(features[0].keys())\n",
        "        \n",
        "        # Check if features have the custom labels (for SSU training)\n",
        "        has_custom_labels = 'labels_fgt' in features[0] and 'labels_rnd' in features[0]\n",
        "        \n",
        "        if not has_custom_labels:\n",
        "            # Debug information\n",
        "            print(f\"Warning: Custom labels not found in features. Available keys: {first_feature_keys}\")\n",
        "            print(\"Creating labels from input_ids as fallback...\")\n",
        "        \n",
        "        # Extract custom labels if they exist\n",
        "        if has_custom_labels:\n",
        "            labels_fgt_list = [f.pop('labels_fgt') for f in features]\n",
        "            labels_rnd_list = [f.pop('labels_rnd') for f in features]\n",
        "        else:\n",
        "            # If no custom labels, create them from input_ids (fallback)\n",
        "            # This should not happen with SequentialUnlearningDataset, but handle it gracefully\n",
        "            labels_fgt_list = []\n",
        "            labels_rnd_list = []\n",
        "            for f in features:\n",
        "                input_ids = f['input_ids']\n",
        "                if isinstance(input_ids, torch.Tensor):\n",
        "                    labels_fgt_list.append(input_ids.clone())\n",
        "                    labels_rnd_list.append(input_ids.clone())\n",
        "                else:\n",
        "                    labels_fgt_list.append(torch.tensor(input_ids).clone())\n",
        "                    labels_rnd_list.append(torch.tensor(input_ids).clone())\n",
        "        \n",
        "        batch = {}\n",
        "        \n",
        "        # Helper to convert to tensor if needed\n",
        "        def to_tensor(x):\n",
        "            if isinstance(x, torch.Tensor):\n",
        "                return x\n",
        "            return torch.tensor(x)\n",
        "        \n",
        "        # Collate input_ids (already tensors from dataset)\n",
        "        input_ids = [to_tensor(f['input_ids']) for f in features]\n",
        "        batch['input_ids'] = pad_sequence(\n",
        "            input_ids, \n",
        "            batch_first=True, \n",
        "            padding_value=self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
        "        )\n",
        "        \n",
        "        # Collate attention_mask\n",
        "        attention_mask = [to_tensor(f['attention_mask']) for f in features]\n",
        "        batch['attention_mask'] = pad_sequence(\n",
        "            attention_mask,\n",
        "            batch_first=True,\n",
        "            padding_value=0\n",
        "        )\n",
        "        \n",
        "        # Add back the custom labels (already tensors from dataset)\n",
        "        batch['labels_fgt'] = pad_sequence(\n",
        "            [to_tensor(l) for l in labels_fgt_list],\n",
        "            batch_first=True,\n",
        "            padding_value=-100  # -100 is ignored in loss computation\n",
        "        )\n",
        "        \n",
        "        batch['labels_rnd'] = pad_sequence(\n",
        "            [to_tensor(l) for l in labels_rnd_list],\n",
        "            batch_first=True,\n",
        "            padding_value=-100\n",
        "        )\n",
        "        \n",
        "        return batch\n",
        "\n",
        "\n",
        "def download_gutenberg_book(book_id, output_dir):\n",
        "    \"\"\"Download a book from Project Gutenberg by ID.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    book_file = os.path.join(output_dir, f\"{book_id}.txt\")\n",
        "    \n",
        "    if os.path.exists(book_file):\n",
        "        print(f\"Book {book_id} already exists, skipping download.\")\n",
        "        return book_file\n",
        "    \n",
        "    url = f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\"\n",
        "    try:\n",
        "        print(f\"Downloading book {book_id} from Project Gutenberg...\")\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        with open(book_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(response.text)\n",
        "        print(f\"Downloaded book {book_id} successfully.\")\n",
        "        return book_file\n",
        "    except:\n",
        "        url_alt = f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
        "        try:\n",
        "            response = requests.get(url_alt, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            with open(book_file, 'w', encoding='utf-8') as f:\n",
        "                f.write(response.text)\n",
        "            print(f\"Downloaded book {book_id} successfully.\")\n",
        "            return book_file\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not download book {book_id}. Using dummy text.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "def load_book_text(book_file):\n",
        "    \"\"\"Load and clean text from a book file.\"\"\"\n",
        "    if not book_file or not os.path.exists(book_file):\n",
        "        return None\n",
        "    with open(book_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        text = f.read()\n",
        "    # Remove Project Gutenberg headers/footers\n",
        "    start_markers = [\"*** START OF\", \"***START OF\", \"START OF THE PROJECT\"]\n",
        "    end_markers = [\"*** END OF\", \"***END OF\", \"END OF THE PROJECT\"]\n",
        "    for marker in start_markers:\n",
        "        idx = text.find(marker)\n",
        "        if idx != -1:\n",
        "            text = text[text.find('\\n', idx) + 1:]\n",
        "            break\n",
        "    for marker in end_markers:\n",
        "        idx = text.find(marker)\n",
        "        if idx != -1:\n",
        "            text = text[:idx]\n",
        "            break\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_all_books_for_initial_finetuning():\n",
        "    \"\"\"Downloads all books for initial fine-tuning (D_f) - makes model memorize them.\"\"\"\n",
        "    print(\"\\n=== Downloading all books for initial fine-tuning (D_f) ===\")\n",
        "    all_books_dir = os.path.join(Config.DATA_DIR, \"all_books\")\n",
        "    os.makedirs(all_books_dir, exist_ok=True)\n",
        "    \n",
        "    book_texts = []\n",
        "    for book_id in Config.ALL_BOOK_IDS:\n",
        "        book_file = download_gutenberg_book(book_id, all_books_dir)\n",
        "        if book_file:\n",
        "            text = load_book_text(book_file)\n",
        "            if text and len(text) > 1000:\n",
        "                book_texts.append(text)\n",
        "                print(f\"✓ Loaded book {book_id} ({len(text)} chars)\")\n",
        "    \n",
        "    if not book_texts:\n",
        "        print(\"Warning: No books downloaded. Using dummy text.\")\n",
        "        book_texts = [DUMMY_BOOK_TEXT]\n",
        "    \n",
        "    return book_texts\n",
        "\n",
        "\n",
        "def get_unlearning_datasets():\n",
        "    \"\"\"Generates sequential datasets D_f^1, D_f^2, ... for each time step.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    datasets = []\n",
        "    \n",
        "    for t in range(Config.NUM_UNLEARNING_STEPS):\n",
        "        print(f\"\\n--- Preparing dataset D_f^{t+1} for time step {t+1} ---\")\n",
        "        \n",
        "        if Config.USE_REAL_BOOKS:\n",
        "            # Download books for this specific time step\n",
        "            time_step_dir = os.path.join(Config.DATA_DIR, f\"time_step_{t+1}\")\n",
        "            os.makedirs(time_step_dir, exist_ok=True)\n",
        "            \n",
        "            book_ids = Config.GUTENBERG_BOOK_IDS.get(t + 1, [])\n",
        "            book_texts = []\n",
        "            for book_id in book_ids:\n",
        "                book_file = download_gutenberg_book(book_id, time_step_dir)\n",
        "                if book_file:\n",
        "                    text = load_book_text(book_file)\n",
        "                    if text and len(text) > 1000:\n",
        "                        book_texts.append(text)\n",
        "                        print(f\"✓ Loaded book {book_id} for step {t+1}\")\n",
        "            if not book_texts:\n",
        "                print(f\"Warning: No valid books for step {t+1}. Using dummy text.\")\n",
        "                book_texts = [DUMMY_BOOK_TEXT]\n",
        "        else:\n",
        "            book_texts = [DUMMY_BOOK_TEXT]\n",
        "        \n",
        "        all_chunks = []\n",
        "        for book_text in book_texts:\n",
        "            chunks = generate_simulated_data(\n",
        "                book_text,\n",
        "                Config.CHUNK_SIZE,\n",
        "                Config.NUM_CHUNKS_PER_STEP // len(book_texts) + 1,\n",
        "                Config.TOKENIZER_NAME\n",
        "            )\n",
        "            all_chunks.extend(chunks)\n",
        "        \n",
        "        data_t = all_chunks[:Config.NUM_CHUNKS_PER_STEP]\n",
        "        print(f\"Created {len(data_t)} chunks for time step {t+1}\")\n",
        "        \n",
        "        dataset_t = SequentialUnlearningDataset(tokenizer, data_t)\n",
        "        datasets.append(dataset_t)\n",
        "        \n",
        "    return datasets\n",
        "\n",
        "\n",
        "def get_retention_dataset():\n",
        "    \"\"\"Generates retention dataset D_nor (non-targeted data to keep).\"\"\"\n",
        "    if not Config.USE_RETENTION_DATA:\n",
        "        return None\n",
        "    \n",
        "    print(\"\\n=== Preparing retention dataset D_nor ===\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(Config.TOKENIZER_NAME)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Use some books that are NOT in the unlearning set\n",
        "    retention_book_ids = [1232, 145, 76, 2591, 30254, 844, 345, 520, 6130, 174][:Config.NUM_RETENTION_BOOKS]\n",
        "    retention_dir = os.path.join(Config.DATA_DIR, \"retention_books\")\n",
        "    os.makedirs(retention_dir, exist_ok=True)\n",
        "    \n",
        "    all_chunks = []\n",
        "    for book_id in retention_book_ids:\n",
        "        if book_id not in Config.ALL_BOOK_IDS:  # Don't use books from D_f\n",
        "            book_file = download_gutenberg_book(book_id, retention_dir)\n",
        "            if book_file:\n",
        "                text = load_book_text(book_file)\n",
        "                if text and len(text) > 1000:\n",
        "                    chunks = generate_simulated_data(\n",
        "                        text,\n",
        "                        Config.CHUNK_SIZE,\n",
        "                        Config.NUM_RETENTION_CHUNKS // Config.NUM_RETENTION_BOOKS + 1,\n",
        "                        Config.TOKENIZER_NAME\n",
        "                    )\n",
        "                    all_chunks.extend(chunks)\n",
        "    \n",
        "    if not all_chunks:\n",
        "        print(\"Warning: No retention books downloaded. Using dummy text.\")\n",
        "        all_chunks = generate_simulated_data(\n",
        "            DUMMY_BOOK_TEXT,\n",
        "            Config.CHUNK_SIZE,\n",
        "            Config.NUM_RETENTION_CHUNKS,\n",
        "            Config.TOKENIZER_NAME\n",
        "        )\n",
        "    \n",
        "    retention_chunks = all_chunks[:Config.NUM_RETENTION_CHUNKS]\n",
        "    print(f\"Created {len(retention_chunks)} retention chunks\")\n",
        "    \n",
        "    return SequentialUnlearningDataset(tokenizer, retention_chunks)\n",
        "\n",
        "print(\"Data utilities loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "\n",
        "class SSUTrainer(Trainer):\n",
        "    \"\"\"Custom Trainer implementing SSU loss and Weight Saliency.\"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        \"\"\"Compute SSU loss (L_fgt + L_rnd) with support for additional kwargs.\n",
        "        \n",
        "        This method accepts any additional keyword arguments that newer versions\n",
        "        of transformers might pass (like num_items_in_batch).\n",
        "        \"\"\"\n",
        "        # Make a copy of inputs to avoid modifying the original dict\n",
        "        inputs_copy = inputs.copy()\n",
        "        \n",
        "        # Extract our custom labels (these are added by SequentialUnlearningDataset)\n",
        "        labels_fgt = inputs_copy.pop('labels_fgt', None)\n",
        "        labels_rnd = inputs_copy.pop('labels_rnd', None)\n",
        "        \n",
        "        if labels_fgt is None or labels_rnd is None:\n",
        "            raise ValueError(\"Missing labels_fgt or labels_rnd in inputs. Check dataset.\")\n",
        "        \n",
        "        # L_fgt (Forgetting Loss)\n",
        "        outputs_fgt = model(**inputs_copy, labels=labels_fgt)\n",
        "        loss_fgt = outputs_fgt.loss \n",
        "\n",
        "        # L_rnd (Random Labeling Loss)\n",
        "        outputs_rnd = model(**inputs_copy, labels=labels_rnd)\n",
        "        loss_rnd = outputs_rnd.loss\n",
        "        \n",
        "        # Combined SSU Loss\n",
        "        loss = Config.EPSILON_1 * loss_fgt + Config.EPSILON_2 * loss_rnd\n",
        "        \n",
        "        return (loss, outputs_fgt) if return_outputs else loss\n",
        "\n",
        "    def optimizer_step(self):\n",
        "        \"\"\"Override optimizer_step to apply weight saliency masking.\"\"\"\n",
        "        # Apply weight saliency mask before optimizer step\n",
        "        if self.accelerator.sync_gradients:\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if param.grad is not None and param.requires_grad:\n",
        "                    if \"lora\" in name.lower():\n",
        "                        grad = param.grad.data\n",
        "                        \n",
        "                        # Saliency Mask: m_s = I(|grad| >= gamma)\n",
        "                        m_s = (grad.abs() >= Config.GAMMA).float()\n",
        "                        \n",
        "                        # Apply mask to gradients (only update parameters with high saliency)\n",
        "                        param.grad.data = grad * m_s\n",
        "        \n",
        "        # Call parent optimizer_step to perform the actual update\n",
        "        super().optimizer_step()\n",
        "\n",
        "\n",
        "def create_lora_model(model):\n",
        "    \"\"\"Adds LoRA adapters to the base model.\"\"\"\n",
        "    peft_config = LoraConfig(\n",
        "        r=Config.LORA_R,\n",
        "        lora_alpha=Config.LORA_ALPHA,\n",
        "        lora_dropout=Config.LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=Config.TARGET_MODULES,\n",
        "    )\n",
        "    return get_peft_model(model, peft_config)\n",
        "\n",
        "\n",
        "def apply_task_vector_negation(base_model, fine_tuned_model, name_prefix):\n",
        "    \"\"\"Task Vector Negation: theta_u^t = 2 * theta_u^{t-1} - theta_ft^t\"\"\"\n",
        "    print(f\"\\n--- Applying Task Vector Negation for {name_prefix} ---\")\n",
        "    \n",
        "    device = next(base_model.parameters()).device\n",
        "    new_unlearned_model = base_model.__class__(config=base_model.config).to(device)\n",
        "    \n",
        "    base_state = base_model.state_dict()\n",
        "    ft_state = fine_tuned_model.state_dict()\n",
        "    \n",
        "    new_state = {}\n",
        "    for name, param in new_unlearned_model.named_parameters():\n",
        "        if name in base_state and name in ft_state:\n",
        "            new_state[name] = 2 * base_state[name] - ft_state[name]\n",
        "        else:\n",
        "            new_state[name] = base_state.get(name, param.data)\n",
        "    \n",
        "    new_unlearned_model.load_state_dict(new_state)\n",
        "    print(f\"Task Vector Negation complete for {name_prefix}.\")\n",
        "    return new_unlearned_model\n",
        "\n",
        "print(\"SSU model utilities loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Loading with Retry Logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading base model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
            "Loading model (attempt 1/3)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\KTS\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded TinyLlama/TinyLlama-1.1B-Chat-v1.0!\n",
            "Model loaded and frozen!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time\n",
        "\n",
        "\n",
        "def load_model_with_retry(model_name, max_retries=3, retry_delay=5):\n",
        "    \"\"\"Load model with automatic retry on network errors.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"Loading model (attempt {attempt + 1}/{max_retries})...\")\n",
        "            \n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            \n",
        "            # Determine device\n",
        "            if torch.cuda.is_available():\n",
        "                device = \"cuda\"\n",
        "                dtype = torch.bfloat16\n",
        "            else:\n",
        "                device = \"cpu\"\n",
        "                dtype = torch.float32\n",
        "            \n",
        "            base_model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name, \n",
        "                torch_dtype=dtype,\n",
        "                device_map=device,  # Use single device instead of \"auto\"\n",
        "                trust_remote_code=True,\n",
        "                resume_download=True  # Resume interrupted downloads\n",
        "            )\n",
        "            \n",
        "            # Ensure model is on the correct device\n",
        "            base_model = base_model.to(device)\n",
        "            \n",
        "            print(f\"Successfully loaded {model_name}!\")\n",
        "            return base_model, tokenizer\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_msg = str(e)\n",
        "            print(f\"Attempt {attempt + 1} failed: {error_msg[:200]}...\")\n",
        "            \n",
        "            if attempt < max_retries - 1:\n",
        "                if \"IncompleteRead\" in error_msg or \"Connection\" in error_msg or \"timeout\" in error_msg.lower():\n",
        "                    print(f\"Network error detected. Retrying in {retry_delay} seconds...\")\n",
        "                    time.sleep(retry_delay)\n",
        "                    retry_delay *= 2  # Exponential backoff\n",
        "                else:\n",
        "                    print(\"Non-network error. Retrying...\")\n",
        "                    time.sleep(2)\n",
        "            else:\n",
        "                print(\"\\nAll retry attempts failed!\")\n",
        "                print(\"\\nTROUBLESHOOTING:\")\n",
        "                print(\"1. Check your internet connection\")\n",
        "                print(\"2. Try a smaller model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "                print(\"3. For gated models, ensure HF_TOKEN is set\")\n",
        "                raise\n",
        "    \n",
        "    return None, None\n",
        "\n",
        "\n",
        "# Load the model\n",
        "print(f\"Loading base model: {Config.MODEL_NAME}\")\n",
        "base_model, tokenizer = load_model_with_retry(Config.MODEL_NAME)\n",
        "base_model.requires_grad_(False)\n",
        "print(\"Model loaded and frozen!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Main SSU Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Memorized model already exists at ssu_unlearned_models/memorized_model\n",
            "Loading existing memorized model...\n",
            "✓ Loaded existing memorized model.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "def initial_finetuning(model, tokenizer, all_books_texts):\n",
        "    \"\"\"\n",
        "    Step 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n",
        "    This is what the paper does BEFORE unlearning.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Fine-tuning vanilla model on all copyrighted books to memorize them...\")\n",
        "    \n",
        "    # Generate chunks from all books\n",
        "    all_chunks = []\n",
        "    for book_text in all_books_texts:\n",
        "        chunks = generate_simulated_data(\n",
        "            book_text,\n",
        "            Config.CHUNK_SIZE,\n",
        "            Config.NUM_CHUNKS_PER_STEP * Config.NUM_UNLEARNING_STEPS // len(all_books_texts) + 1,\n",
        "            Config.TOKENIZER_NAME\n",
        "        )\n",
        "        all_chunks.extend(chunks)\n",
        "    \n",
        "    print(f\"Created {len(all_chunks)} chunks from all books\")\n",
        "    \n",
        "    # For initial fine-tuning, use standard dataset (not SSU dual labels)\n",
        "    from torch.utils.data import Dataset as TorchDataset\n",
        "    class StandardDataset(TorchDataset):\n",
        "        def __init__(self, tokenizer, data_texts):\n",
        "            tokenized = tokenizer(\n",
        "                data_texts,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=Config.CHUNK_SIZE,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            self.input_ids = tokenized['input_ids']\n",
        "            self.attention_mask = tokenized['attention_mask']\n",
        "        \n",
        "        def __len__(self):\n",
        "            return len(self.input_ids)\n",
        "        \n",
        "        def __getitem__(self, idx):\n",
        "            return {\n",
        "                'input_ids': self.input_ids[idx].clone(),\n",
        "                'attention_mask': self.attention_mask[idx].clone(),\n",
        "                'labels': self.input_ids[idx].clone()  # Standard labels for next token prediction\n",
        "            }\n",
        "    \n",
        "    initial_dataset = StandardDataset(tokenizer, all_chunks)\n",
        "    \n",
        "    # Ensure model is on correct device before creating LoRA\n",
        "    device = next(model.parameters()).device\n",
        "    if device.type == \"meta\" or str(device) == \"meta\":\n",
        "        # If model is on meta device, move to actual device\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        model = model.to(device)\n",
        "    else:\n",
        "        # Convert device object to string if needed\n",
        "        device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n",
        "    \n",
        "    # Create LoRA model for initial fine-tuning (PEFT handles device automatically)\n",
        "    lora_model = create_lora_model(model)\n",
        "    lora_model.print_trainable_parameters()\n",
        "    \n",
        "    # PEFT models inherit device from base model, no need to call .to()\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"{Config.OUTPUT_DIR}/initial_ft_checkpoints\",\n",
        "        per_device_train_batch_size=Config.BATCH_SIZE,\n",
        "        gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=10,\n",
        "        learning_rate=Config.LEARNING_RATE,\n",
        "        num_train_epochs=Config.NUM_EPOCHS_INITIAL_FT,\n",
        "        logging_steps=10,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        "        fp16=False,\n",
        "        bf16=torch.cuda.is_available() and device == \"cuda\",\n",
        "        dataloader_pin_memory=False,  # Fix device issues\n",
        "    )\n",
        "    \n",
        "    # Use standard Trainer for initial fine-tuning (not SSU)\n",
        "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,  # Causal LM, not masked LM\n",
        "    )\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        model=lora_model,\n",
        "        args=training_args,\n",
        "        train_dataset=initial_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "    \n",
        "    print(\"Starting initial fine-tuning...\")\n",
        "    trainer.train()\n",
        "    \n",
        "    # Merge LoRA weights into base model\n",
        "    memorized_model = lora_model.merge_and_unload()\n",
        "    memorized_model.requires_grad_(False)\n",
        "    \n",
        "    print(\"✓ Initial fine-tuning complete. Model has memorized all books.\")\n",
        "    return memorized_model\n",
        "\n",
        "\n",
        "def run_initial_finetuning():\n",
        "    \"\"\"STEP 0: Initial fine-tuning on all books (D_f) to make model memorize them.\n",
        "    \n",
        "    Run this cell once to create the memorized model. After this completes,\n",
        "    you can run the sequential unlearning steps without re-running this.\n",
        "    \"\"\"\n",
        "    # Ensure all required dependencies are available\n",
        "    missing = []\n",
        "    try:\n",
        "        _ = Config.OUTPUT_DIR\n",
        "    except NameError:\n",
        "        missing.append(\"Config (cell 4)\")\n",
        "    \n",
        "    try:\n",
        "        _ = base_model\n",
        "    except NameError:\n",
        "        missing.append(\"base_model (cell 11)\")\n",
        "    \n",
        "    try:\n",
        "        _ = tokenizer\n",
        "    except NameError:\n",
        "        missing.append(\"tokenizer (cell 11)\")\n",
        "    \n",
        "    if missing:\n",
        "        raise NameError(\n",
        "            f\"The following are not defined: {', '.join(missing)}. \"\n",
        "            f\"Please run the required cells first to set up the dependencies.\"\n",
        "        )\n",
        "    \n",
        "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
        "    \n",
        "    # Check if memorized model already exists\n",
        "    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n",
        "    if os.path.exists(memorized_model_path):\n",
        "        print(f\"Memorized model already exists at {memorized_model_path}\")\n",
        "        print(\"Loading existing memorized model...\")\n",
        "        from transformers import AutoModelForCausalLM\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "        memorized_model = AutoModelForCausalLM.from_pretrained(\n",
        "            memorized_model_path,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=device\n",
        "        )\n",
        "        memorized_model.requires_grad_(False)\n",
        "        print(\"✓ Loaded existing memorized model.\")\n",
        "        return memorized_model\n",
        "    \n",
        "    # STEP 0: Initial fine-tuning on all books (D_f)\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 0: INITIAL FINE-TUNING ON ALL BOOKS (D_f)\")\n",
        "    print(\"=\"*60)\n",
        "    all_books_texts = get_all_books_for_initial_finetuning()\n",
        "    memorized_model = initial_finetuning(base_model, tokenizer, all_books_texts)\n",
        "    \n",
        "    # Save the memorized model\n",
        "    memorized_model.save_pretrained(memorized_model_path)\n",
        "    tokenizer.save_pretrained(memorized_model_path)\n",
        "    print(f\"\\n✓ Memorized model saved to {memorized_model_path}\")\n",
        "    \n",
        "    return memorized_model\n",
        "\n",
        "\n",
        "# Run initial fine-tuning (only need to run this once)\n",
        "memorized_model = run_initial_finetuning()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting from memorized model for step 1\n",
            "\n",
            "--- Preparing dataset D_f^1 for time step 1 ---\n",
            "Book 1661 already exists, skipping download.\n",
            "✓ Loaded book 1661 for step 1\n",
            "Created 50 chunks for time step 1\n",
            "\n",
            "--- Preparing dataset D_f^2 for time step 2 ---\n",
            "Book 1342 already exists, skipping download.\n",
            "✓ Loaded book 1342 for step 2\n",
            "Created 50 chunks for time step 2\n",
            "\n",
            "--- Preparing dataset D_f^3 for time step 3 ---\n",
            "Book 11 already exists, skipping download.\n",
            "✓ Loaded book 11 for step 3\n",
            "Created 50 chunks for time step 3\n",
            "\n",
            "Generated 3 sequential unlearning datasets.\n",
            "\n",
            "=== Preparing retention dataset D_nor ===\n",
            "Book 1232 already exists, skipping download.\n",
            "Book 145 already exists, skipping download.\n",
            "Book 76 already exists, skipping download.\n",
            "Book 2591 already exists, skipping download.\n",
            "Book 30254 already exists, skipping download.\n",
            "Book 844 already exists, skipping download.\n",
            "Book 345 already exists, skipping download.\n",
            "Book 520 already exists, skipping download.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\KTS\\AppData\\Local\\Temp\\ipykernel_21596\\2112152554.py:126: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SSUTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SSUTrainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 20 retention chunks\n",
            "\n",
            "============================================================\n",
            "UNLEARNING STEP 1 (Unlearning D_f^1)\n",
            "============================================================\n",
            "Preparing LoRA model for fine-tuning on D_f^1...\n",
            "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n",
            "Starting fine-tuning with SSU Loss for step_1...\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'labels_fgt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 183\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m current_model\n\u001b[32m    180\u001b[39m \u001b[38;5;66;03m# Run sequential unlearning steps\u001b[39;00m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# You can specify start_step and end_step to resume from a specific step\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Example: run_sequential_unlearning(start_step=2, end_step=3) to run only steps 2-3\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m final_unlearned_model = \u001b[43mrun_sequential_unlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mrun_sequential_unlearning\u001b[39m\u001b[34m(start_step, end_step)\u001b[39m\n\u001b[32m    126\u001b[39m trainer = SSUTrainer(\n\u001b[32m    127\u001b[39m     model=lora_model,\n\u001b[32m    128\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    131\u001b[39m     data_collator=data_collator,\n\u001b[32m    132\u001b[39m )\n\u001b[32m    134\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting fine-tuning with SSU Loss for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;66;03m# Task Vector Negation Stage\u001b[39;00m\n\u001b[32m    138\u001b[39m theta_ft_t = lora_model.merge_and_unload()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:2618\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2616\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2617\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2618\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[32m   2620\u001b[39m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[32m   2621\u001b[39m \u001b[38;5;28mself\u001b[39m.current_gradient_accumulation_steps = \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer.py:5654\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5652\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5653\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5654\u001b[39m         batch_samples.append(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   5655\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5656\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\accelerate\\data_loader.py:567\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    569\u001b[39m     \u001b[38;5;28mself\u001b[39m.end()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\trainer_utils.py:872\u001b[39m, in \u001b[36mRemoveColumnsCollator.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    870\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[32m    871\u001b[39m     features = [\u001b[38;5;28mself\u001b[39m._remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[32m--> \u001b[39m\u001b[32m872\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mSSUDataCollator.__call__\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequence\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Extract custom labels before collation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m labels_fgt_list = [\u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabels_fgt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[32m    152\u001b[39m labels_rnd_list = [f.pop(\u001b[33m'\u001b[39m\u001b[33mlabels_rnd\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[32m    154\u001b[39m batch = {}\n",
            "\u001b[31mKeyError\u001b[39m: 'labels_fgt'"
          ]
        }
      ],
      "source": [
        "## 8. Sequential Unlearning Steps\n",
        "\n",
        "def run_sequential_unlearning(start_step=1, end_step=None):\n",
        "    \"\"\"Run sequential unlearning steps.\n",
        "    \n",
        "    Args:\n",
        "        start_step: Starting unlearning step (1-indexed). Default: 1\n",
        "        end_step: Ending unlearning step (inclusive). If None, uses Config.NUM_UNLEARNING_STEPS\n",
        "    \"\"\"\n",
        "    # Ensure all required dependencies are available\n",
        "    missing = []\n",
        "    try:\n",
        "        _ = Config.OUTPUT_DIR\n",
        "    except NameError:\n",
        "        missing.append(\"Config (cell 4)\")\n",
        "    \n",
        "    try:\n",
        "        _ = tokenizer\n",
        "    except NameError:\n",
        "        missing.append(\"tokenizer (cell 11)\")\n",
        "    \n",
        "    if missing:\n",
        "        raise NameError(\n",
        "            f\"The following are not defined: {', '.join(missing)}. \"\n",
        "            f\"Please run the required cells first.\"\n",
        "        )\n",
        "    \n",
        "    if end_step is None:\n",
        "        end_step = Config.NUM_UNLEARNING_STEPS\n",
        "    \n",
        "    # Load or start from memorized model (check if file exists, not variable)\n",
        "    memorized_model_path = f\"{Config.OUTPUT_DIR}/memorized_model\"\n",
        "    if not os.path.exists(memorized_model_path):\n",
        "        raise FileNotFoundError(\n",
        "            f\"Memorized model not found at {memorized_model_path}. \"\n",
        "            f\"Please run cell 13 (initial fine-tuning) first.\"\n",
        "        )\n",
        "    \n",
        "    # Determine starting model\n",
        "    if start_step == 1:\n",
        "        # Load memorized model\n",
        "        from transformers import AutoModelForCausalLM\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "        current_model = AutoModelForCausalLM.from_pretrained(\n",
        "            memorized_model_path,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=device\n",
        "        )\n",
        "        current_model.requires_grad_(False)\n",
        "        print(f\"Starting from memorized model for step 1\")\n",
        "    else:\n",
        "        # Load model from previous step\n",
        "        prev_step_path = f\"{Config.OUTPUT_DIR}/step_{start_step-1}_unlearned_model\"\n",
        "        if not os.path.exists(prev_step_path):\n",
        "            raise FileNotFoundError(\n",
        "                f\"Previous step model not found at {prev_step_path}. \"\n",
        "                f\"Please run steps 1 to {start_step-1} first.\"\n",
        "            )\n",
        "        from transformers import AutoModelForCausalLM\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
        "        current_model = AutoModelForCausalLM.from_pretrained(\n",
        "            prev_step_path,\n",
        "            torch_dtype=dtype,\n",
        "            device_map=device\n",
        "        )\n",
        "        current_model.requires_grad_(False)\n",
        "        print(f\"Starting from step {start_step-1} model for step {start_step}\")\n",
        "    \n",
        "    # Load sequential datasets for unlearning\n",
        "    unlearning_datasets = get_unlearning_datasets()\n",
        "    print(f\"\\nGenerated {len(unlearning_datasets)} sequential unlearning datasets.\")\n",
        "    \n",
        "    # Get retention dataset (D_nor) if needed\n",
        "    retention_dataset = get_retention_dataset()\n",
        "    \n",
        "    # STEP 1-N: Sequential Unlearning Loop\n",
        "    for t in range(start_step - 1, min(end_step, Config.NUM_UNLEARNING_STEPS)):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"UNLEARNING STEP {t+1} (Unlearning D_f^{t+1})\")\n",
        "        print(f\"{'='*60}\")\n",
        "        \n",
        "        dataset_t = unlearning_datasets[t]\n",
        "        step_prefix = f\"step_{t+1}\"\n",
        "        \n",
        "        # D_prev: Previously unlearned books (for t > 1)\n",
        "        # In the paper, this ensures model doesn't re-learn old content\n",
        "        if t > 0 and Config.USE_RETENTION_DATA:\n",
        "            print(f\"Note: D_prev includes books from steps 1-{t}\")\n",
        "        \n",
        "        # Fine-Tuning Stage\n",
        "        print(f\"Preparing LoRA model for fine-tuning on D_f^{t+1}...\")\n",
        "        \n",
        "        # Ensure model is on correct device\n",
        "        device = next(current_model.parameters()).device\n",
        "        if device.type == \"meta\" or str(device) == \"meta\":\n",
        "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            current_model = current_model.to(device)\n",
        "        else:\n",
        "            # Convert device object to string if needed\n",
        "            device = str(device).split(':')[0]  # Get 'cuda' or 'cpu'\n",
        "        \n",
        "        lora_model = create_lora_model(current_model)\n",
        "        # PEFT models inherit device from base model, no need to call .to()\n",
        "        lora_model.print_trainable_parameters()\n",
        "        \n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=f\"{Config.OUTPUT_DIR}/{step_prefix}_ft_checkpoints\",\n",
        "            per_device_train_batch_size=Config.BATCH_SIZE,\n",
        "            gradient_accumulation_steps=Config.GRADIENT_ACCUMULATION_STEPS,\n",
        "            warmup_steps=10,\n",
        "            learning_rate=Config.LEARNING_RATE,\n",
        "            num_train_epochs=Config.NUM_EPOCHS_FT,\n",
        "            logging_steps=10,\n",
        "            save_strategy=\"no\",\n",
        "            report_to=\"none\",\n",
        "            fp16=False,\n",
        "            bf16=torch.cuda.is_available() and device == \"cuda\",\n",
        "            dataloader_pin_memory=False,  # Fix device issues\n",
        "        )\n",
        "\n",
        "        # Use custom data collator that preserves labels_fgt and labels_rnd\n",
        "        data_collator = SSUDataCollator(tokenizer=tokenizer)\n",
        "        \n",
        "        trainer = SSUTrainer(\n",
        "            model=lora_model,\n",
        "            args=training_args,\n",
        "            train_dataset=dataset_t,\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        print(f\"Starting fine-tuning with SSU Loss for {step_prefix}...\")\n",
        "        trainer.train()\n",
        "        \n",
        "        # Task Vector Negation Stage\n",
        "        theta_ft_t = lora_model.merge_and_unload()\n",
        "        unlearned_model_t = apply_task_vector_negation(current_model, theta_ft_t, step_prefix)\n",
        "        unlearned_model_t.requires_grad_(False)\n",
        "        \n",
        "        # Save the unlearned model\n",
        "        unlearned_model_t.save_pretrained(f\"{Config.OUTPUT_DIR}/{step_prefix}_unlearned_model\")\n",
        "        tokenizer.save_pretrained(f\"{Config.OUTPUT_DIR}/{step_prefix}_unlearned_model\")\n",
        "        print(f\"✓ Unlearned model {step_prefix} saved.\")\n",
        "        \n",
        "        current_model = unlearned_model_t\n",
        "\n",
        "    # Final Evaluation\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"SEQUENTIAL UNLEARNING COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "    final_step = min(end_step, Config.NUM_UNLEARNING_STEPS)\n",
        "    print(f\"Final Unlearned Model: {Config.OUTPUT_DIR}/step_{final_step}_unlearned_model\")\n",
        "    \n",
        "    # Test generation\n",
        "    prompt = \"The quick brown fox\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    device = next(current_model.parameters()).device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    print(\"\\nTesting generation with final unlearned model...\")\n",
        "    current_model.eval()\n",
        "    with torch.no_grad():\n",
        "        output_tokens = current_model.generate(\n",
        "            **inputs, \n",
        "            max_new_tokens=30, \n",
        "            do_sample=True, \n",
        "            top_p=0.9, \n",
        "            temperature=0.7\n",
        "        )\n",
        "    \n",
        "    generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(f\"Generated: {generated_text}\")\n",
        "    \n",
        "    return current_model\n",
        "\n",
        "\n",
        "# Run sequential unlearning steps\n",
        "# You can specify start_step and end_step to resume from a specific step\n",
        "# Example: run_sequential_unlearning(start_step=2, end_step=3) to run only steps 2-3\n",
        "final_unlearned_model = run_sequential_unlearning()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DL",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
